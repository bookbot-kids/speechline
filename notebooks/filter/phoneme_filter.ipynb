{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce09a874cbc5437dbab01db55974eddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from datasets import Dataset, load_dataset\n",
    "from speechline.transcribers import Wav2Vec2Transcriber\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "transcriber = Wav2Vec2Transcriber(\"bookbot/w2v-bert-2.0-bb-libri-cv-giga-dean2zak\", None)\n",
    "\n",
    "dataset = load_dataset(\"bookbot/bookbot_en_v3_parakeet-ctc-1.1b_filtered\", \"default\", split=\"train\", num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h ɛ l ˈoʊ,w ˈɚ l d!\n",
      "h e l l o\n"
     ]
    }
   ],
   "source": [
    "from gruut import sentences\n",
    "def gruut_g2p(text: str):\n",
    "        phonemes = []\n",
    "        for words in sentences(text, lang=\"EN\"):\n",
    "            for word in words:\n",
    "                if word.is_major_break or word.is_minor_break:\n",
    "                    phonemes.append(word.text)\n",
    "                elif word.phonemes:\n",
    "                    phonemes.append(\" \".join(word.phonemes))\n",
    "        return \"\".join(phonemes)\n",
    "    \n",
    "print(gruut_g2p(\"Hello, World!\"))\n",
    "\n",
    "from g2p_id import G2p\n",
    "def g2p_id(text: str):\n",
    "    g2p = G2p()\n",
    "    results = g2p(text)\n",
    "    results = [phoneme for word in results for phoneme in word ]\n",
    "    return \" \".join(results)\n",
    "print(g2p_id(\"Hello\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient\n",
    "import os\n",
    "\n",
    "COSMOS_DB_KEY = os.getenv('COSMOS_DB_KEY')\n",
    "COSMOS_URL = \"https://bookbot.documents.azure.com:443/\"\n",
    "\n",
    "\n",
    "def get_cosmos_client(url, key, database_name):\n",
    "    \"\"\"Initialize and return a CosmosDB client.\"\"\"\n",
    "    client = CosmosClient(url, credential=key)\n",
    "    database = client.get_database_client(database_name)\n",
    "    word_container = database.get_container_client(\"WordUniversal\")\n",
    "    return word_container\n",
    "\n",
    "def get_lexicon(word_container, language_code):\n",
    "    \"\"\"Retrieve the lexicon for a specific language from CosmosDB.\"\"\"\n",
    "    query = f'SELECT * FROM c WHERE c.language = \"{language_code}\" and not is_defined(c.deletedAt)'\n",
    "    query_iterable = word_container.query_items(\n",
    "        query=query,\n",
    "        partition_key=\"default\",\n",
    "        max_item_count=10000,\n",
    "    )\n",
    "    lexicon = {}\n",
    "    for item in query_iterable:\n",
    "        if \"lexicons\" in item:\n",
    "            lexicon[item[\"word\"]] = set(item[\"lexicons\"])\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "cosmos_client = get_cosmos_client(COSMOS_URL, COSMOS_DB_KEY, \"Bookbot\")\n",
    "# cosmos_lexicon = get_lexicon(cosmos_client, \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechline.segmenters import PhonemeOverlapSegmenter\n",
    "from lexikos import Lexicon as Lexicos\n",
    "from g2p_id import G2p\n",
    "\n",
    "class Lexicon(PhonemeOverlapSegmenter):\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        self.cosmos_lexicon = get_lexicon(cosmos_client, language)\n",
    "        self._init_g2p(language)   \n",
    "        \n",
    "        # If language is english, use Lexicos\n",
    "        if language == \"en\":\n",
    "            lexicos_lexicon = Lexicos()\n",
    "            for k, v in lexicos_lexicon.items():\n",
    "                self.cosmos_lexicon[k] = self.cosmos_lexicon[k].union(set(v)) if k in self.cosmos_lexicon else set(v)\n",
    "        super().__init__(self.cosmos_lexicon)\n",
    "        \n",
    "    def gruut_g2p(self, text: str) -> List[str]:\n",
    "        phonemes = []\n",
    "        for words in sentences(text, lang=self.language):\n",
    "            for word in words:\n",
    "                if word.is_major_break or word.is_minor_break:\n",
    "                    phonemes.append(word.text)\n",
    "                elif word.phonemes:\n",
    "                    phonemes.append(\" \".join(word.phonemes))\n",
    "        return phonemes\n",
    "    \n",
    "    def g2p_id(self, text: str) -> List[str]:\n",
    "        g2p = G2p()\n",
    "        return \" \".join(g2p(text)[0])\n",
    "    \n",
    "    def _init_g2p(self, language):\n",
    "        if language == \"en\":\n",
    "            self.g2p = self.gruut_g2p\n",
    "        elif language == \"id\":\n",
    "            self.g2p = self.g2p_id\n",
    "        elif language == \"sw\":\n",
    "            self.g2p = self.gruut_g2p\n",
    "        \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        text = text.lower().strip()\n",
    "        return text  \n",
    "    \n",
    "    def _generate_combinations(self, ground_truth: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Generate all possible phoneme combinations for a given word.\n",
    "\n",
    "        Args:\n",
    "            ground_truth (List[str]):\n",
    "                List of words.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]:\n",
    "                List of phoneme combinations.\n",
    "        \"\"\"\n",
    "        combinations = []\n",
    "        for word in ground_truth:\n",
    "            normalized_word = self._normalize_text(word)\n",
    "            if normalized_word in self.lexicon:\n",
    "                phonemes = self.lexicon[normalized_word]\n",
    "            else:\n",
    "                phonemes = self.g2p(normalized_word)\n",
    "            combinations.append(phonemes)\n",
    "        return combinations\n",
    "    \n",
    "lexicon = Lexicon(\"en\")\n",
    "# ground_truth = [\"Hello\", \"World\"]\n",
    "# ground_truth = lexicon._generate_combinations(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from speechline.utils.tokenizer import WordTokenizer\n",
    "\n",
    "filtered_dataset = {\"audio\": [], \"transcript\": [], \"text\": [], \"language\": [], \"speaker\": []}\n",
    "dataset = dataset.select(range(100))\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "def check_phoneme_match(phoneme_transcript, ground_truth):\n",
    "    \"\"\"\n",
    "    Check if each phoneme in transcript exists in corresponding ground truth set\n",
    "    \n",
    "    Args:\n",
    "        phoneme_transcript (List[str]): List of phonemes from transcript\n",
    "        ground_truth (List[Set[str]]): List of sets containing valid phonemes\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all phonemes match their ground truth sets\n",
    "    Example:\n",
    "        phoneme_transcript = ['ɪn', 'ðɛɹ', 'deɪ']\n",
    "        ground_truth = [{'ɪ n', 'ɪ ŋ'}, {'ð ɛ ɹ', 'ð ɛ r'}, {'d e ɪ', 'd e ɪ'}]\n",
    "    \"\"\"\n",
    "    # Check lengths match first\n",
    "    if len(phoneme_transcript) != len(ground_truth):\n",
    "        return False\n",
    "        \n",
    "    # Check each phoneme against its ground truth set\n",
    "    for phoneme, valid_phonemes in zip(phoneme_transcript, ground_truth):\n",
    "        # Remove spaces from transcript phoneme for comparison\n",
    "        valid_phonemes_no_spaces = {p.replace(\" \", \"\") for p in valid_phonemes}\n",
    "        \n",
    "        if phoneme not in valid_phonemes_no_spaces:\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "\n",
    "for datum in dataset:\n",
    "    audio_data = datum[\"audio\"].copy()\n",
    "    result = transcriber.pipeline(audio_data, chunk_length_s=30)\n",
    "    phoneme_transcript = result[\"text\"]\n",
    "    list_phoneme_transcript = phoneme_transcript.split()\n",
    "    \n",
    "    # Get ground all lexicon combinations for each word from the ground_truth text\n",
    "    list_ground_truth_phonemes = lexicon._generate_combinations(tokenizer(datum[\"text\"]))\n",
    "\n",
    "    # print(f\"Phoneme Transcript: {phoneme_transcript}\")\n",
    "    # print(f\"List Phoneme transcript: {list_phoneme_transcript}\")\n",
    "    # print(f\"Ground truth text: {datum['text']}\")\n",
    "    # print(f\"Ground truth: {list_ground_truth_phonemes}\")\n",
    "    \n",
    "    if not check_phoneme_match(list_phoneme_transcript, list_ground_truth_phonemes):\n",
    "        continue\n",
    "    filtered_dataset[\"audio\"].append(datum[\"audio\"])\n",
    "    filtered_dataset[\"transcript\"].append(phoneme_transcript)\n",
    "    filtered_dataset[\"text\"].append(datum[\"text\"])\n",
    "    filtered_dataset[\"language\"].append(datum[\"language\"])\n",
    "    filtered_dataset[\"speaker\"].append(datum[\"speaker\"])\n",
    "    \n",
    "filtered_dataset = Dataset.from_dict(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.push_to_hub(\"bookbot/bookbot_en_v3_parakeet-ctc-1.1b_filtered_phoneme\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
