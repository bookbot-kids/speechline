{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookbot/common-voice-accent-gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "from dataclasses import dataclass\n",
    "from string import punctuation\n",
    "from typing import List\n",
    "from num2words import num2words\n",
    "from unidecode import unidecode\n",
    "from num2words import num2words\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "output_dir = Path(\"./cv-gb-alignment-result\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "# load MMS aligner model\n",
    "bundle = torchaudio.pipelines.MMS_FA\n",
    "model = bundle.get_model().to(device)\n",
    "chunk_size_s = 15\n",
    "DICTIONARY = bundle.get_dict()\n",
    "MMS_SUBSAMPLING_RATIO = 400\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = unidecode(text)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"\\d+\", lambda x: num2words(int(x.group(0)), lang=\"en\"), text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def align(emission, tokens, device):\n",
    "    targets = torch.tensor([tokens], dtype=torch.int32, device=device)\n",
    "    alignments, scores = F.forced_align(emission, targets, blank=0)\n",
    "\n",
    "    alignments, scores = alignments[0], scores[0]  # remove batch dimension for simplicity\n",
    "    scores = scores.exp()  # convert back to probability\n",
    "    return alignments, scores\n",
    "\n",
    "\n",
    "def unflatten(list_, lengths):\n",
    "    assert len(list_) == sum(lengths)\n",
    "    i = 0\n",
    "    ret = []\n",
    "    for l in lengths:\n",
    "        ret.append(list_[i : i + l])\n",
    "        i += l\n",
    "    return ret\n",
    "\n",
    "def compute_alignments(emission, transcript, dictionary, device):\n",
    "    tokens = [dictionary[char] for word in transcript for char in word]\n",
    "    alignment, scores = align(emission, tokens, device)\n",
    "    token_spans = F.merge_tokens(alignment, scores)\n",
    "    word_spans = unflatten(token_spans, [len(word) for word in transcript])\n",
    "    return word_spans\n",
    "\n",
    "def get_word_segments(datum):\n",
    "    transcript = datum[\"sentence\"]\n",
    "    transcript = preprocess_text(transcript)\n",
    "    words = transcript.split()\n",
    "    audio = datum[\"audio\"]\n",
    "    sampling_rate = audio[\"sampling_rate\"]\n",
    "    audio_array = torch.from_numpy(audio[\"array\"])\n",
    "    audio_id = Path(audio[\"path\"]).stem\n",
    "\n",
    "    resampler = T.Resample(sampling_rate, bundle.sample_rate, dtype=audio_array.dtype)\n",
    "    resampled_waveform = resampler(audio_array)\n",
    "\n",
    "    # split audio into chunks to avoid OOM and faster inference\n",
    "    chunk_size_frames = chunk_size_s * bundle.sample_rate\n",
    "    resampled_waveform = torch.unsqueeze(resampled_waveform, 0).float()\n",
    "    chunks = [\n",
    "        resampled_waveform[:, i : i + chunk_size_frames]\n",
    "        for i in range(0, resampled_waveform.shape[1], chunk_size_frames)\n",
    "    ]\n",
    "\n",
    "    # collect per-chunk emissions, rejoin\n",
    "    emissions = []\n",
    "    with torch.inference_mode():\n",
    "        for chunk in chunks:\n",
    "            # NOTE: we could pad here, but it'll need to be removed later\n",
    "            # skipping for simplicity, since it's at most 25ms\n",
    "            print(chunk.size(1))\n",
    "            if chunk.size(1) >= MMS_SUBSAMPLING_RATIO:\n",
    "                emission, _ = model(chunk.to(device))\n",
    "                print(emission.shape)\n",
    "                emissions.append(emission)\n",
    "\n",
    "\n",
    "    emission = torch.cat(emissions, dim=1)\n",
    "    num_frames = emission.size(1)\n",
    "    assert len(DICTIONARY) == emission.shape[2]\n",
    "\n",
    "    # perform forced-alignment\n",
    "    word_spans = compute_alignments(emission, words, DICTIONARY, device)\n",
    "    assert len(word_spans) == len(words)\n",
    "\n",
    "    # collect verse-level segments\n",
    "    segments, labels, start = [], [], 0\n",
    "    for word, span in zip(words, word_spans):\n",
    "        ratio = resampled_waveform.size(1) / num_frames\n",
    "        x0 = int(ratio * span[0].start)\n",
    "        x1 = int(ratio * span[-1].end)\n",
    "        segment = resampled_waveform[:, x0:x1]\n",
    "        segments.append(segment)\n",
    "        labels.append(word)\n",
    "\n",
    "    for segment, label in zip(segments, labels):\n",
    "        audio_name = audio_id + \"-\" + label\n",
    "        # write audio\n",
    "        audio_path = (output_dir / audio_name).with_suffix(\".wav\")\n",
    "        write(audio_path, bundle.sample_rate, segment.squeeze().numpy())\n",
    "\n",
    "        # write transcript\n",
    "        transcript_path = (output_dir / audio_name).with_suffix(\".txt\")\n",
    "        with open(transcript_path, \"w\") as f:\n",
    "            f.write(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110976\n",
      "torch.Size([1, 346, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for datum in tqdm(dataset[\"train\"].select(range(1))):\n",
    "    get_word_segments(datum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
