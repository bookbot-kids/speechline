{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf99\ufe0f SpeechLine","text":"<p>SpeechLine is a speech labeling pipeline that handles end-to-end, offline, batch audio categorization, transcription, segmentation, and logging.</p> <p></p>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"#available-modules","title":"Available Modules","text":"Figure inspired by BERTopic's Modularity Diagram"},{"location":"#examples","title":"Examples","text":"<ul> <li>SpeechLine on AWS SageMaker</li> </ul>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at team@bookbotkids.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to SpeechLine","text":"<p>Credits</p> <p>This document was adapted from the open-source contribution guidelines for Facebook's Draft and HuggingFace's Contributing Guidelines.</p> <p>Hi there! Thanks for taking your time to contribute to SpeechLine!</p> <p>We welcome everyone to contribute and we value each contribution, even the smallest ones! We want to make contributing to this project as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul> <p>Tip</p> <p>Please be mindful to respect our Code of Conduct.</p>"},{"location":"CONTRIBUTING/#create-a-pull-request","title":"Create a Pull Request","text":"<p>We use GitHub to host code, to track issues and feature requests, as well as accept pull requests. Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests!</p> <ol> <li> <p>Fork the repository by clicking on the Fork button on the repository's page. This creates a copy of the code under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/speechline.git\ncd speechline\ngit remote add upstream https://github.com/bookbot-kids/speechline.git\n</code></pre> </li> <li> <p>Create a new branch to hold your development changes:</p> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> </li> <li> <p>Set up a development environment by running the following command in a virtual environment:</p> <pre><code>pip install .\npip install -r requirements_test.txt\n</code></pre> </li> <li> <p>Install Linux package dependencies</p> <pre><code>sudo apt install ffmpeg\nsudo apt-get install libsndfile1-dev\n</code></pre> </li> <li> <p>Develop the features on your branch, add tests and documentation.</p> <p>As you work on your code, you should make sure the test suite passes. Run the tests impacted by your changes like this:</p> <pre><code>pytest tests/&lt;TEST_TO_RUN&gt;.py\n</code></pre> <p>SpeechLine relies on <code>black</code> and <code>isort</code> to format its source code consistently. After you make changes, apply automatic style corrections and code verifications that can't be automated in one go with:</p> <pre><code>make style\n</code></pre> <p>SpeechLine also uses <code>flake8</code> to check for coding mistakes. Quality controls are run by the CI, but you can run the same checks with:</p> <pre><code>make quality\n</code></pre> <p>This will also ensure that the documentation can still be built.</p> <p>Once you're happy with your changes, add changed files with git add and record your changes locally with git commit:</p> <pre><code>git add modified_file.py\ngit commit -m \"YOUR_COMMIT_MESSAGE_HERE\"\n</code></pre> <p>To keep your copy of the code up to date with the original repository, rebase your branch on upstream/branch before you open a pull request or if requested by a maintainer:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Push your changes to your branch:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <p>If you've already opened a pull request, you'll need to force push with the --force flag. Otherwise, if the pull request hasn't been opened yet, you can just push your changes normally.</p> </li> <li> <p>Now you can go to your fork of the repository on GitHub and click on Pull request to open a pull request. When you're ready, you can send your changes to the project maintainers for review.</p> </li> <li> <p>It's ok if maintainers request changes, it happens to our core contributors too! So everyone can see the changes in the pull request, work in your local branch and push the changes to your fork. They will automatically appear in the pull request.</p> </li> </ol>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>An extensive test suite is included to test the library behavior and several examples. Library tests can be found in the tests folder.</p> <p>We use <code>pytest</code> to run our tests. From the root of the repository, specify a path to a subfolder or a test file to run the test.</p> <pre><code>python -m pytest --cov-report term-missing --cov -v ./tests/&lt;TEST_TO_RUN&gt;.py\n</code></pre> <p>Alternatively, you can run <code>make cov</code>.</p> <p>Moreover, for our CI, we utilize <code>tox</code> that runs on GitHub Actions. You can similarly run this set of extensive tests locally by running:</p> <pre><code>tox -r\n</code></pre> <p>or equivalently, <code>make test</code>. </p>"},{"location":"CONTRIBUTING/#style-guide","title":"Style guide","text":"<p>For documentation strings, SpeechLine follows the Google Python Style Guide.</p>"},{"location":"CONTRIBUTING/#report-bugs-using-githubs-issues","title":"Report bugs using Github's issues","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue. This is an example of a good and thorough bug report.</p> <p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce<ul> <li>Be specific!</li> <li>Give sample code if you can.</li> </ul> </li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under its Apache 2.0 License. In short, when you submit code changes, your submissions are understood to be under the same Apache 2.0 License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"INSTALLATION/","title":"Installation","text":""},{"location":"INSTALLATION/#install-with-pip","title":"Install with pip","text":"<p>The easiest way to install SpeechLine is via <code>pip</code>. </p>"},{"location":"INSTALLATION/#latest-pypi-release","title":"Latest PyPI Release","text":"<pre><code>pip install speechline\n</code></pre> <p>This will install the latest released version of SpeechLine from PyPI. However, it might not be up to date with the one found in git. We recommend installing via <code>pip</code> but cloning the latest <code>main</code> branch.</p>"},{"location":"INSTALLATION/#latest-git-main-branch","title":"Latest Git Main Branch","text":"<pre><code>pip install git+https://github.com/bookbot-kids/speechline.git\n</code></pre>"},{"location":"INSTALLATION/#editable-install","title":"Editable Install","text":"<p>You will need an editable install if you\u2019d like to:</p> <ul> <li>Use the <code>main</code> version of the source code.</li> <li>Contribute to SpeechLine and need to test changes in the code.</li> </ul> <p>Clone the repository and install SpeechLine with the following commands:</p> <pre><code>git clone https://github.com/bookbot-kids/speechline.git\ncd speechline\npip install -e .\n</code></pre> <p>Also install several other dependencies for testing purposes:</p> <pre><code>pip install -r requirements_test.txt\n</code></pre> <p>Now you can easily update your clone to the latest version of SpeechLine with the following command:</p> <pre><code>git pull\n</code></pre> <p>Your Python environment will find the main version of SpeechLine on the next run.</p>"},{"location":"INSTALLATION/#linux-packages","title":"Linux Packages","text":"<p>SpeechLine relies on several Linux packages in order to run properly. On Linux, you can easily install dependencies by running:</p> <pre><code>sudo apt install ffmpeg\nsudo apt-get install libsndfile1-dev\n</code></pre>"},{"location":"PROJECT_CHARTER/","title":"Project Charter","text":""},{"location":"PROJECT_CHARTER/#vision-statement","title":"Vision statement","text":"<p>Literacy is fundamental, not only for our personal and social development, but also for our ability to function effectively in society. Our vision at Bookbot is that every child should have the opportunity to develop their reading, writing and communication skills to create a happy and successful life.</p>"},{"location":"PROJECT_CHARTER/#mission-statement","title":"Mission statement","text":"<p>Deliver the Bookbot app that combines speech recognition and a scientifically designed reading program for school children to achieve greater literacy and providing better tools for educators to monitor a child\u2019s reading progress. </p>"},{"location":"PROJECT_CHARTER/#community-impact-statement","title":"Community (Impact) statement","text":"<p>Bookbot is founded on the grounds of building a community of learners. Members of the Bookbot community consist of software developers, educators, students, writers, editors, linguists, people with disabilities, and more. We exist to ensure that every child, regardless of their situation, is able to develop their literacy skills.</p>"},{"location":"PROJECT_CHARTER/#licensing-strategy","title":"Licensing strategy","text":"<p>Open source (creative commons), and reseller model for the app. Parts of code are Apache 2</p>"},{"location":"PROJECT_CHARTER/#identification-of-key-trademarks","title":"Identification of key trademarks","text":"<p>No key trademarks</p>"},{"location":"guides/forced_align_punctuations/","title":"Force Aligning Punctuations","text":"<p>This guide will show the steps on how to align (or recover) punctuation using a Punctuation Forced Aligner from SpeechLine. </p> <p>As you may or may not know, transcription results from a Wav2Vec 2.0 usually do not include punctuations, unless it was trained to do so. In certain cases, however, you might want to have punctuations. An example would be transcribing phonemes of an audio as text-to-speech data, where punctuations are necessary.</p>"},{"location":"guides/forced_align_punctuations/#use-case","title":"Use Case","text":"<p>For instance, you know the corresponding utterance text of an audio, but you're unsure about the phonemes that were actually uttered by the speaker. Moreover, you want timestamps for each phoneme, such that you could segment them. With a phoneme-level Transcriber and Punctuation Forced Aligner, you could \"recover\" punctuations and add them to the string of phonemes, as follows:</p> <ul> <li>Text: <code>Her red umbrella, is just the best!</code></li> <li>Transcript: <code>h h \u025a i d \u028c m b \u0279 \u025b l \u0259 \u026a z d\u0361\u0292 \u028c s t \u00f0 \u0259 b \u025b s t</code></li> <li>Recovered Transcript: <code>h h \u025a i d \u028c m b \u0279 \u025b l \u0259 , \u026a z d\u0361\u0292 \u028c s t \u00f0 \u0259 b \u025b s t !</code></li> </ul> <p>You can restore and align punctuations by simply passing in the ground truth text to a <code>PunctuationForcedAligner</code>. More details will be discuss in the following example.</p> <p>The first step is, of course, to transcribe your text by loading in the transcription model</p> <pre><code>from speechline.transcribers import Wav2Vec2Transcriber\n\ntranscriber = Wav2Vec2Transcriber(\"bookbot/wav2vec2-ljspeech-gruut\")\n</code></pre> <p>Load the audio file into a <code>Dataset</code> format and pass it into the model</p> <pre><code>from datasets import Dataset, Audio\n\ndataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]})\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=transcriber.sampling_rate))\n</code></pre> <pre><code>phoneme_offsets = transcriber.predict(dataset, output_offsets=True, return_timestamps=\"char\")\n</code></pre> <pre><code>Transcribing Audios:   0%|          | 0/1 [00:00&lt;?, ?ex/s]\n</code></pre> <p>Now we will need utilize <code>gruut</code>, a grapheme-to-phoneme library that can help transform our ground truth text (given in <code>sample.txt</code>) into phonemes. Note that <code>gruut</code> retains punctuations during the g2p conversion. This information will be exploited by SpeechLine's <code>PunctuationForcedAligner</code> to restore the punctuations from the Wav2Vec 2.0 output and to estimate the right location to insert each punctuation.</p> <p>Simply use the following g2p function to convert any text string into phonemes. You can, of course, provide your own g2p function if you wish to do so.</p> <pre><code>from gruut import sentences\n\ndef g2p(text):\n    phonemes = []\n    for words in sentences(text):\n        for word in words:\n            if word.is_major_break or word.is_minor_break:\n                phonemes += word.text\n            elif word.phonemes:\n                phonemes += word.phonemes\n    return phonemes\n</code></pre> <pre><code>text = open(\"sample.txt\").readline()\ntext\n</code></pre> <pre><code>'Her red umbrella, is just the best!'\n</code></pre> <p>Instantiate <code>PunctuationForcedAlinger</code> by passing into it your g2p function. Finally, you can perform punctuation restoration by feeding in the offsets from the transcription model and the ground truth text.</p> <pre><code>from speechline.aligners import PunctuationForcedAligner\n\npfa = PunctuationForcedAligner(g2p)\npfa(phoneme_offsets[0], text)\n</code></pre> <pre><code>[{'end_time': 0.04, 'start_time': 0.0, 'text': 'h'},\n {'end_time': 0.2, 'start_time': 0.14, 'text': 'h'},\n {'end_time': 0.28, 'start_time': 0.24, 'text': '\u025a'},\n {'end_time': 0.44, 'start_time': 0.42, 'text': 'i'},\n {'end_time': 0.54, 'start_time': 0.5, 'text': 'd'},\n {'end_time': 0.66, 'start_time': 0.64, 'text': '\u028c'},\n {'end_time': 0.74, 'start_time': 0.7, 'text': 'm'},\n {'end_time': 0.82, 'start_time': 0.78, 'text': 'b'},\n {'end_time': 0.9, 'start_time': 0.84, 'text': '\u0279'},\n {'end_time': 0.94, 'start_time': 0.92, 'text': '\u025b'},\n {'end_time': 1.04, 'start_time': 1.0, 'text': 'l'},\n {'end_time': 1.12, 'start_time': 1.08, 'text': '\u0259'},\n {'text': ',', 'start_time': 1.12, 'end_time': 1.36},\n {'end_time': 1.38, 'start_time': 1.36, 'text': '\u026a'},\n {'end_time': 1.58, 'start_time': 1.54, 'text': 'z'},\n {'end_time': 1.62, 'start_time': 1.58, 'text': 'd\u0361\u0292'},\n {'end_time': 1.66, 'start_time': 1.62, 'text': '\u028c'},\n {'end_time': 1.76, 'start_time': 1.72, 'text': 's'},\n {'end_time': 1.82, 'start_time': 1.78, 'text': 't'},\n {'end_time': 1.88, 'start_time': 1.86, 'text': '\u00f0'},\n {'end_time': 1.94, 'start_time': 1.92, 'text': '\u0259'},\n {'end_time': 2.0, 'start_time': 1.98, 'text': 'b'},\n {'end_time': 2.06, 'start_time': 2.04, 'text': '\u025b'},\n {'end_time': 2.26, 'start_time': 2.22, 'text': 's'},\n {'end_time': 2.4, 'start_time': 2.38, 'text': 't'},\n {'text': '!', 'start_time': 2.4, 'end_time': 2.4}]\n</code></pre>"},{"location":"guides/google-cloud/","title":"SpeechLine on Google Cloud VM","text":"<p>SpeechLine is a platform-independent framework. You should be able to run SpeechLine on any platform, even without GPU. </p> <p>In this guide, we are going to walk you through all the steps of running SpeehLine on Google Cloud Platform (GCP), with additional utility scripts involving other services such as AirTable for loggings. While they are readily available in SpeechLine, they are not required to run the end-to-end pipeline.</p>"},{"location":"guides/google-cloud/#create-gcp-vm-instance","title":"Create GCP VM Instance","text":"<p>Because we are only going to perform inference on GPU, we don't really need a GPU with high VRAM. Because of that, we prefer to run the instance <code>n1-standard-16</code> with 1 NVIDIA T4, which has up to 16GB of VRAM. This is sufficient to run both classification and transcription with a reasonable batch size. Simply spin one up via the Google Cloud Compute Engine page, and enter the VM via SSH.</p>"},{"location":"guides/google-cloud/#install-linux-packages","title":"Install Linux Packages","text":"<p>We need to install all dependencies of SpeechLine. We'll begin with ffmpeg and ffprobe. </p> <pre><code>sudo apt-get install -y ffmpeg\n</code></pre> <p>You can verify that installation is successful by running</p> <pre><code>ffmpeg -version\nffprobe -version\n</code></pre>"},{"location":"guides/google-cloud/#install-speechline","title":"Install SpeechLine","text":"<p>We have to manually clone the source code and install SpeechLine from there. </p> <pre><code>git clone https://github.com/bookbot-kids/speechline.git\ncd speechline\npip install .\n</code></pre> <p>Ensure that your PyTorch installation has access to GPU by running:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>If you're getting an error like the following,</p> Error Log <pre><code>Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/__init__.py\", line 172, in _load_global_deps\n    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n  File \"/opt/conda/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\n    self._handle = _dlopen(self._name, mode)\nOSError: /opt/conda/lib/python3.7/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: symbol cublasLtHSHMatmulAlgoInit version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/conda/lib/python3.7/site-packages/torch/__init__.py\", line 217, in &lt;module&gt;\n    _load_global_deps()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/__init__.py\", line 178, in _load_global_deps\n    _preload_cuda_deps()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/__init__.py\", line 158, in _preload_cuda_deps\n    ctypes.CDLL(cublas_path)\n  File \"/opt/conda/lib/python3.7/ctypes/__init__.py\", line 364, in __init__\n    self._handle = _dlopen(self._name, mode)\nOSError: /opt/conda/lib/python3.7/site-packages/nvidia/cublas/lib/libcublas.so.11: symbol cublasLtHSHMatmulAlgoInit version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference\n</code></pre> <p>Export the following variable to solve the issue above.</p> <pre><code>export LD_LIBRARY_PATH=/opt/conda/lib/python3.7/site-packages/nvidia/cublas/lib/:$LD_LIBRARY_PATH\n</code></pre> <p>and then run the same line of code to recheck if your PyTorch has access to your GPU.</p>"},{"location":"guides/google-cloud/#optional-download-audio-data-from-google-cloud-storage","title":"(Optional) Download Audio Data from Google Cloud Storage","text":"<p>If your files are located on Google Cloud Storage as a zipped file, you can simply use the <code>gcloud</code> command to copy the file from Cloud Storage into your VM.</p> <pre><code>gcloud alpha storage cp gs://{BUCKET}/{FILENAME.zip} {PATH/TO/OUTPUT}\n</code></pre> <p>which you can then simply unzip as per normal.</p> <pre><code>unzip -q {PATH/TO/OUTPUT/FILENAME.zip}\n</code></pre> <p>SpeechLine has a specific requirement on how the folder structure should be. Because we would want to group the files into their specific languages to access the correct models, it should be of the following structure:</p> <pre><code>path_to_files\n\u251c\u2500\u2500 langX\n\u2502   \u251c\u2500\u2500 a.wav\n\u2502   \u251c\u2500\u2500 a.txt\n\u2502   \u251c\u2500\u2500 b.wav\n\u2502   \u2514\u2500\u2500 b.txt\n\u2514\u2500\u2500 langY\n    \u251c\u2500\u2500 c.wav\n    \u2514\u2500\u2500 c.txt\n</code></pre> <p>where <code>*.txt</code> files are the ground truth transcripts of the utterance <code>*.wav</code> files. Note that the <code>*.txt</code> files are optional -- the script will still run just fine if you don't have them.</p> <p>For instance, your directory should look like this:</p> <pre><code>dropbox/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 utt_0.wav\n    \u251c\u2500\u2500 utt_0.txt\n    ...\n\u251c\u2500\u2500 id-id\n    \u251c\u2500\u2500 utt_1.wav\n    \u251c\u2500\u2500 utt_1.txt\n    ...\n...\n</code></pre>"},{"location":"guides/google-cloud/#optional-convert-aac-audios-to-wav","title":"(Optional) Convert aac Audios to wav","text":"<p>We like to store our audios in aac format due to its smaller size. However, we do need the audio files to be of <code>*.wav</code> format for data loading. Hence, we also provided a conversion script that handles this. Simply specify your input directory containing the audios, in the folder structure shown above, and run the script.</p> <pre><code>export INPUT_DIR=\"dropbox/\"\nexport OUTPUT_DIR=\"training/\"\n</code></pre> <pre><code>python scripts/aac_to_wav.py --input_dir=$INPUT_DIR\n</code></pre>"},{"location":"guides/google-cloud/#run-speechline","title":"Run SpeechLine","text":"<p>Finally, you should be able to run the end-to-end SpeechLine pipeline. All you need to specify are the input and output directories, and the configuration file. You can use the pre-configured file in <code>examples/config.json</code>, or modify based on your requirements.</p> <pre><code>python speechline/run.py --input_dir=$INPUT_DIR --output_dir=$OUTPUT_DIR --config=\"examples/config.json\"\n</code></pre> <p>This should generate the resultant classified, transcribed, and chunked audios in the specified <code>$OUTPUT_DIR</code>. An example would be:</p> <pre><code>train/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 utt_0.wav\n    \u251c\u2500\u2500 utt_0.tsv\n    ...\n\u251c\u2500\u2500 id-id\n    \u251c\u2500\u2500 utt_1.wav\n    \u251c\u2500\u2500 utt_1.tsv\n    ...\n</code></pre> <p>A structure like shown above, with the right formats, should be easier to be ingested to a different training framework like Kaldi -- which was the main purpose of developing SpeechLine.</p>"},{"location":"guides/google-cloud/#optional-log-data-to-airtable","title":"(Optional) Log Data to AirTable","text":"<p>To help monitor the trend of our data collection and training data generation, we like to log our results to AirTable. We are going to write a separate guide on how to set the correct table up, but once that's done and you've logged in, you can simply run the following scripts to log the durations of both raw and generated data.</p> <pre><code>export AIRTABLE_URL=\"https://api.airtable.com/v0/URL/TABLE\"\nexport AIRTABLE_API_KEY=\"MY_API_KEY\"\npython scripts/data_logger.py --url=$AIRTABLE_URL --input_dir=$OUTPUT_DIR --label=\"training\"\npython scripts/data_logger.py --url=$AIRTABLE_URL --input_dir=$INPUT_DIR --label=\"archive\"\n</code></pre> <p>With the logged data, you can analyze how much raw data came in, and how much training data is generated, on a per-language basis.</p>"},{"location":"guides/pipeline/","title":"SpeechLine Pipeline Explained","text":"<p>This guide will explain the SpeechLine pipeline (<code>speechline/run.py</code>) in detail: what it does, the different components, and how you can customize it's behaviour.</p> <p>Info</p> <p>This guide was written for SpeechLine v0.0.2. There may be changes in the API in the coming releases.</p>"},{"location":"guides/pipeline/#overview","title":"Overview","text":"<p>As shown in the diagram above, SpeechLine will:</p> <ol> <li>Load raw audio data (in a specific folder structure).</li> <li>Format raw data as a Pandas DataFrame, with additional metadata.</li> <li>Convert the Pandas DataFrame into a HuggingFace <code>Dataset</code> object.</li> <li>Pass the <code>Dataset</code> into a child/adult audio classifier (optional).</li> <li>Pass the <code>Dataset</code> into an audio transcriber.</li> <li>Segment audio into smaller chunks, with customizable strategies.</li> <li>Export audio chunks into clean audio-transcript pairs.</li> </ol>"},{"location":"guides/pipeline/#1-data-loading","title":"1. Data Loading","text":"<p>SpeechLine expects raw audio data to be structured as such:</p> <pre><code>{INPUT_DIR}\n\u251c\u2500\u2500 {lang-region}\n    \u251c\u2500\u2500 {speaker}_{utterance}.wav\n    \u251c\u2500\u2500 {speaker}_{utterance}.txt\n    ...\n\u251c\u2500\u2500 {lang-region}\n    \u251c\u2500\u2500 {speaker}_{utterance}.wav\n    \u251c\u2500\u2500 {speaker}_{utterance}.txt\n    ...\n...\n</code></pre> <p>where audio are grouped by their languages, in <code>.wav</code> format, and could be of any sample rate (they will be resampled on-the-fly, if needed). Audio files must have the naming convention of <code>{speaker}_{utterance}.wav</code>.</p> <p>Optionally, they could have their corresponding ground truth transcripts in a separate <code>.txt</code> file of the same name, if you'd like to later segment it with <code>Word Overlap Segmenter</code>.</p> <p>For example, it should look something like</p> <pre><code>dropbox/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 bob_001.wav\n    \u251c\u2500\u2500 bob_001.txt\n    \u251c\u2500\u2500 bob_002.wav\n    \u251c\u2500\u2500 alice_001.wav\n    \u251c\u2500\u2500 alice_001.txt\n    \u2514\u2500\u2500 alice_002.wav\n\u2514\u2500\u2500 en-au\n    \u251c\u2500\u2500 tom_001.wav\n    \u2514\u2500\u2500 tom_001.txt\n</code></pre>"},{"location":"guides/pipeline/#2-pandas-dataframe","title":"2. Pandas DataFrame","text":"<p>If you have structure you're files as outlined above, it will then be loaded as a Pandas DataFrame with the following structure:</p> <code>audio</code> <code>id</code> <code>language</code> <code>language_code</code> <code>ground_truth</code> dropbox/en-us/bob_001.wav bob_001 en en-us hi, my name is bob. dropbox/en-us/bob_002.wav bob_002 en en-us dropbox/en-us/alice_001.wav alice_001 en en-us hey how are you? dropbox/en-us/alice_002.wav alice_002 en en-us dropbox/en-au/alice_001.wav tom_001 en en-au zom is eating bread. <p>Notice that there are several entries of <code>ground_truth</code> which are empty. That's because the audio files does not have a corresponding <code>.txt</code> file, hence it's assumed that the audio has no expected ground truth text which was spoken.</p> <p>Warning</p> <p>If you're going to segment with the <code>Silence Segmenter</code>, this is perfectly fine as it does not rely on a separate ground truth text to verify the transcriber's outputs. However, if you're going to use <code>Word Overlap Segmenter</code>, it will immediately assume that your audio is empty, as assumed from the ground truth.</p> <p>Moreover, in the Config, you could specify <code>\"filter_empty_transcript\": true</code>, and it will immediately drop all audios that have empty transcripts. These will then be ignored in the next stage of the pipeline.</p> <p>More details can be found in <code>Audio Dataset</code>.</p>"},{"location":"guides/pipeline/#3-huggingface-dataset","title":"3. HuggingFace <code>Dataset</code>","text":"<p>The conversion from a Pandas DataFrame to a HuggingFace <code>Dataset</code> is as simple as</p> <pre><code>dataset: Dataset = Dataset.from_pandas(df)\n</code></pre> <p>and that it does absolutely nothing other than interfacing the DataFrame into a format that HuggingFace recognizes.</p> <p>Though it actually does one more thing behind the scene. Namely, it takes the <code>audio</code> column and tells HuggingFace that it's actually going to be an audio file. <code>Datasets</code> will later automatically read the audio file as an audio array, and even resample it if necessary. But upfront, it does not need extra attention.</p>"},{"location":"guides/pipeline/#4-audio-classification","title":"4. Audio Classification","text":"<p>If you have set <code>\"do_classify\": true</code> in your <code>config.json</code> file, SpeechLine will classify your audio files as adult or child audio. Subsequently, only child-classified audio will be selected.</p> <p>You can also change the classifier model (by specifying the its Huggingface Hub model checkpoint) and its configuration in <code>config.json</code>, like so:</p> <pre><code>{\n    \"do_classify\": true,\n    \"classifier\": {\n        \"model\": \"bookbot/distil-wav2vec2-adult-child-cls-52m\",\n        \"max_duration_s\": 3.0 // (1)\n    }\n}\n</code></pre> <ol> <li>This refers to the maximum length to which your audio tensors were padded to during training, but in seconds. So if your sampling rate is 16kHz and you padded your tensors to a maximum length of 16000, then <code>max_duration_s = 1.0</code>. This follows the preprocessing explained here.</li> </ol> <p>Your custom classification model must have a <code>\"child\"</code> label, which will be used to filter the DataFrame like so:</p> <pre><code>df = df[df[\"category\"] == \"child\"]\n</code></pre>"},{"location":"guides/pipeline/#5-audio-transcription","title":"5. Audio Transcription","text":"<p>Once the data preprocessing and the optional classification are complete, SpeechLine will proceed to transcribe your audio files. SpeechLine supports 2 transcription models, namely <code>wav2vec</code> or <code>whisper</code>. You will have to specify the <code>return_timestamps</code> config that is unique to both of these models. An example trascriber config may look like this:</p> <pre><code>{\n    \"transcriber\": {\n        \"type\": \"wav2vec2\", // (1)\n        \"model\": \"bookbot/wav2vec2-bookbot-en-lm\", // (2)\n        \"return_timestamps\": \"word\", // (3)\n        \"chunk_length_s\": 30 // (4)\n    },\n}\n</code></pre> <ol> <li>Type of model, either <code>wav2vec2</code> or <code>whisper</code>. Additional architectures might be supported in future versions of SpeechLine.</li> <li>HuggingFace model checkpoint found in the Hub.</li> <li>Timestamp level. Either <code>word</code> or <code>char</code> for wav2vec 2.0, or <code>True</code> for Whisper.</li> <li>Maximum length of audio chunk for long-audio transcription.</li> </ol> <p>For <code>wav2vec2</code>, you have the option to use either <code>char</code> or <code>word</code>. This depends on the type of transcriber model that you have and subsequently the type of label that you want the output audio to have.</p> <p>If you have the usual word-level wav2vec2 model like Wav2Vec2-Base-960h, and want your output audio to have word-level timestamps, then set <code>\"return_timestamps\": \"word\"</code>. It will result in timestamps that look like:</p> <pre><code>[\n  {\n    \"end_time\": 1.7,\n    \"start_time\": 1.68,\n    \"text\": \"A\"\n  },\n  {\n    \"end_time\": 2.72,\n    \"start_time\": 2.64,\n    \"text\": \"YOU\"\n  },\n  {\n    \"end_time\": 2.96,\n    \"start_time\": 2.82,\n    \"text\": \"SEE\"\n  },\n  {\n    \"end_time\": 3.4,\n    \"start_time\": 3.32,\n    \"text\": \"AN\"\n  }\n]\n</code></pre> <p>Otherwise, if you want character-level timetamps, then set <code>\"return_timestamps\": \"char\"</code>. It will result in timestamps that look like:</p> <pre><code>[\n  {\n    \"text\": \"f\",\n    \"start_time\": 1.62,\n    \"end_time\": 1.66\n  },\n  {\n    \"text\": \"i\",\n    \"start_time\": 1.7,\n    \"end_time\": 1.72\n  },\n  {\n    \"text\": \"t\",\n    \"start_time\": 1.78,\n    \"end_time\": 1.8\n  },\n  {\n    \"text\": \"p\",\n    \"start_time\": 1.8,\n    \"end_time\": 1.82\n  },\n  {\n    \"text\": \"l\",\n    \"start_time\": 1.86,\n    \"end_time\": 1.88\n  },\n  {\n    \"text\": \"i\",\n    \"start_time\": 1.92,\n    \"end_time\": 1.94\n  }\n]\n</code></pre> <p>If you use <code>whisper</code>, set <code>return_timestamps</code> to <code>True</code>. Whisper, by design, only supports utterance-level or sentence-level timestamps. So expect that your timestamps will also therefore be sentence-level. It may look something like:</p> <pre><code>[\n  {\n    \"end_time\": 1.7,\n    \"start_time\": 6.82,\n    \"text\": \"The quick brown fox jumps over the lazy dog.\"\n  },\n  {\n    \"end_time\": 7.01,\n    \"start_time\": 10.5,\n    \"text\": \"Never gonna give you up.\"\n  }\n]\n</code></pre> <p>The remaining transcriber config is the chunk length in seconds. This is used in transcribing long audios that may cause out-of-memory issues. We won't discuss it in detail, but you can read more about it here. Simply put, this is the maximum audio duration that the transcriber will infer. Anything more than that, then the audio will be divided into chunks of even lengths, and the transcription will be the joint predictions of each chunk.</p>"},{"location":"guides/pipeline/#6-audio-segmentation","title":"6. Audio Segmentation","text":"<p>After transcription, SpeechLine will segment your audio files into smaller chunks. SpeechLine employs 2 different segmentation strategies, namely through silence gaps or word overlaps.</p> <p>To segment based on silence gaps, simply set <code>\"type\": \"silence\"</code> and specify the <code>silence_duration</code> (in seconds) in the segmenter config. Here is an example of segmenter config with the silence gap strategy:</p> <pre><code>{\n    \"segmenter\": {\n        \"type\": \"silence\",\n        \"silence_duration\": 3.0,\n        \"minimum_chunk_duration\": 1.0\n    }\n}\n</code></pre> <p>The config above will tell SpeechLine to split your audio files on every point of at least 3-second silence gaps. </p> <p></p> <p>In the diagram above, the audio will be split into two chunks because there is at least a 3-second silence gap in between the two chunks. One chunk will therefore contain the audio for <code>\"hello world\"</code> and the other <code>\"hi my name is\"</code>.</p> <p>Alternatively, you can set <code>\"type\": \"word_overlap\"</code> to use to word overlap segmenation strategy. Here is an example of such config:</p> <pre><code>{\n    \"segmenter\": {\n        \"type\": \"word_overlap\",\n        \"minimum_chunk_duration\": 1.0\n    }\n}\n</code></pre> <p>As explained in the previous sections, this method relies on having a corresponding \"ground truth\" text, or what the audio is expected to contain. </p> <p></p> <p>In the diagram above, the algorithm matches <code>\"zom is\"</code> to its corresponding segment that the transcriber predicts as <code>\"zom is\"</code>, and the same for <code>\"bread\"</code>. However, since there is no match for the word <code>\"eating\"</code>, the segment in between will not be accepted. There might be two possibilities for this:</p> <ol> <li>The speaker mis-said 'eating' as 'feeding', or</li> <li>The transcriber mis-labeled 'eating' as 'feeding'.</li> </ol> <p>For the case (1), this is perfectly fine as we don't want to produce incorrect labels in the resultant output data, and word overlap ensures cases like these are filtered. However, if case (2) is true, then our transcriber is inaccurate.</p> <p>Info</p> <p>For the latter, this can only be fixed by using a more accurate transcription model, and fine-tuning wav2vec2 on your audio domain should improve it for this purpose.</p> <p>Ultimately, word overlap is more strict in a sense that it requires an agreement between the expected ground truth and the transcriber. As per our experiments, this resulted in a dataset of higher quality. The only cases where this is unreliable is if our transcriber predicts a false positive, but this is seemingly quite rare.</p> <p>More details can be found in <code>Silence Segmenter</code> and <code>Word Overlap Segmenter</code>.</p>"},{"location":"guides/pipeline/#7-chunk-exporting","title":"7. Chunk Exporting","text":"<p>Lastly, <code>\"minimum_chunk_duration\"</code> refers to the minimum chunk duration (in seconds) to be exported.</p> <p></p> <p>Continuing the example and using the config above, the second segment will not be exported since it is less than the minimum 1.0 second. On the contrary, the first segment is longer than 1.0 second and will therefore be exported.</p>"},{"location":"guides/pipeline/#resultant-output-dataset","title":"Resultant Output Dataset","text":"<p>Once the pipeline has finished running, you will end up with an output directory that looks like the following:</p> <pre><code>{OUTPUT_DIR}\n\u251c\u2500\u2500 {lang-region}\n    \u251c\u2500\u2500 {speaker}_{utterance}.wav\n    \u251c\u2500\u2500 {speaker}_{utterance}.tsv\n    ...\n\u251c\u2500\u2500 {lang-region}\n    \u251c\u2500\u2500 {speaker}_{utterance}.wav\n    \u251c\u2500\u2500 {speaker}_{utterance}.tsv\n    ...\n...\n</code></pre> <p>For example:</p> <pre><code>training/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 bob_001.wav\n    \u2514\u2500\u2500 bob_001.tsv\n\u2514\u2500\u2500 en-au\n    \u251c\u2500\u2500 tom_001.wav\n    \u2514\u2500\u2500 tom_001.tsv\n</code></pre> <p>The <code>.wav</code> file corresponds to the exported chunks, while the <code>.tsv</code> file contains the timestamped-transcript that is of the format:</p> <pre><code>{start_time_in_secs}\\t{end_time_in_secs}\\t{label}\n</code></pre> <p>For example:</p> <pre><code>12.28   12.42   THIS\n12.54   12.9    POINTED\n13.06   13.44   STRAIGHT\n13.66   13.72   AT\n13.82   13.96   HIM\n14.92   15.02   ONE\n15.14   15.34   GOOD\n15.58   15.78   SHE\n16.0    16.26   SHOULD\n16.4    16.72   DEAL\n16.94   17.14   HIM\n</code></pre> <p>which you can then pass to other models to train with.</p>"},{"location":"guides/sagemaker/","title":"SpeechLine on AWS SageMaker","text":"<p>SpeechLine is a platform-independent framework. You should be able to run SpeechLine on any platform, even without GPU. However, we developed SpeechLine with the AWS ecosystem in mind, as SpeechLine is an actual framework we use in Bookbot.</p> <p>In this guide, we are going to walk you through all the steps of running SpeehLine on AWS SageMaker, with additional utility scripts involving other services such as S3, and AirTable for loggings. While they are readily available in SpeechLine, they are not required to run the end-to-end pipeline.</p>"},{"location":"guides/sagemaker/#create-sagemaker-instance","title":"Create SageMaker Instance","text":"<p>Because we are only going to perform inference on GPU, we don't really need a GPU with high VRAM. Because of that, we prefer to run the cheapest GPU instance on SageMaker, <code>ml.g4dn.xlarge</code>. It comes with 1 NVIDIA T4, which has up to 16GB of VRAM. This is sufficient to run both classification and transcription with a reasonable batch size. Simply spin one up via the AWS Web Console, and enter Jupyter Lab.</p>"},{"location":"guides/sagemaker/#install-linux-packages","title":"Install Linux Packages","text":"<p>We need to install all dependencies of SpeechLine. We'll begin with ffmpeg and ffprobe. Since the VM runs on AWS Linux, it does not have access to <code>apt</code>. For this, we have to resort to download static builds of ffmpeg and ffprobe. You can follow this guide which explains how to install ffmpeg on AWS Linux. We're going to modify several of their steps.</p> <p>First, let's move to <code>/usr/local/bin</code>, download the latest static build of ffmpeg (with ffprobe), unzip the file, and create a symbolic link of the runnables to <code>/usr/bin</code>:</p> <pre><code>cd /usr/local/bin\nsudo wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\nsudo tar -xf ffmpeg-release-amd64-static.tar.xz\nsudo ln -s /usr/local/bin/ffmpeg-5.1.1-amd64-static/ffmpeg /usr/bin/ffmpeg # (1)\nsudo ln -s /usr/local/bin/ffmpeg-5.1.1-amd64-static/ffprobe /usr/bin/ffprobe # (2)\n</code></pre> <ol> <li>The latest version of ffmpeg might be different from this guide's. Replace <code>ffmpeg-5.1.1-amd64-static</code> with the latest version that you downloaded.</li> <li>The latest version of ffmpeg might be different from this guide's. Replace <code>ffmpeg-5.1.1-amd64-static</code> with the latest version that you downloaded.</li> </ol> <p>For cleanliness, let's delete the downloaded compressed file and move back to the working directory.</p> <pre><code>sudo rm ffmpeg-release-amd64-static.tar.xz\ncd /home/ec2-user/SageMaker\n</code></pre> <p>You can verify that the runnable is now accessible from anywhere by running</p> <pre><code>ffmpeg -version\nffprobe -version\n</code></pre>"},{"location":"guides/sagemaker/#install-speechline","title":"Install SpeechLine","text":"<p>At the time of writing, SpeechLine is yet to be availabe on PyPI. We have to manually clone the source code and install it from there. Also, for convenience, we'll be using the latest PyTorch Conda environment provided for us out-of-the-box.</p> <pre><code>source activate amazonei_pytorch_latest_p37\ngit clone https://github.com/bookbot-kids/speechline.git\ncd speechline\npip install .\n</code></pre>"},{"location":"guides/sagemaker/#optional-download-audio-data-from-s3","title":"(Optional) Download Audio Data from S3","text":"<p>If your files are located on AWS S3 and you need to bulk download an entire folder on S3, you can run the <code>download_s3_bucket</code> script located under <code>scripts</code>. Simply specify the bucket's name, prefix to the folder in the bucket, and the output directory. For ease, let's export some variables which we can reuse later down the line.</p> <pre><code>export BUCKET_NAME=\"my-bucket\"\nexport INPUT_DIR=\"dropbox/\"\nexport OUTPUT_DIR=\"train/\"\n</code></pre> <pre><code>python scripts/download_s3_bucket.py --bucket=$BUCKET_NAME --prefix=$INPUT_DIR --output_dir=$INPUT_DIR\n</code></pre> <p>Moreover, SpeechLine has a specific requirement on how the folder structure should be. Because we would want to group the files into their specific languages to access the correct models, it should be of the following structure:</p> <pre><code>path_to_files\n\u251c\u2500\u2500 langX\n\u2502   \u251c\u2500\u2500 a.wav\n\u2502   \u251c\u2500\u2500 a.txt\n\u2502   \u251c\u2500\u2500 b.wav\n\u2502   \u2514\u2500\u2500 b.txt\n\u2514\u2500\u2500 langY\n    \u251c\u2500\u2500 c.wav\n    \u2514\u2500\u2500 c.txt\n</code></pre> <p>where <code>*.txt</code> files are the ground truth transcripts of the utterance <code>*.wav</code> files. Note that the <code>*.txt</code> files are optional -- the script will still run just fine if you don't have them.</p> <p>For instance, your directory should look like this:</p> <pre><code>dropbox/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 utt_0.wav\n    \u251c\u2500\u2500 utt_0.txt\n    ...\n\u251c\u2500\u2500 id-id\n    \u251c\u2500\u2500 utt_1.wav\n    \u251c\u2500\u2500 utt_1.txt\n    ...\n...\n</code></pre>"},{"location":"guides/sagemaker/#optional-convert-aac-audios-to-wav","title":"(Optional) Convert aac Audios to wav","text":"<p>We like to store our audios in aac format due to its smaller size. However, we do need the audio files to be of <code>*.wav</code> format for data loading. Hence, we also provided a conversion script that handles this. Simply specify your input directory containing the audios, in the folder structure shown above, and run the script.</p> <pre><code>python scripts/aac_to_wav.py --input_dir=$INPUT_DIR\n</code></pre>"},{"location":"guides/sagemaker/#run-speechline","title":"Run SpeechLine","text":"<p>Finally, you should be able to run the end-to-end SpeechLine pipeline. All you need to specify are the input and output directories, and the configuration file. You can use the pre-configured file in <code>examples/config.json</code>, or modify based on your requirements.</p> <pre><code>python speechline/run.py --input_dir=$INPUT_DIR --output_dir=$OUTPUT_DIR --config=\"examples/config.json\"\n</code></pre> <p>This should generate the resultant classified, transcribed, and chunked audios in the specified <code>$OUTPUT_DIR</code>. An example would be:</p> <pre><code>train/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 utt_0.wav\n    \u251c\u2500\u2500 utt_0.tsv\n    ...\n\u251c\u2500\u2500 id-id\n    \u251c\u2500\u2500 utt_1.wav\n    \u251c\u2500\u2500 utt_1.tsv\n    ...\n</code></pre> <p>A structure like shown above, with the right formats, should be easier to be ingested to a different training framework like Kaldi -- which was the main purpose of developing SpeechLine.</p>"},{"location":"guides/sagemaker/#optional-upload-audio-data-to-s3","title":"(Optional) Upload Audio Data to S3","text":"<p>Continuing our requirement of storing files in S3, we have similarly provided another script to upload files back to S3 in parallel. Like downloading, uploading simply requires the bucket name, the input directory, and a folder prefix under which the files will be stored.</p> <pre><code>python scripts/upload_s3_bucket.py --bucket=$BUCKET_NAME --prefix=$OUTPUT_DIR --input_dir=$OUTPUT_DIR\n</code></pre> <p>This will upload files with the following path:</p> <pre><code>s3://{BUCKET_NAME}/{PREFIX}/{LANGUAGE}/*.{wav,tsv}\n</code></pre>"},{"location":"guides/sagemaker/#optional-log-data-to-airtable","title":"(Optional) Log Data to AirTable","text":"<p>To help monitor the trend of our data collection and training data generation, we like to log our results to AirTable. We are going to write a separate guide on how to set the correct table up, but once that's done and you've logged in, you can simply run the following scripts to log the durations of both raw and generated data.</p> <pre><code>export AIRTABLE_URL=\"https://api.airtable.com/v0/URL/TABLE\"\nexport AIRTABLE_API_KEY=\"MY_API_KEY\"\npython scripts/data_logger.py --url=$AIRTABLE_URL --input_dir=$OUTPUT_DIR --label=\"training\"\npython scripts/data_logger.py --url=$AIRTABLE_URL --input_dir=$INPUT_DIR --label=\"archive\"\n</code></pre> <p>With the logged data, you can analyze how much raw data came in, and how much training data is generated, on a per-language basis.</p>"},{"location":"guides/transcribe/","title":"Performing Audio Transcription","text":"<p>This guide will explain how to transcribe audio files using SpeechLine.</p> <p>First, load in the your transcription model by passing its Hugging Face model checkpoint into <code>Wav2Vec2Transcriber</code>.</p> <pre><code>from speechline.transcribers import Wav2Vec2Transcriber\n\ntranscriber = Wav2Vec2Transcriber(\"bookbot/wav2vec2-ljspeech-gruut\")\n</code></pre> <p>Next, you will need to transform your input audio file (given by <code>sample.wav</code>) into a <code>Dataset</code> format like the following</p> <pre><code>from datasets import Dataset, Audio\n\ndataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]})\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=transcriber.sampling_rate))\n</code></pre> <p>Once preprocessing is finished, simply pass the input data into the transcriber.</p> <pre><code>phoneme_offsets = transcriber.predict(dataset, output_offsets=True, return_timestamps=\"char\")\n</code></pre> <pre><code>Transcribing Audios:   0%|          | 0/1 [00:00&lt;?, ?ex/s]\n</code></pre> <p>The output format of the transcription model is shown below. It is a list of dictionary containing the transcribed <code>text</code>, <code>start_time</code> and <code>end_time</code> stamps of the corresponding phoneme token.</p> <pre><code>phoneme_offsets\n</code></pre> <pre><code>[[{'end_time': 0.02, 'start_time': 0.0, 'text': '\u026a'},\n  {'end_time': 0.3, 'start_time': 0.26, 'text': 't'},\n  {'end_time': 0.36, 'start_time': 0.34, 'text': '\u026a'},\n  {'end_time': 0.44, 'start_time': 0.42, 'text': 'z'},\n  {'end_time': 0.54, 'start_time': 0.5, 'text': 'n'},\n  {'end_time': 0.58, 'start_time': 0.54, 'text': 'o\u028a'},\n  {'end_time': 0.62, 'start_time': 0.58, 'text': 't'},\n  {'end_time': 0.78, 'start_time': 0.76, 'text': '\u028c'},\n  {'end_time': 0.94, 'start_time': 0.92, 'text': 'p'}]]\n</code></pre> <p>You can manually check the model output by playing a segment (using the start and end timestamps) of your input audio file. </p> <p>First, load your audio file.</p> <pre><code>from pydub import AudioSegment\n\naudio = AudioSegment.from_file(\"sample.wav\")\naudio\n</code></pre> <p>      Your browser does not support the audio element. </p> <p>You can use the following function to play a segment of your audio from a given offset</p> <pre><code>def play_segment(offsets, index: int):\n    start = offsets[index][\"start_time\"]\n    end = offsets[index][\"end_time\"]\n    print(offsets[index][\"text\"])\n    return audio[start * 1000 : end * 1000]\n</code></pre> <p>Here are some examples of the phoneme segments</p> <pre><code>play_segment(phoneme_offsets[0], 0)\n</code></pre> <pre><code>\u026a\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 1)\n</code></pre> <pre><code>t\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 2)\n</code></pre> <pre><code>\u026a\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 3)\n</code></pre> <pre><code>z\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 4)\n</code></pre> <pre><code>n\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 5)\n</code></pre> <pre><code>o\u028a\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 6)\n</code></pre> <pre><code>t\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 7)\n</code></pre> <pre><code>\u028c\n</code></pre> <p>      Your browser does not support the audio element. </p> <pre><code>play_segment(phoneme_offsets[0], 8)\n</code></pre> <pre><code>p\n</code></pre> <p>      Your browser does not support the audio element. </p>"},{"location":"reference/config/","title":"Config","text":""},{"location":"reference/config/#example-config-file","title":"Example Config File","text":"example_config.json<pre><code>{\n    \"do_classify\": true,\n    \"filter_empty_transcript\": true,\n    \"classifier\": {\n        \"model\": \"bookbot/distil-wav2vec2-adult-child-cls-52m\",\n        \"max_duration_s\": 3.0\n    },\n    \"transcriber\": {\n        \"type\": \"wav2vec2\",\n        \"model\": \"bookbot/wav2vec2-bookbot-en-lm\",\n        \"return_timestamps\": \"word\",\n        \"chunk_length_s\": 30\n    },\n    \"do_noise_classify\": true,\n    \"noise_classifier\": {\n        \"model\": \"bookbot/distil-ast-audioset\",\n        \"minimum_empty_duration\": 0.3,\n        \"threshold\": 0.2\n    },\n    \"segmenter\": {\n        \"type\": \"word_overlap\",\n        \"minimum_chunk_duration\": 1.0\n    }\n}\n</code></pre>"},{"location":"reference/config/#speechline.config.NoiseClassifierConfig","title":"<code> speechline.config.NoiseClassifierConfig        </code>  <code>dataclass</code>","text":"<p>Noise classifier config.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>HuggingFace Hub model hub checkpoint.</p> required <code>min_empty_duration</code> <code>float</code> <p>Minimum non-transcribed segment duration to be segmented, and passed to noise classifier. Defaults to <code>1.0</code> seconds.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold for the multi label classification. Defaults to <code>0.3</code>.</p> <code>0.3</code> <code>batch_size</code> <code>int</code> <p>Batch size during inference. Defaults to <code>1</code>.</p> <code>1</code> Source code in <code>speechline/config.py</code> <pre><code>class NoiseClassifierConfig:\n    \"\"\"\n    Noise classifier config.\n\n    Args:\n        model (str):\n            HuggingFace Hub model hub checkpoint.\n        min_empty_duration (float, optional):\n            Minimum non-transcribed segment duration to be segmented,\n            and passed to noise classifier.\n            Defaults to `1.0` seconds.\n        threshold (float, optional):\n            The probability threshold for the multi label classification.\n            Defaults to `0.3`.\n        batch_size (int, optional):\n            Batch size during inference. Defaults to `1`.\n\n    \"\"\"\n\n    model: str\n    minimum_empty_duration: float = 1.0\n    threshold: float = 0.3\n    batch_size: int = 1\n</code></pre>"},{"location":"reference/config/#speechline.config.ClassifierConfig","title":"<code> speechline.config.ClassifierConfig        </code>  <code>dataclass</code>","text":"<p>Audio classifier config.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>HuggingFace Hub model hub checkpoint.</p> required <code>max_duration_s</code> <code>float</code> <p>Maximum audio duration for padding. Defaults to <code>3.0</code> seconds.</p> <code>3.0</code> <code>batch_size</code> <code>int</code> <p>Batch size during inference. Defaults to <code>1</code>.</p> <code>1</code> Source code in <code>speechline/config.py</code> <pre><code>class ClassifierConfig:\n    \"\"\"\n    Audio classifier config.\n\n    Args:\n        model (str):\n            HuggingFace Hub model hub checkpoint.\n        max_duration_s (float, optional):\n            Maximum audio duration for padding. Defaults to `3.0` seconds.\n        batch_size (int, optional):\n            Batch size during inference. Defaults to `1`.\n    \"\"\"\n\n    model: str\n    max_duration_s: float = 3.0\n    batch_size: int = 1\n</code></pre>"},{"location":"reference/config/#speechline.config.TranscriberConfig","title":"<code> speechline.config.TranscriberConfig        </code>  <code>dataclass</code>","text":"<p>Audio transcriber config.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Transcriber model architecture type.</p> required <code>model</code> <code>str</code> <p>HuggingFace Hub model hub checkpoint.</p> required <code>return_timestamps</code> <code>Union[str, bool]</code> <p><code>return_timestamps</code> argument in <code>AutomaticSpeechRecognitionPipeline</code>'s <code>__call__</code> method. Use <code>\"char\"</code> for CTC-based models and <code>True</code> for Whisper-based models.</p> required <code>chunk_length_s</code> <code>int</code> <p>Audio chunk length in seconds.</p> required Source code in <code>speechline/config.py</code> <pre><code>class TranscriberConfig:\n    \"\"\"\n    Audio transcriber config.\n\n    Args:\n        type (str):\n            Transcriber model architecture type.\n        model (str):\n            HuggingFace Hub model hub checkpoint.\n        return_timestamps (Union[str, bool]):\n            `return_timestamps` argument in `AutomaticSpeechRecognitionPipeline`'s\n            `__call__` method. Use `\"char\"` for CTC-based models and\n            `True` for Whisper-based models.\n        chunk_length_s (int):\n            Audio chunk length in seconds.\n    \"\"\"\n\n    type: str\n    model: str\n    return_timestamps: Union[str, bool]\n    chunk_length_s: int\n\n    def __post_init__(self):\n        SUPPORTED_MODELS = {\"wav2vec2\", \"whisper\"}\n        WAV2VEC_TIMESTAMPS = {\"word\", \"char\"}\n\n        if self.type not in SUPPORTED_MODELS:\n            raise ValueError(f\"Transcriber of type {self.type} is not yet supported!\")\n\n        if self.type == \"wav2vec2\" and self.return_timestamps not in WAV2VEC_TIMESTAMPS:\n            raise ValueError(\"wav2vec2 only supports `'word'` or `'char'` timestamps!\")\n        elif self.type == \"whisper\" and self.return_timestamps is not True:\n            raise ValueError(\"Whisper only supports `True` timestamps!\")\n</code></pre>"},{"location":"reference/config/#speechline.config.SegmenterConfig","title":"<code> speechline.config.SegmenterConfig        </code>  <code>dataclass</code>","text":"<p>Audio segmenter config.</p> <p>Parameters:</p> Name Type Description Default <code>silence_duration</code> <code>float</code> <p>Minimum in-between silence duration (in seconds) to consider as gaps. Defaults to <code>3.0</code> seconds.</p> <code>0.0</code> <code>minimum_chunk_duration</code> <code>float</code> <p>Minimum chunk duration (in seconds) to be exported. Defaults to 0.2 second.</p> <code>0.2</code> <code>lexicon_path</code> <code>str</code> <p>Path to lexicon file. Defaults to <code>None</code>.</p> <code>None</code> <code>keep_whitespace</code> <code>bool</code> <p>Whether to keep whitespace in transcript. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>speechline/config.py</code> <pre><code>class SegmenterConfig:\n    \"\"\"\n    Audio segmenter config.\n\n    Args:\n        silence_duration (float, optional):\n            Minimum in-between silence duration (in seconds) to consider as gaps.\n            Defaults to `3.0` seconds.\n        minimum_chunk_duration (float, optional):\n            Minimum chunk duration (in seconds) to be exported.\n            Defaults to 0.2 second.\n        lexicon_path (str, optional):\n            Path to lexicon file. Defaults to `None`.\n        keep_whitespace (bool, optional):\n            Whether to keep whitespace in transcript. Defaults to `False`.\n    \"\"\"\n\n    type: str\n    silence_duration: float = 0.0\n    minimum_chunk_duration: float = 0.2\n    lexicon_path: str = None\n    keep_whitespace: bool = False\n\n    def __post_init__(self):\n        SUPPORTED_TYPES = {\"silence\", \"word_overlap\", \"phoneme_overlap\"}\n\n        if self.type not in SUPPORTED_TYPES:\n            raise ValueError(f\"Segmenter of type {self.type} is not yet supported!\")\n</code></pre>"},{"location":"reference/config/#speechline.config.Config","title":"<code> speechline.config.Config        </code>  <code>dataclass</code>","text":"<p>Main SpeechLine config, contains all other subconfigs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to JSON config file.</p> required Source code in <code>speechline/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Main SpeechLine config, contains all other subconfigs.\n\n    Args:\n        path (str):\n            Path to JSON config file.\n    \"\"\"\n\n    path: str\n\n    def __post_init__(self):\n        config = json.load(open(self.path))\n        self.do_classify = config.get(\"do_classify\", False)\n        self.do_noise_classify = config.get(\"do_noise_classify\", False)\n        self.filter_empty_transcript = config.get(\"filter_empty_transcript\", False)\n\n        if self.do_classify:\n            self.classifier = ClassifierConfig(**config[\"classifier\"])\n\n        if self.do_noise_classify:\n            self.noise_classifier = NoiseClassifierConfig(**config[\"noise_classifier\"])\n\n        self.transcriber = TranscriberConfig(**config[\"transcriber\"])\n        self.segmenter = SegmenterConfig(**config[\"segmenter\"])\n</code></pre>"},{"location":"reference/runner/","title":"Runner","text":""},{"location":"reference/runner/#speechline.run.Runner","title":"<code> speechline.run.Runner        </code>  <code>dataclass</code>","text":"<p>Runner()</p> Source code in <code>speechline/run.py</code> <pre><code>class Runner:\n    @staticmethod\n    def parse_args(args: List[str]) -&gt; argparse.Namespace:\n        \"\"\"\n        Utility argument parser function for SpeechLine.\n\n        Args:\n            args (List[str]):\n                List of arguments.\n\n        Returns:\n            argparse.Namespace:\n                Objects with arguments values as attributes.\n        \"\"\"\n        parser = argparse.ArgumentParser(\n            prog=\"python speechline/run.py\",\n            description=\"Perform end-to-end speech labeling pipeline.\",\n        )\n\n        parser.add_argument(\n            \"-i\",\n            \"--input_dir\",\n            type=str,\n            required=True,\n            help=\"Directory of input audios.\",\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output_dir\",\n            type=str,\n            required=True,\n            help=\"Directory to save pipeline results.\",\n        )\n        parser.add_argument(\n            \"-c\",\n            \"--config\",\n            type=str,\n            default=\"examples/config.json\",\n            help=\"SpeechLine configuration file.\",\n        )\n        return parser.parse_args(args)\n\n    @staticmethod\n    def run(config: Config, input_dir: str, output_dir: str) -&gt; None:\n        \"\"\"\n        Runs end-to-end SpeechLine pipeline.\n\n        ### Pipeline Overview\n        - Classifies for children's speech audio (optional).\n        - Transcribes audio.\n        - Segments audio into chunks based on silences.\n\n        Args:\n            config (Config):\n                SpeechLine Config object.\n            input_dir (str):\n                Path to input directory.\n            output_dir (str):\n                Path to output directory.\n        \"\"\"\n        logger.info(\"Preparing DataFrame..\")\n        df = prepare_dataframe(input_dir, audio_extension=\"wav\")\n\n        if config.filter_empty_transcript:\n            df = df[df[\"ground_truth\"] != \"\"]\n\n        if config.do_classify:\n            # load classifier model\n            classifier = Wav2Vec2Classifier(\n                config.classifier.model,\n                max_duration_s=config.classifier.max_duration_s,\n            )\n\n            # perform audio classification\n            dataset = format_audio_dataset(df, sampling_rate=classifier.sampling_rate)\n            df[\"category\"] = classifier.predict(dataset)\n\n            # filter audio by category\n            df = df[df[\"category\"] == \"child\"]\n\n        # load transcriber model\n        if config.transcriber.type == \"wav2vec2\":\n            transcriber = Wav2Vec2Transcriber(config.transcriber.model)\n        elif config.transcriber.type == \"whisper\":\n            transcriber = WhisperTranscriber(config.transcriber.model)\n\n        # perform audio transcription\n        dataset = format_audio_dataset(df, sampling_rate=transcriber.sampling_rate)\n\n        output_offsets = transcriber.predict(\n            dataset,\n            chunk_length_s=config.transcriber.chunk_length_s,\n            output_offsets=True,\n            return_timestamps=config.transcriber.return_timestamps,\n            keep_whitespace=config.segmenter.keep_whitespace,\n        )\n\n        # segment audios based on offsets\n        if config.segmenter.type == \"silence\":\n            segmenter = SilenceSegmenter()\n        elif config.segmenter.type == \"word_overlap\":\n            segmenter = WordOverlapSegmenter()\n        elif config.segmenter.type == \"phoneme_overlap\":\n            lexicon = Lexicon()\n            if config.segmenter.lexicon_path:\n                with open(config.segmenter.lexicon_path) as json_file:\n                    lex = json.load(json_file)\n                # merge dict with lexicon\n                for k, v in lex.items():\n                    lexicon[k] = lexicon[k].union(set(v)) if k in lexicon else set(v)\n            segmenter = PhonemeOverlapSegmenter(lexicon)\n\n        tokenizer = WordTokenizer()\n\n        if config.do_noise_classify:\n            noise_classifier = config.noise_classifier.model\n            minimum_empty_duration = config.noise_classifier.minimum_empty_duration\n            noise_classifier_threshold = config.noise_classifier.threshold\n        else:\n            noise_classifier = None\n            minimum_empty_duration = None\n            noise_classifier_threshold = None\n\n        def export_and_chunk(\n            audio_path: str,\n            ground_truth: str,\n            offsets: List[Dict[str, Union[str, float]]],\n        ):\n            json_path = Path(audio_path).with_suffix(\".json\")\n            # export JSON transcripts\n            export_transcripts_json(str(json_path), offsets)\n            # chunk audio into segments\n            segmenter.chunk_audio_segments(\n                audio_path,\n                output_dir,\n                offsets,\n                do_noise_classify=config.do_noise_classify,\n                noise_classifier=noise_classifier,\n                minimum_empty_duration=minimum_empty_duration,\n                minimum_chunk_duration=config.segmenter.minimum_chunk_duration,\n                noise_classifier_threshold=noise_classifier_threshold,\n                silence_duration=config.segmenter.silence_duration,\n                ground_truth=tokenizer(ground_truth),\n            )\n\n        thread_map(\n            export_and_chunk,\n            df[\"audio\"],\n            df[\"ground_truth\"],\n            output_offsets,\n            desc=\"Segmenting Audio into Chunks\",\n            total=len(df),\n        )\n</code></pre>"},{"location":"reference/runner/#speechline.run.Runner.parse_args","title":"<code>parse_args(args)</code>  <code>staticmethod</code>","text":"<p>Utility argument parser function for SpeechLine.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>speechline/run.py</code> <pre><code>@staticmethod\ndef parse_args(args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for SpeechLine.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python speechline/run.py\",\n        description=\"Perform end-to-end speech labeling pipeline.\",\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Directory of input audios.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Directory to save pipeline results.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config\",\n        type=str,\n        default=\"examples/config.json\",\n        help=\"SpeechLine configuration file.\",\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/runner/#speechline.run.Runner.run","title":"<code>run(config, input_dir, output_dir)</code>  <code>staticmethod</code>","text":"<p>Runs end-to-end SpeechLine pipeline.</p>"},{"location":"reference/runner/#speechline.run.Runner.run--pipeline-overview","title":"Pipeline Overview","text":"<ul> <li>Classifies for children's speech audio (optional).</li> <li>Transcribes audio.</li> <li>Segments audio into chunks based on silences.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>SpeechLine Config object.</p> required <code>input_dir</code> <code>str</code> <p>Path to input directory.</p> required <code>output_dir</code> <code>str</code> <p>Path to output directory.</p> required Source code in <code>speechline/run.py</code> <pre><code>@staticmethod\ndef run(config: Config, input_dir: str, output_dir: str) -&gt; None:\n    \"\"\"\n    Runs end-to-end SpeechLine pipeline.\n\n    ### Pipeline Overview\n    - Classifies for children's speech audio (optional).\n    - Transcribes audio.\n    - Segments audio into chunks based on silences.\n\n    Args:\n        config (Config):\n            SpeechLine Config object.\n        input_dir (str):\n            Path to input directory.\n        output_dir (str):\n            Path to output directory.\n    \"\"\"\n    logger.info(\"Preparing DataFrame..\")\n    df = prepare_dataframe(input_dir, audio_extension=\"wav\")\n\n    if config.filter_empty_transcript:\n        df = df[df[\"ground_truth\"] != \"\"]\n\n    if config.do_classify:\n        # load classifier model\n        classifier = Wav2Vec2Classifier(\n            config.classifier.model,\n            max_duration_s=config.classifier.max_duration_s,\n        )\n\n        # perform audio classification\n        dataset = format_audio_dataset(df, sampling_rate=classifier.sampling_rate)\n        df[\"category\"] = classifier.predict(dataset)\n\n        # filter audio by category\n        df = df[df[\"category\"] == \"child\"]\n\n    # load transcriber model\n    if config.transcriber.type == \"wav2vec2\":\n        transcriber = Wav2Vec2Transcriber(config.transcriber.model)\n    elif config.transcriber.type == \"whisper\":\n        transcriber = WhisperTranscriber(config.transcriber.model)\n\n    # perform audio transcription\n    dataset = format_audio_dataset(df, sampling_rate=transcriber.sampling_rate)\n\n    output_offsets = transcriber.predict(\n        dataset,\n        chunk_length_s=config.transcriber.chunk_length_s,\n        output_offsets=True,\n        return_timestamps=config.transcriber.return_timestamps,\n        keep_whitespace=config.segmenter.keep_whitespace,\n    )\n\n    # segment audios based on offsets\n    if config.segmenter.type == \"silence\":\n        segmenter = SilenceSegmenter()\n    elif config.segmenter.type == \"word_overlap\":\n        segmenter = WordOverlapSegmenter()\n    elif config.segmenter.type == \"phoneme_overlap\":\n        lexicon = Lexicon()\n        if config.segmenter.lexicon_path:\n            with open(config.segmenter.lexicon_path) as json_file:\n                lex = json.load(json_file)\n            # merge dict with lexicon\n            for k, v in lex.items():\n                lexicon[k] = lexicon[k].union(set(v)) if k in lexicon else set(v)\n        segmenter = PhonemeOverlapSegmenter(lexicon)\n\n    tokenizer = WordTokenizer()\n\n    if config.do_noise_classify:\n        noise_classifier = config.noise_classifier.model\n        minimum_empty_duration = config.noise_classifier.minimum_empty_duration\n        noise_classifier_threshold = config.noise_classifier.threshold\n    else:\n        noise_classifier = None\n        minimum_empty_duration = None\n        noise_classifier_threshold = None\n\n    def export_and_chunk(\n        audio_path: str,\n        ground_truth: str,\n        offsets: List[Dict[str, Union[str, float]]],\n    ):\n        json_path = Path(audio_path).with_suffix(\".json\")\n        # export JSON transcripts\n        export_transcripts_json(str(json_path), offsets)\n        # chunk audio into segments\n        segmenter.chunk_audio_segments(\n            audio_path,\n            output_dir,\n            offsets,\n            do_noise_classify=config.do_noise_classify,\n            noise_classifier=noise_classifier,\n            minimum_empty_duration=minimum_empty_duration,\n            minimum_chunk_duration=config.segmenter.minimum_chunk_duration,\n            noise_classifier_threshold=noise_classifier_threshold,\n            silence_duration=config.segmenter.silence_duration,\n            ground_truth=tokenizer(ground_truth),\n        )\n\n    thread_map(\n        export_and_chunk,\n        df[\"audio\"],\n        df[\"ground_truth\"],\n        output_offsets,\n        desc=\"Segmenting Audio into Chunks\",\n        total=len(df),\n    )\n</code></pre>"},{"location":"reference/aligners/punctuation_forced_aligner/","title":"Punctuation Forced Aligner","text":""},{"location":"reference/aligners/punctuation_forced_aligner/#speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner","title":"<code> speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner        </code>","text":"<p>Force-align predicted phoneme offsets with ground truth text with punctuation.</p> <p>Parameters:</p> Name Type Description Default <code>g2p</code> <code>Callable[[str], List[str]]</code> <p>Callable grapheme-to-phoneme function.</p> required <code>punctuations</code> <code>Optional[List[str]]</code> <p>List of punctuations to include. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>speechline/aligners/punctuation_forced_aligner.py</code> <pre><code>class PunctuationForcedAligner:\n    \"\"\"\n    Force-align predicted phoneme offsets with ground truth text with punctuation.\n\n    Args:\n        g2p (Callable[[str], List[str]]):\n            Callable grapheme-to-phoneme function.\n        punctuations (Optional[List[str]], optional):\n            List of punctuations to include. Defaults to `None`.\n    \"\"\"\n\n    def __init__(\n        self, g2p: Callable[[str], List[str]], punctuations: Optional[List[str]] = None\n    ):\n        self.punctuations = (\n            [\"?\", \",\", \".\", \"!\", \";\"] if not punctuations else punctuations\n        )\n        self.g2p = g2p\n\n    def __call__(\n        self, offsets: List[Dict[str, Union[str, float]]], text: str\n    ) -&gt; List[Dict[str, Union[str, float]]]:\n        \"\"\"\n        Performs punctuation-forced alignment on output offsets\n        from phoneme-recognition models like wav2vec 2.0.\n\n        ### Example\n        ```pycon title=\"example_punctuation_forced_aligner.py\"\n        &gt;&gt;&gt; from gruut import sentences\n        &gt;&gt;&gt; def g2p(text):\n        ...     phonemes = []\n        ...     for words in sentences(text):\n        ...         for word in words:\n        ...             if word.is_major_break or word.is_minor_break:\n        ...                 phonemes += word.text\n        ...             elif word.phonemes:\n        ...                 phonemes += word.phonemes\n        ...     return phonemes\n        &gt;&gt;&gt; pfa = PunctuationForcedAligner(g2p)\n        &gt;&gt;&gt; offsets = [\n        ...     {\"text\": \"h\", \"start_time\": 0.0, \"end_time\": 0.2},\n        ...     {\"text\": \"\u025a\", \"start_time\": 0.24, \"end_time\": 0.28},\n        ...     {\"text\": \"i\", \"start_time\": 0.42, \"end_time\": 0.44},\n        ...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n        ...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n        ...     {\"text\": \"\u028c\", \"start_time\": 0.64, \"end_time\": 0.66},\n        ...     {\"text\": \"m\", \"start_time\": 0.7, \"end_time\": 0.74},\n        ...     {\"text\": \"b\", \"start_time\": 0.78, \"end_time\": 0.82},\n        ...     {\"text\": \"\u0279\", \"start_time\": 0.84, \"end_time\": 0.9},\n        ...     {\"text\": \"\u025b\", \"start_time\": 0.92, \"end_time\": 0.94},\n        ...     {\"text\": \"l\", \"start_time\": 1.0, \"end_time\": 1.04},\n        ...     {\"text\": \"\u0259\", \"start_time\": 1.08, \"end_time\": 1.12},\n        ... ]\n        &gt;&gt;&gt; transcript = \"Her red, umbrella.\"\n        &gt;&gt;&gt; pfa(offsets, transcript)\n        [\n            {'text': 'h', 'start_time': 0.0, 'end_time': 0.2},\n            {'text': '\u025a', 'start_time': 0.24, 'end_time': 0.28},\n            {'text': 'i', 'start_time': 0.42, 'end_time': 0.44},\n            {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n            {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n            {'text': ',', 'start_time': 0.54, 'end_time': 0.64},\n            {'text': '\u028c', 'start_time': 0.64, 'end_time': 0.66},\n            {'text': 'm', 'start_time': 0.7, 'end_time': 0.74},\n            {'text': 'b', 'start_time': 0.78, 'end_time': 0.82},\n            {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.9},\n            {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n            {'text': 'l', 'start_time': 1.0, 'end_time': 1.04},\n            {'text': '\u0259', 'start_time': 1.08, 'end_time': 1.12},\n            {'text': '.', 'start_time': 1.12, 'end_time': 1.12}\n        ]\n        ```\n\n        Args:\n            offsets (List[Dict[str, Union[str, float]]]):\n                List of offsets containing information of phonemes\n                and their respective start and end times\n            text (str):\n                ground truth transcript which contains punctuations\n\n        Returns:\n            List[Dict[str, Union[str, float]]]:\n                List of newly updated offsets which includes punctuations\n        \"\"\"\n        updated_offsets = offsets[:]\n        predicted_phonemes = [offset[\"text\"] for offset in updated_offsets]\n        ground_truth_phonemes = self.g2p(text)\n\n        # segment phonemes based on `self.punctuations`\n        segments, cleaned_segments = self.segment_phonemes_punctuations(\n            ground_truth_phonemes\n        )\n\n        # generate all possible segments from predicted phonemes\n        potential_segments = self.generate_partitions(\n            predicted_phonemes, n=len(cleaned_segments)\n        )\n\n        # if there are multiple possible partitions\n        if len(cleaned_segments) &gt; 1:\n            # filter for highly probable candidates\n            potential_segments = self._filter_candidates_stdev(\n                cleaned_segments, potential_segments\n            )\n\n        # find most similar predicted segment to actual segments\n        max_similarity, aligned_segments = -1, None\n        for potential in potential_segments:\n            similarity = sum(\n                self.similarity(\" \".join(hyp), seg)\n                for hyp, seg in zip(potential, segments)\n            ) / len(cleaned_segments)\n            if similarity &gt; max_similarity:\n                max_similarity = similarity\n                aligned_segments = potential\n\n        # insert punctuations from real segment to predicted segments\n        for idx, token in enumerate(segments):\n            if token in self.punctuations:\n                aligned_segments.insert(idx, [token])\n\n        # add punctuations to offsets\n        idx = 0\n        for segment in aligned_segments:\n            token = segment[0]\n            # skip non-punctuation segments\n            if token not in self.punctuations:\n                idx += len(segment)\n                continue\n\n            # start of punctuation is end time of previous token\n            start = updated_offsets[idx - 1][\"end_time\"]\n\n            # end of punctuation is start time of next token\n            if idx &lt; len(updated_offsets):\n                end = updated_offsets[idx][\"start_time\"]\n            else:\n                end = start  # if it's last, end = start\n\n            offset = {\"text\": token, \"start_time\": start, \"end_time\": end}\n            updated_offsets.insert(idx, offset)\n            idx += 1\n\n        return updated_offsets\n\n    def _filter_candidates_stdev(\n        self,\n        ground_truth_segments: List[List[str]],\n        potential_segments: List[List[List[str]]],\n        k: int = 1,\n    ) -&gt; List[List[List[str]]]:\n        \"\"\"\n        Filters potential segment candidates based on range of\n        standard deviation of segment lengths.\n\n        Args:\n            ground_truth_segments (List[List[str]]):\n                Ground truth segments.\n            potential_segments (List[List[List[str]]]):\n                List of potential segment candidates to filter.\n            k (int, optional):\n                Acceptable upper/lower bounds of standard deviation.\n                Defaults to `1`.\n\n        Returns:\n            List[List[List[str]]]:\n                List of filtered segment candidates.\n        \"\"\"\n        target_stdev = stdev([len(x.split()) for x in ground_truth_segments])\n        stdev_lengths = [\n            stdev([len(x) for x in segment]) for segment in potential_segments\n        ]\n\n        candidate_idxs = [\n            i\n            for i, x in enumerate(stdev_lengths)\n            if target_stdev - k &lt;= x &lt;= target_stdev + k\n        ]\n        candidates = [potential_segments[i] for i in candidate_idxs]\n\n        return candidates\n\n    def segment_phonemes_punctuations(\n        self, phonemes: List[str]\n    ) -&gt; Tuple[List[str], List[str]]:\n        \"\"\"\n        Segment/group list of phonemes consecutively, up to a punctuation.\n\n        Args:\n            phonemes (List[str]):\n                List of phonemes.\n\n        Returns:\n            Tuple[List[str], List[str]]:\n                Pair of equivalently segmented phonemes.\n                Second index returns segments without punctuations.\n        \"\"\"\n        phoneme_string = \" \".join(phonemes)\n        backslash_char = \"\\\\\"\n        segments = re.split(\n            f\"({'|'.join(f'{backslash_char}{p}' for p in self.punctuations)})\",\n            phoneme_string,\n        )\n        segments = [s.strip() for s in segments if s.strip() != \"\"]\n        cleaned_segments = [s for s in segments if s not in self.punctuations]\n        return segments, cleaned_segments\n\n    def similarity(self, a: str, b: str) -&gt; float:\n        return SequenceMatcher(None, a, b).ratio()\n\n    def generate_partitions(self, lst: List, n: int) -&gt; List[List[List]]:\n        \"\"\"\n        Generate all possible `n` consecutive partitions.\n        Source: [StackOverflow](https://stackoverflow.com/a/73356868).\n\n        Args:\n            lst (List):\n                List to be partitioned.\n            n (int):\n                Number of partitions to generate.\n\n        Returns:\n            List[List[List]]:\n                List of all possible list of segments.\n        \"\"\"\n        result = []\n        for indices in combinations(range(1, len(lst)), n - 1):\n            splits = []\n            start = 0\n            for stop in indices:\n                splits.append(lst[start:stop])\n                start = stop\n            splits.append(lst[start:])\n            result.append(splits)\n        return result\n</code></pre>"},{"location":"reference/aligners/punctuation_forced_aligner/#speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner.__call__","title":"<code>__call__(self, offsets, text)</code>  <code>special</code>","text":"<p>Performs punctuation-forced alignment on output offsets from phoneme-recognition models like wav2vec 2.0.</p>"},{"location":"reference/aligners/punctuation_forced_aligner/#speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner.__call__--example","title":"Example","text":"example_punctuation_forced_aligner.py<pre><code>&gt;&gt;&gt; from gruut import sentences\n&gt;&gt;&gt; def g2p(text):\n...     phonemes = []\n...     for words in sentences(text):\n...         for word in words:\n...             if word.is_major_break or word.is_minor_break:\n...                 phonemes += word.text\n...             elif word.phonemes:\n...                 phonemes += word.phonemes\n...     return phonemes\n&gt;&gt;&gt; pfa = PunctuationForcedAligner(g2p)\n&gt;&gt;&gt; offsets = [\n...     {\"text\": \"h\", \"start_time\": 0.0, \"end_time\": 0.2},\n...     {\"text\": \"\u025a\", \"start_time\": 0.24, \"end_time\": 0.28},\n...     {\"text\": \"i\", \"start_time\": 0.42, \"end_time\": 0.44},\n...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n...     {\"text\": \"\u028c\", \"start_time\": 0.64, \"end_time\": 0.66},\n...     {\"text\": \"m\", \"start_time\": 0.7, \"end_time\": 0.74},\n...     {\"text\": \"b\", \"start_time\": 0.78, \"end_time\": 0.82},\n...     {\"text\": \"\u0279\", \"start_time\": 0.84, \"end_time\": 0.9},\n...     {\"text\": \"\u025b\", \"start_time\": 0.92, \"end_time\": 0.94},\n...     {\"text\": \"l\", \"start_time\": 1.0, \"end_time\": 1.04},\n...     {\"text\": \"\u0259\", \"start_time\": 1.08, \"end_time\": 1.12},\n... ]\n&gt;&gt;&gt; transcript = \"Her red, umbrella.\"\n&gt;&gt;&gt; pfa(offsets, transcript)\n[\n    {'text': 'h', 'start_time': 0.0, 'end_time': 0.2},\n    {'text': '\u025a', 'start_time': 0.24, 'end_time': 0.28},\n    {'text': 'i', 'start_time': 0.42, 'end_time': 0.44},\n    {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n    {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n    {'text': ',', 'start_time': 0.54, 'end_time': 0.64},\n    {'text': '\u028c', 'start_time': 0.64, 'end_time': 0.66},\n    {'text': 'm', 'start_time': 0.7, 'end_time': 0.74},\n    {'text': 'b', 'start_time': 0.78, 'end_time': 0.82},\n    {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.9},\n    {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n    {'text': 'l', 'start_time': 1.0, 'end_time': 1.04},\n    {'text': '\u0259', 'start_time': 1.08, 'end_time': 1.12},\n    {'text': '.', 'start_time': 1.12, 'end_time': 1.12}\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>List of offsets containing information of phonemes and their respective start and end times</p> required <code>text</code> <code>str</code> <p>ground truth transcript which contains punctuations</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Union[str, float]]]</code> <p>List of newly updated offsets which includes punctuations</p> Source code in <code>speechline/aligners/punctuation_forced_aligner.py</code> <pre><code>def __call__(\n    self, offsets: List[Dict[str, Union[str, float]]], text: str\n) -&gt; List[Dict[str, Union[str, float]]]:\n    \"\"\"\n    Performs punctuation-forced alignment on output offsets\n    from phoneme-recognition models like wav2vec 2.0.\n\n    ### Example\n    ```pycon title=\"example_punctuation_forced_aligner.py\"\n    &gt;&gt;&gt; from gruut import sentences\n    &gt;&gt;&gt; def g2p(text):\n    ...     phonemes = []\n    ...     for words in sentences(text):\n    ...         for word in words:\n    ...             if word.is_major_break or word.is_minor_break:\n    ...                 phonemes += word.text\n    ...             elif word.phonemes:\n    ...                 phonemes += word.phonemes\n    ...     return phonemes\n    &gt;&gt;&gt; pfa = PunctuationForcedAligner(g2p)\n    &gt;&gt;&gt; offsets = [\n    ...     {\"text\": \"h\", \"start_time\": 0.0, \"end_time\": 0.2},\n    ...     {\"text\": \"\u025a\", \"start_time\": 0.24, \"end_time\": 0.28},\n    ...     {\"text\": \"i\", \"start_time\": 0.42, \"end_time\": 0.44},\n    ...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n    ...     {\"text\": \"d\", \"start_time\": 0.5, \"end_time\": 0.54},\n    ...     {\"text\": \"\u028c\", \"start_time\": 0.64, \"end_time\": 0.66},\n    ...     {\"text\": \"m\", \"start_time\": 0.7, \"end_time\": 0.74},\n    ...     {\"text\": \"b\", \"start_time\": 0.78, \"end_time\": 0.82},\n    ...     {\"text\": \"\u0279\", \"start_time\": 0.84, \"end_time\": 0.9},\n    ...     {\"text\": \"\u025b\", \"start_time\": 0.92, \"end_time\": 0.94},\n    ...     {\"text\": \"l\", \"start_time\": 1.0, \"end_time\": 1.04},\n    ...     {\"text\": \"\u0259\", \"start_time\": 1.08, \"end_time\": 1.12},\n    ... ]\n    &gt;&gt;&gt; transcript = \"Her red, umbrella.\"\n    &gt;&gt;&gt; pfa(offsets, transcript)\n    [\n        {'text': 'h', 'start_time': 0.0, 'end_time': 0.2},\n        {'text': '\u025a', 'start_time': 0.24, 'end_time': 0.28},\n        {'text': 'i', 'start_time': 0.42, 'end_time': 0.44},\n        {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n        {'text': 'd', 'start_time': 0.5, 'end_time': 0.54},\n        {'text': ',', 'start_time': 0.54, 'end_time': 0.64},\n        {'text': '\u028c', 'start_time': 0.64, 'end_time': 0.66},\n        {'text': 'm', 'start_time': 0.7, 'end_time': 0.74},\n        {'text': 'b', 'start_time': 0.78, 'end_time': 0.82},\n        {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.9},\n        {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n        {'text': 'l', 'start_time': 1.0, 'end_time': 1.04},\n        {'text': '\u0259', 'start_time': 1.08, 'end_time': 1.12},\n        {'text': '.', 'start_time': 1.12, 'end_time': 1.12}\n    ]\n    ```\n\n    Args:\n        offsets (List[Dict[str, Union[str, float]]]):\n            List of offsets containing information of phonemes\n            and their respective start and end times\n        text (str):\n            ground truth transcript which contains punctuations\n\n    Returns:\n        List[Dict[str, Union[str, float]]]:\n            List of newly updated offsets which includes punctuations\n    \"\"\"\n    updated_offsets = offsets[:]\n    predicted_phonemes = [offset[\"text\"] for offset in updated_offsets]\n    ground_truth_phonemes = self.g2p(text)\n\n    # segment phonemes based on `self.punctuations`\n    segments, cleaned_segments = self.segment_phonemes_punctuations(\n        ground_truth_phonemes\n    )\n\n    # generate all possible segments from predicted phonemes\n    potential_segments = self.generate_partitions(\n        predicted_phonemes, n=len(cleaned_segments)\n    )\n\n    # if there are multiple possible partitions\n    if len(cleaned_segments) &gt; 1:\n        # filter for highly probable candidates\n        potential_segments = self._filter_candidates_stdev(\n            cleaned_segments, potential_segments\n        )\n\n    # find most similar predicted segment to actual segments\n    max_similarity, aligned_segments = -1, None\n    for potential in potential_segments:\n        similarity = sum(\n            self.similarity(\" \".join(hyp), seg)\n            for hyp, seg in zip(potential, segments)\n        ) / len(cleaned_segments)\n        if similarity &gt; max_similarity:\n            max_similarity = similarity\n            aligned_segments = potential\n\n    # insert punctuations from real segment to predicted segments\n    for idx, token in enumerate(segments):\n        if token in self.punctuations:\n            aligned_segments.insert(idx, [token])\n\n    # add punctuations to offsets\n    idx = 0\n    for segment in aligned_segments:\n        token = segment[0]\n        # skip non-punctuation segments\n        if token not in self.punctuations:\n            idx += len(segment)\n            continue\n\n        # start of punctuation is end time of previous token\n        start = updated_offsets[idx - 1][\"end_time\"]\n\n        # end of punctuation is start time of next token\n        if idx &lt; len(updated_offsets):\n            end = updated_offsets[idx][\"start_time\"]\n        else:\n            end = start  # if it's last, end = start\n\n        offset = {\"text\": token, \"start_time\": start, \"end_time\": end}\n        updated_offsets.insert(idx, offset)\n        idx += 1\n\n    return updated_offsets\n</code></pre>"},{"location":"reference/aligners/punctuation_forced_aligner/#speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner.generate_partitions","title":"<code>generate_partitions(self, lst, n)</code>","text":"<p>Generate all possible <code>n</code> consecutive partitions. Source: StackOverflow.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>List</code> <p>List to be partitioned.</p> required <code>n</code> <code>int</code> <p>Number of partitions to generate.</p> required <p>Returns:</p> Type Description <code>List[List[List]]</code> <p>List of all possible list of segments.</p> Source code in <code>speechline/aligners/punctuation_forced_aligner.py</code> <pre><code>def generate_partitions(self, lst: List, n: int) -&gt; List[List[List]]:\n    \"\"\"\n    Generate all possible `n` consecutive partitions.\n    Source: [StackOverflow](https://stackoverflow.com/a/73356868).\n\n    Args:\n        lst (List):\n            List to be partitioned.\n        n (int):\n            Number of partitions to generate.\n\n    Returns:\n        List[List[List]]:\n            List of all possible list of segments.\n    \"\"\"\n    result = []\n    for indices in combinations(range(1, len(lst)), n - 1):\n        splits = []\n        start = 0\n        for stop in indices:\n            splits.append(lst[start:stop])\n            start = stop\n        splits.append(lst[start:])\n        result.append(splits)\n    return result\n</code></pre>"},{"location":"reference/aligners/punctuation_forced_aligner/#speechline.aligners.punctuation_forced_aligner.PunctuationForcedAligner.segment_phonemes_punctuations","title":"<code>segment_phonemes_punctuations(self, phonemes)</code>","text":"<p>Segment/group list of phonemes consecutively, up to a punctuation.</p> <p>Parameters:</p> Name Type Description Default <code>phonemes</code> <code>List[str]</code> <p>List of phonemes.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Pair of equivalently segmented phonemes.     Second index returns segments without punctuations.</p> Source code in <code>speechline/aligners/punctuation_forced_aligner.py</code> <pre><code>def segment_phonemes_punctuations(\n    self, phonemes: List[str]\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Segment/group list of phonemes consecutively, up to a punctuation.\n\n    Args:\n        phonemes (List[str]):\n            List of phonemes.\n\n    Returns:\n        Tuple[List[str], List[str]]:\n            Pair of equivalently segmented phonemes.\n            Second index returns segments without punctuations.\n    \"\"\"\n    phoneme_string = \" \".join(phonemes)\n    backslash_char = \"\\\\\"\n    segments = re.split(\n        f\"({'|'.join(f'{backslash_char}{p}' for p in self.punctuations)})\",\n        phoneme_string,\n    )\n    segments = [s.strip() for s in segments if s.strip() != \"\"]\n    cleaned_segments = [s for s in segments if s not in self.punctuations]\n    return segments, cleaned_segments\n</code></pre>"},{"location":"reference/classifiers/ast/","title":"Audio Spectogram Transformer Classifier","text":""},{"location":"reference/classifiers/ast/#speechline.classifiers.ast.ASTClassifier","title":"<code> speechline.classifiers.ast.ASTClassifier            (AudioMultiLabelClassifier)         </code>","text":"<p>Audio classifier with feature extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace model hub checkpoint.</p> required Source code in <code>speechline/classifiers/ast.py</code> <pre><code>class ASTClassifier(AudioMultiLabelClassifier):\n    \"\"\"\n    Audio classifier with feature extractor.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace model hub checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str) -&gt; None:\n        super().__init__(model_checkpoint)\n\n    def predict(\n        self, dataset: Dataset, threshold: float = 0.5\n    ) -&gt; List[Dict[str, Union[str, float]]]:\n        \"\"\"\n        Performs audio classification (inference) on `dataset`.\n        Preprocesses datasets, performs inference, then returns predictions.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n            threshold (float):\n                Threshold probability for predicted labels.\n                Anything above this threshold will be considered as a valid prediction.\n\n        Returns:\n            List[Dict[str, Union[str, float]]]:\n                List of predictions in the format of dictionaries,\n                consisting of the predicted label and probability.\n        \"\"\"\n        return self.inference(dataset, threshold)\n</code></pre>"},{"location":"reference/classifiers/ast/#speechline.classifiers.ast.ASTClassifier.predict","title":"<code>predict(self, dataset, threshold=0.5)</code>","text":"<p>Performs audio classification (inference) on <code>dataset</code>. Preprocesses datasets, performs inference, then returns predictions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <code>threshold</code> <code>float</code> <p>Threshold probability for predicted labels. Anything above this threshold will be considered as a valid prediction.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>List[Dict[str, Union[str, float]]]</code> <p>List of predictions in the format of dictionaries,     consisting of the predicted label and probability.</p> Source code in <code>speechline/classifiers/ast.py</code> <pre><code>def predict(\n    self, dataset: Dataset, threshold: float = 0.5\n) -&gt; List[Dict[str, Union[str, float]]]:\n    \"\"\"\n    Performs audio classification (inference) on `dataset`.\n    Preprocesses datasets, performs inference, then returns predictions.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n        threshold (float):\n            Threshold probability for predicted labels.\n            Anything above this threshold will be considered as a valid prediction.\n\n    Returns:\n        List[Dict[str, Union[str, float]]]:\n            List of predictions in the format of dictionaries,\n            consisting of the predicted label and probability.\n    \"\"\"\n    return self.inference(dataset, threshold)\n</code></pre>"},{"location":"reference/classifiers/wav2vec2/","title":"Wav2Vec2 Classifier","text":""},{"location":"reference/classifiers/wav2vec2/#speechline.classifiers.wav2vec2.Wav2Vec2Classifier","title":"<code> speechline.classifiers.wav2vec2.Wav2Vec2Classifier            (AudioClassifier)         </code>","text":"<p>Audio classifier with feature extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace model hub checkpoint.</p> required <code>max_duration_s</code> <code>float</code> <p>Maximum audio duration in seconds.</p> required Source code in <code>speechline/classifiers/wav2vec2.py</code> <pre><code>class Wav2Vec2Classifier(AudioClassifier):\n    \"\"\"\n    Audio classifier with feature extractor.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace model hub checkpoint.\n        max_duration_s (float):\n            Maximum audio duration in seconds.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str, max_duration_s: float) -&gt; None:\n        super().__init__(model_checkpoint, max_duration_s=max_duration_s)\n\n    def predict(self, dataset: Dataset) -&gt; List[str]:\n        \"\"\"\n        Performs audio classification (inference) on `dataset`.\n        Preprocesses datasets, performs inference, then returns predictions.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n\n        Returns:\n            List[str]:\n                List of predictions (as strings of labels).\n        \"\"\"\n        return self.inference(dataset)\n</code></pre>"},{"location":"reference/classifiers/wav2vec2/#speechline.classifiers.wav2vec2.Wav2Vec2Classifier.predict","title":"<code>predict(self, dataset)</code>","text":"<p>Performs audio classification (inference) on <code>dataset</code>. Preprocesses datasets, performs inference, then returns predictions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of predictions (as strings of labels).</p> Source code in <code>speechline/classifiers/wav2vec2.py</code> <pre><code>def predict(self, dataset: Dataset) -&gt; List[str]:\n    \"\"\"\n    Performs audio classification (inference) on `dataset`.\n    Preprocesses datasets, performs inference, then returns predictions.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n\n    Returns:\n        List[str]:\n            List of predictions (as strings of labels).\n    \"\"\"\n    return self.inference(dataset)\n</code></pre>"},{"location":"reference/metrics/phoneme_error_rate/","title":"Phoneme Error Rate","text":""},{"location":"reference/metrics/phoneme_error_rate/#speechline.metrics.phoneme_error_rate.PhonemeErrorRate","title":"<code> speechline.metrics.phoneme_error_rate.PhonemeErrorRate        </code>","text":"<p>Phoneme-Error Rate metric, with flexibility in lexicon.</p> <p>Parameters:</p> Name Type Description Default <code>lexicon</code> <code>Dict[str, List[List[str]]]</code> <p>Pronunciation lexicon with word (grapheme) as key, and list of valid phoneme-list pronunciations.</p> required Source code in <code>speechline/metrics/phoneme_error_rate.py</code> <pre><code>class PhonemeErrorRate:\n    \"\"\"\n    Phoneme-Error Rate metric, with flexibility in lexicon.\n\n    Args:\n        lexicon (Dict[str, List[List[str]]]):\n            Pronunciation lexicon with word (grapheme) as key,\n            and list of valid phoneme-list pronunciations.\n    \"\"\"\n\n    def __init__(\n        self, lexicon: Dict[str, List[List[str]]], epsilon_token: str = \"&lt;*&gt;\"\n    ) -&gt; None:\n        self.lexicon = deepcopy(lexicon)\n        self.epsilon_token = epsilon_token\n\n    def __call__(\n        self, sequences: List[List[str]], predictions: List[List[str]]\n    ) -&gt; float:\n        \"\"\"\n        Calculates PER given list of ground truth words, predicted phonemes,\n        and corresponding lexicon.\n\n        ### Example\n        ```pycon title=\"example_phoneme_error_rate.py\"\n        &gt;&gt;&gt; lexicon = {\n        ...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n        ...     \"guy\": [[\"g\", \"a\", \"i\"]]\n        ... }\n        &gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n        &gt;&gt;&gt; sequences = [\n        ...     [\"hello\", \"hello\"],\n        ...     [\"hello\", \"guy\"]\n        ... ]\n        &gt;&gt;&gt; predictions = [\n        ...     [\"h\", \"e\", \"l\", \"l\", \"o\", \"b\", \"e\", \"l\", \"l\", \"o\"],\n        ...     [\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n        ... ]\n        &gt;&gt;&gt; per(sequences=sequences, predictions=predictions)\n        0.05555555555555555\n        ```\n\n        Args:\n            sequences (List[List[str]]):\n                List of list of ground truth words in a batch.\n            predictions (List[List[str]]):\n                List of list of predicted phonemes in a batch.\n\n        Raises:\n            ValueError: Mismatch in the number of predictions and sequences.\n            KeyError: Words not found in the lexicon.\n\n        Returns:\n            float:\n                Phoneme error rate.\n        \"\"\"\n        if len(sequences) != len(predictions):\n            raise ValueError(\n                f\"Mismatch in the number of predictions ({len(predictions)}) and sequences ({len(sequences)})\"  # noqa: E501\n            )\n\n        oovs = [word for seq in sequences for word in seq if word not in self.lexicon]\n        if len(oovs) &gt; 0:\n            raise KeyError(f\"Words not found in the lexicon: {oovs}\")\n\n        errors, total = 0, 0\n        for words, prediction in zip(sequences, predictions):\n            measures = self.compute_measures(words, prediction)\n            errors += measures[\"errors\"]\n            total += measures[\"total\"]\n        return errors / total\n\n    def compute_measures(\n        self, words: List[str], prediction: List[str]\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Computes the number of phoneme-level errors.\n\n        ### Example\n        ```pycon title=\"example_compute_measures.py\"\n        &gt;&gt;&gt; lexicon = {\n        ...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n        ...     \"guy\": [[\"g\", \"a\", \"i\"]]\n        ... }\n        &gt;&gt;&gt; words = [\"hello\", \"guy\"]\n        &gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n        &gt;&gt;&gt; per.compute_measures(\n        ...     words,\n        ...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n        ... )\n        {'errors': 0, 'total': 8}\n        &gt;&gt;&gt; per.compute_measures(\n        ...     words,\n        ...     prediction=[\"h\", \"a\", \"l\", \"a\", \"i\"]\n        ... )\n        {'errors': 3, 'total': 8}\n        &gt;&gt;&gt; per.compute_measures(\n        ...     words,\n        ...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"b\", \"h\", \"a\", \"i\"]\n        ... )\n        {'errors': 2, 'total': 8}\n        ```\n\n        Args:\n            words (List[str]):\n                List of ground truth words.\n            prediction (List[str]):\n                List of predicted phonemes.\n\n        Returns:\n            Dict[str, int]:\n                A dictionary with number of errors and total number of true phonemes.\n        \"\"\"\n        stack = self._build_pronunciation_stack(words)\n        reference = [\n            phoneme for word in words for phoneme in max(self.lexicon[word], key=len)\n        ]\n\n        editops = Levenshtein.editops(reference, prediction)\n        # get initial number of errors\n        errors = len(editops)\n\n        for tag, i, j in editops:\n            # if there are &gt;1 valid phonemes at position in stack\n            if i &lt; len(stack) and len(stack[i]) &gt; 1:\n                # check if pair of phoneme is in list of valid phoneme pairs\n                # or is substituted by epsilon, which we will thus ignore\n                permutes = permutations(stack[i], 2)\n                if tag == \"replace\" and (reference[i], prediction[j]) in permutes:\n                    errors -= 1\n                # or is an epsilon and hence skippable\n                elif tag == \"delete\" and reference[i] == self.epsilon_token:\n                    errors -= 1\n\n        return {\"errors\": errors, \"total\": len(reference)}\n\n    def _build_pronunciation_stack(self, words: List[str]) -&gt; List[Set[str]]:\n        \"\"\"\n        Builds a list of expected pronunciation \"stack\".\n\n        ### Example\n        ```pycon title=\"example_build_pronunciation_stack.py\"\n        &gt;&gt;&gt; lexicon = {\n        ...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n        ...     \"guy\": [[\"g\", \"a\", \"i\"]]\n        ... }\n        &gt;&gt;&gt; words = [\"hello\", \"guy\"]\n        &gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n        &gt;&gt;&gt; per._build_pronunciation_stack(words)\n        [{'h'}, {'a', 'e'}, {'l'}, {'l'}, {'o'}, {'g'}, {'a'}, {'i'}]\n        ```\n\n        Args:\n            words (List[str]):\n                List of words whose pronunciation stack will be built.\n\n        Returns:\n            List[Set[str]]:\n                List of possible phonemes of the input words.\n        \"\"\"\n\n        def insert_epsilon(\n            pronunciations: List[List[str]], epsilon_token: str = \"&lt;*&gt;\"\n        ) -&gt; List[List[str]]:\n            \"\"\"\n            Insert epsilon (skippable) token into pronunciation phonemes.\n            Epsilon tokens will be ignored during phoneme matching step.\n\n            Args:\n                pronunciations (List[List[str]]):\n                    List of phoneme pronunciations.\n\n            Returns:\n                List[List[str]]:\n                    List of updated phoneme pronunciations.\n            \"\"\"\n            updated_pronunciations = pronunciations[:]\n            # get longest pronunciation\n            longest_pron = max(updated_pronunciations, key=len)\n            for pron in updated_pronunciations:\n                if len(pron) != len(longest_pron):\n                    editops = Levenshtein.editops(pron, longest_pron)\n                    for op, i, _ in editops:\n                        # insert epsilon on insertion index\n                        if op == \"insert\":\n                            pron.insert(i, epsilon_token)\n\n            # repeat, insert epsilon based on new longest pronunciation\n            # See: https://github.com/bookbot-kids/speechline/issues/64.\n            longest_pron = max(updated_pronunciations, key=len)\n            for pron in updated_pronunciations:\n                if len(pron) != len(longest_pron):\n                    editops = Levenshtein.editops(pron, longest_pron)\n                    # only this time following the target index\n                    for op, _, j in editops:\n                        # insert epsilon on insertion index\n                        if op == \"insert\":\n                            pron.insert(j, epsilon_token)\n\n            return updated_pronunciations\n\n        stack = []\n        for word in words:\n            pronunciations = insert_epsilon(self.lexicon[word], self.epsilon_token)\n            length = len(pronunciations[0])\n            word_stack = [\n                set(pron[i] for pron in pronunciations) for i in range(length)\n            ]\n            stack += word_stack\n        return stack\n</code></pre>"},{"location":"reference/metrics/phoneme_error_rate/#speechline.metrics.phoneme_error_rate.PhonemeErrorRate.__call__","title":"<code>__call__(self, sequences, predictions)</code>  <code>special</code>","text":"<p>Calculates PER given list of ground truth words, predicted phonemes, and corresponding lexicon.</p>"},{"location":"reference/metrics/phoneme_error_rate/#speechline.metrics.phoneme_error_rate.PhonemeErrorRate.__call__--example","title":"Example","text":"example_phoneme_error_rate.py<pre><code>&gt;&gt;&gt; lexicon = {\n...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n...     \"guy\": [[\"g\", \"a\", \"i\"]]\n... }\n&gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n&gt;&gt;&gt; sequences = [\n...     [\"hello\", \"hello\"],\n...     [\"hello\", \"guy\"]\n... ]\n&gt;&gt;&gt; predictions = [\n...     [\"h\", \"e\", \"l\", \"l\", \"o\", \"b\", \"e\", \"l\", \"l\", \"o\"],\n...     [\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n... ]\n&gt;&gt;&gt; per(sequences=sequences, predictions=predictions)\n0.05555555555555555\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>List[List[str]]</code> <p>List of list of ground truth words in a batch.</p> required <code>predictions</code> <code>List[List[str]]</code> <p>List of list of predicted phonemes in a batch.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Mismatch in the number of predictions and sequences.</p> <code>KeyError</code> <p>Words not found in the lexicon.</p> <p>Returns:</p> Type Description <code>float</code> <p>Phoneme error rate.</p> Source code in <code>speechline/metrics/phoneme_error_rate.py</code> <pre><code>def __call__(\n    self, sequences: List[List[str]], predictions: List[List[str]]\n) -&gt; float:\n    \"\"\"\n    Calculates PER given list of ground truth words, predicted phonemes,\n    and corresponding lexicon.\n\n    ### Example\n    ```pycon title=\"example_phoneme_error_rate.py\"\n    &gt;&gt;&gt; lexicon = {\n    ...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n    ...     \"guy\": [[\"g\", \"a\", \"i\"]]\n    ... }\n    &gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n    &gt;&gt;&gt; sequences = [\n    ...     [\"hello\", \"hello\"],\n    ...     [\"hello\", \"guy\"]\n    ... ]\n    &gt;&gt;&gt; predictions = [\n    ...     [\"h\", \"e\", \"l\", \"l\", \"o\", \"b\", \"e\", \"l\", \"l\", \"o\"],\n    ...     [\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n    ... ]\n    &gt;&gt;&gt; per(sequences=sequences, predictions=predictions)\n    0.05555555555555555\n    ```\n\n    Args:\n        sequences (List[List[str]]):\n            List of list of ground truth words in a batch.\n        predictions (List[List[str]]):\n            List of list of predicted phonemes in a batch.\n\n    Raises:\n        ValueError: Mismatch in the number of predictions and sequences.\n        KeyError: Words not found in the lexicon.\n\n    Returns:\n        float:\n            Phoneme error rate.\n    \"\"\"\n    if len(sequences) != len(predictions):\n        raise ValueError(\n            f\"Mismatch in the number of predictions ({len(predictions)}) and sequences ({len(sequences)})\"  # noqa: E501\n        )\n\n    oovs = [word for seq in sequences for word in seq if word not in self.lexicon]\n    if len(oovs) &gt; 0:\n        raise KeyError(f\"Words not found in the lexicon: {oovs}\")\n\n    errors, total = 0, 0\n    for words, prediction in zip(sequences, predictions):\n        measures = self.compute_measures(words, prediction)\n        errors += measures[\"errors\"]\n        total += measures[\"total\"]\n    return errors / total\n</code></pre>"},{"location":"reference/metrics/phoneme_error_rate/#speechline.metrics.phoneme_error_rate.PhonemeErrorRate.compute_measures","title":"<code>compute_measures(self, words, prediction)</code>","text":"<p>Computes the number of phoneme-level errors.</p>"},{"location":"reference/metrics/phoneme_error_rate/#speechline.metrics.phoneme_error_rate.PhonemeErrorRate.compute_measures--example","title":"Example","text":"example_compute_measures.py<pre><code>&gt;&gt;&gt; lexicon = {\n...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n...     \"guy\": [[\"g\", \"a\", \"i\"]]\n... }\n&gt;&gt;&gt; words = [\"hello\", \"guy\"]\n&gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n&gt;&gt;&gt; per.compute_measures(\n...     words,\n...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n... )\n{'errors': 0, 'total': 8}\n&gt;&gt;&gt; per.compute_measures(\n...     words,\n...     prediction=[\"h\", \"a\", \"l\", \"a\", \"i\"]\n... )\n{'errors': 3, 'total': 8}\n&gt;&gt;&gt; per.compute_measures(\n...     words,\n...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"b\", \"h\", \"a\", \"i\"]\n... )\n{'errors': 2, 'total': 8}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>words</code> <code>List[str]</code> <p>List of ground truth words.</p> required <code>prediction</code> <code>List[str]</code> <p>List of predicted phonemes.</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>A dictionary with number of errors and total number of true phonemes.</p> Source code in <code>speechline/metrics/phoneme_error_rate.py</code> <pre><code>def compute_measures(\n    self, words: List[str], prediction: List[str]\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Computes the number of phoneme-level errors.\n\n    ### Example\n    ```pycon title=\"example_compute_measures.py\"\n    &gt;&gt;&gt; lexicon = {\n    ...     \"hello\": [[\"h\", \"e\", \"l\", \"l\", \"o\"], [\"h\", \"a\", \"l\", \"l\", \"o\"]],\n    ...     \"guy\": [[\"g\", \"a\", \"i\"]]\n    ... }\n    &gt;&gt;&gt; words = [\"hello\", \"guy\"]\n    &gt;&gt;&gt; per = PhonemeErrorRate(lexicon)\n    &gt;&gt;&gt; per.compute_measures(\n    ...     words,\n    ...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"g\", \"a\", \"i\"]\n    ... )\n    {'errors': 0, 'total': 8}\n    &gt;&gt;&gt; per.compute_measures(\n    ...     words,\n    ...     prediction=[\"h\", \"a\", \"l\", \"a\", \"i\"]\n    ... )\n    {'errors': 3, 'total': 8}\n    &gt;&gt;&gt; per.compute_measures(\n    ...     words,\n    ...     prediction=[\"h\", \"a\", \"l\", \"l\", \"o\", \"b\", \"h\", \"a\", \"i\"]\n    ... )\n    {'errors': 2, 'total': 8}\n    ```\n\n    Args:\n        words (List[str]):\n            List of ground truth words.\n        prediction (List[str]):\n            List of predicted phonemes.\n\n    Returns:\n        Dict[str, int]:\n            A dictionary with number of errors and total number of true phonemes.\n    \"\"\"\n    stack = self._build_pronunciation_stack(words)\n    reference = [\n        phoneme for word in words for phoneme in max(self.lexicon[word], key=len)\n    ]\n\n    editops = Levenshtein.editops(reference, prediction)\n    # get initial number of errors\n    errors = len(editops)\n\n    for tag, i, j in editops:\n        # if there are &gt;1 valid phonemes at position in stack\n        if i &lt; len(stack) and len(stack[i]) &gt; 1:\n            # check if pair of phoneme is in list of valid phoneme pairs\n            # or is substituted by epsilon, which we will thus ignore\n            permutes = permutations(stack[i], 2)\n            if tag == \"replace\" and (reference[i], prediction[j]) in permutes:\n                errors -= 1\n            # or is an epsilon and hence skippable\n            elif tag == \"delete\" and reference[i] == self.epsilon_token:\n                errors -= 1\n\n    return {\"errors\": errors, \"total\": len(reference)}\n</code></pre>"},{"location":"reference/modules/audio_classifier/","title":"Audio Classifier","text":""},{"location":"reference/modules/audio_classifier/#speechline.modules.audio_classifier.AudioClassifier","title":"<code> speechline.modules.audio_classifier.AudioClassifier            (AudioModule)         </code>","text":"<p>Generic AudioClassifier Module. Performs padded audio classification.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace Hub model checkpoint.</p> required Source code in <code>speechline/modules/audio_classifier.py</code> <pre><code>class AudioClassifier(AudioModule):\n    \"\"\"\n    Generic AudioClassifier Module. Performs padded audio classification.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace Hub model checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str, **kwargs) -&gt; None:\n        classifier = pipeline(\n            \"audio-classification\",\n            model=model_checkpoint,\n            device=0 if torch.cuda.is_available() else -1,\n            pipeline_class=AudioClassificationWithPaddingPipeline,\n            **kwargs,\n        )\n        super().__init__(pipeline=classifier)\n\n    def inference(self, dataset: Dataset) -&gt; List[str]:\n        \"\"\"\n        Inference function for audio classification.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n\n        Returns:\n            List[str]:\n                List of predicted labels.\n        \"\"\"\n\n        def _get_audio_array(\n            dataset: Dataset,\n        ) -&gt; np.ndarray:\n            for item in dataset:\n                yield item[\"audio\"][\"array\"]\n\n        results = []\n\n        for out in tqdm(\n            self.pipeline(_get_audio_array(dataset), top_k=1),\n            total=len(dataset),\n            desc=\"Classifying Audios\",\n        ):\n            prediction = out[0][\"label\"]\n            results.append(prediction)\n\n        return results\n</code></pre>"},{"location":"reference/modules/audio_classifier/#speechline.modules.audio_classifier.AudioClassifier.inference","title":"<code>inference(self, dataset)</code>","text":"<p>Inference function for audio classification.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of predicted labels.</p> Source code in <code>speechline/modules/audio_classifier.py</code> <pre><code>def inference(self, dataset: Dataset) -&gt; List[str]:\n    \"\"\"\n    Inference function for audio classification.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n\n    Returns:\n        List[str]:\n            List of predicted labels.\n    \"\"\"\n\n    def _get_audio_array(\n        dataset: Dataset,\n    ) -&gt; np.ndarray:\n        for item in dataset:\n            yield item[\"audio\"][\"array\"]\n\n    results = []\n\n    for out in tqdm(\n        self.pipeline(_get_audio_array(dataset), top_k=1),\n        total=len(dataset),\n        desc=\"Classifying Audios\",\n    ):\n        prediction = out[0][\"label\"]\n        results.append(prediction)\n\n    return results\n</code></pre>"},{"location":"reference/modules/audio_module/","title":"Audio Module","text":""},{"location":"reference/modules/audio_module/#speechline.modules.audio_module.AudioModule","title":"<code> speechline.modules.audio_module.AudioModule        </code>","text":"<p>Base AudioModule. Inherit this class for other audio models. An AudioModule should have an inference pipeline, and an inference function utilizing the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>HuggingFace <code>transformers</code> <code>Pipeline</code> for inference.</p> required Source code in <code>speechline/modules/audio_module.py</code> <pre><code>class AudioModule:\n    \"\"\"\n    Base AudioModule. Inherit this class for other audio models.\n    An AudioModule should have an inference pipeline,\n    and an inference function utilizing the pipeline.\n\n    Args:\n        pipeline (Pipeline):\n            HuggingFace `transformers` `Pipeline` for inference.\n    \"\"\"\n\n    def __init__(self, pipeline: Pipeline) -&gt; None:\n        self.pipeline = pipeline\n        self.sampling_rate = self.pipeline.feature_extractor.sampling_rate\n</code></pre>"},{"location":"reference/modules/audio_multilabel_classifier/","title":"Audio Multilabel Classifier","text":""},{"location":"reference/modules/audio_multilabel_classifier/#speechline.modules.audio_multilabel_classifier.AudioMultiLabelClassifier","title":"<code> speechline.modules.audio_multilabel_classifier.AudioMultiLabelClassifier            (AudioModule)         </code>","text":"<p>Generic AudioClassifier Module. Performs padded audio classification.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace Hub model checkpoint.</p> required Source code in <code>speechline/modules/audio_multilabel_classifier.py</code> <pre><code>class AudioMultiLabelClassifier(AudioModule):\n    \"\"\"\n    Generic AudioClassifier Module. Performs padded audio classification.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace Hub model checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str, **kwargs) -&gt; None:\n        classifier = pipeline(\n            \"audio-classification\",\n            model=model_checkpoint,\n            feature_extractor=model_checkpoint,\n            device=0 if torch.cuda.is_available() else -1,\n            pipeline_class=AudioMultiLabelClassificationPipeline,\n            **kwargs,\n        )\n        super().__init__(pipeline=classifier)\n\n    def inference(\n        self, dataset: Dataset, threshold: float = 0.5\n    ) -&gt; List[Dict[str, Union[str, float]]]:\n        \"\"\"\n        Inference function for audio classification.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n            threshold (float):\n                Threshold probability for predicted labels.\n                Anything above this threshold will be considered as a valid prediction.\n\n        Returns:\n            List[Dict[str, Union[str, float]]]:\n                List of predictions in the format of dictionaries,\n                consisting of the predicted label and probability.\n        \"\"\"\n\n        def _get_audio_array(\n            dataset: Dataset,\n        ) -&gt; np.ndarray:\n            for item in dataset:\n                yield item[\"audio\"][\"array\"]\n\n        results = []\n\n        for out in tqdm(\n            self.pipeline(_get_audio_array(dataset), top_k=1),\n            total=len(dataset),\n            desc=\"Classifying Audios\",\n        ):\n            ids = np.where(out &gt;= threshold)[0].tolist()\n            if len(ids) &gt; 0:\n                prediction = [\n                    {\n                        \"label\": self.pipeline.model.config.id2label[id],\n                        \"score\": out[id],\n                    }\n                    for id in ids\n                ]\n                results.append(prediction)\n\n        return results\n</code></pre>"},{"location":"reference/modules/audio_multilabel_classifier/#speechline.modules.audio_multilabel_classifier.AudioMultiLabelClassifier.inference","title":"<code>inference(self, dataset, threshold=0.5)</code>","text":"<p>Inference function for audio classification.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <code>threshold</code> <code>float</code> <p>Threshold probability for predicted labels. Anything above this threshold will be considered as a valid prediction.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>List[Dict[str, Union[str, float]]]</code> <p>List of predictions in the format of dictionaries,     consisting of the predicted label and probability.</p> Source code in <code>speechline/modules/audio_multilabel_classifier.py</code> <pre><code>def inference(\n    self, dataset: Dataset, threshold: float = 0.5\n) -&gt; List[Dict[str, Union[str, float]]]:\n    \"\"\"\n    Inference function for audio classification.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n        threshold (float):\n            Threshold probability for predicted labels.\n            Anything above this threshold will be considered as a valid prediction.\n\n    Returns:\n        List[Dict[str, Union[str, float]]]:\n            List of predictions in the format of dictionaries,\n            consisting of the predicted label and probability.\n    \"\"\"\n\n    def _get_audio_array(\n        dataset: Dataset,\n    ) -&gt; np.ndarray:\n        for item in dataset:\n            yield item[\"audio\"][\"array\"]\n\n    results = []\n\n    for out in tqdm(\n        self.pipeline(_get_audio_array(dataset), top_k=1),\n        total=len(dataset),\n        desc=\"Classifying Audios\",\n    ):\n        ids = np.where(out &gt;= threshold)[0].tolist()\n        if len(ids) &gt; 0:\n            prediction = [\n                {\n                    \"label\": self.pipeline.model.config.id2label[id],\n                    \"score\": out[id],\n                }\n                for id in ids\n            ]\n            results.append(prediction)\n\n    return results\n</code></pre>"},{"location":"reference/modules/audio_transcriber/","title":"Audio Transcriber","text":""},{"location":"reference/modules/audio_transcriber/#speechline.modules.audio_transcriber.AudioTranscriber","title":"<code> speechline.modules.audio_transcriber.AudioTranscriber            (AudioModule)         </code>","text":"<p>Generic AudioTranscriber class for speech/phoneme recognition.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace Hub model hub checkpoint.</p> required Source code in <code>speechline/modules/audio_transcriber.py</code> <pre><code>class AudioTranscriber(AudioModule):\n    \"\"\"\n    Generic AudioTranscriber class for speech/phoneme recognition.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace Hub model hub checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str) -&gt; None:\n        asr = pipeline(\n            \"automatic-speech-recognition\",\n            model=model_checkpoint,\n            device=0 if torch.cuda.is_available() else -1,\n            pipeline_class=AutomaticSpeechRecognitionFilteredPipeline,\n        )\n        super().__init__(pipeline=asr)\n\n    def inference(\n        self,\n        dataset: Dataset,\n        chunk_length_s: int = 0,\n        output_offsets: bool = False,\n        offset_key: str = \"text\",\n        return_timestamps: Union[str, bool] = True,\n        keep_whitespace: bool = False,\n        **kwargs,\n    ) -&gt; Union[List[List[Dict[str, Union[str, float]]]], List[str]]:\n        \"\"\"\n        Inference/prediction function to be mapped to a dataset.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n            chunk_length_s (int, optional):\n                Audio chunk length in seconds. Defaults to `30`.\n            output_offsets (bool, optional):\n                Whether to output offsets. Defaults to `False`.\n            offset_key (str, optional):\n                Offset dictionary key. Defaults to `\"text\"`.\n            return_timestamps (Union[str, bool], optional):\n                `return_timestamps` argument in `AutomaticSpeechRecognitionPipeline`'s\n                `__call__` method. Use `\"char\"` for CTC-based models and\n                `True` for Whisper-based models.\n                Defaults to `True`.\n            keep_whitespace (bool, optional):\n                Whether to presere whitespace predictions. Defaults to `False`.\n\n        Returns:\n            Union[List[List[Dict[str, Union[str, float]]]], List[str]]:\n                List of predictions.\n        \"\"\"\n\n        def _format_timestamps_to_offsets(\n            timestamps: Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]],\n            offset_key: str = \"text\",\n            keep_whitespace: bool = False,\n        ) -&gt; List[Dict[str, Union[str, float]]]:\n            \"\"\"\n            Formats `AutomaticSpeechRecognitionPipeline`'s timestamp outputs to\n            a list of offsets with the following format:\n\n            ```json\n            [\n                {\n                    \"{offset_key}\": {text},\n                    \"start_time\": {start_time},\n                    \"end_time\": {end_time}\n                },\n                {\n                    \"{offset_key}\": {text},\n                    \"start_time\": {start_time},\n                    \"end_time\": {end_time}\n                },\n                ...\n            ]\n            ```\n\n            Args:\n                timestamps (Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]]):  # noqa: E501\n                    Output timestamps from `AutomaticSpeechRecognitionPipeline`.\n                offset_key (str, optional):\n                    Transcript dictionary key in offset. Defaults to `\"text\"`.\n                keep_whitespace (bool, optional):\n                    Whether to presere whitespace predictions. Defaults to `False`.\n\n            Returns:\n                List[Dict[str, Union[str, float]]]:\n                    List of offsets.\n            \"\"\"\n            return [\n                {\n                    offset_key: o[\"text\"] if keep_whitespace else o[\"text\"].strip(),\n                    \"start_time\": round(o[\"timestamp\"][0], 3),\n                    \"end_time\": round(o[\"timestamp\"][1], 3),\n                }\n                for o in timestamps[\"chunks\"]\n                if o[\"text\"] != \" \" or keep_whitespace\n            ]\n\n        def _format_timestamps_to_transcript(\n            timestamps: Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]],\n        ) -&gt; str:\n            \"\"\"\n            Formats `AutomaticSpeechRecognitionPipeline`'s timestamp outputs\n            to a transcript string.\n\n            Args:\n                timestamps (Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]]):  # noqa: E501\n                    Output timestamps from `AutomaticSpeechRecognitionPipeline`.\n\n            Returns:\n                str:\n                    Transcript string.\n            \"\"\"\n            return \" \".join([o[\"text\"].strip() for o in timestamps[\"chunks\"] if o[\"text\"] != \" \"])\n\n        def _get_audio_array(\n            dataset: Dataset,\n        ) -&gt; Generator[Dict[str, Union[np.ndarray, int, str]], None, None]:\n            for item in dataset:\n                yield {**item[\"audio\"]}\n\n        results = []\n\n        for out in tqdm(\n            self.pipeline(\n                _get_audio_array(dataset),\n                chunk_length_s=chunk_length_s,\n                return_timestamps=return_timestamps,\n                **kwargs,\n            ),\n            total=len(dataset),\n            desc=\"Transcribing Audios\",\n        ):\n            prediction = (\n                _format_timestamps_to_offsets(\n                    out,\n                    offset_key=offset_key,\n                    keep_whitespace=keep_whitespace,\n                )\n                if output_offsets\n                else _format_timestamps_to_transcript(out)\n            )\n            results.append(prediction)\n\n        return results\n</code></pre>"},{"location":"reference/modules/audio_transcriber/#speechline.modules.audio_transcriber.AudioTranscriber.inference","title":"<code>inference(self, dataset, chunk_length_s=0, output_offsets=False, offset_key='text', return_timestamps=True, keep_whitespace=False, **kwargs)</code>","text":"<p>Inference/prediction function to be mapped to a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <code>chunk_length_s</code> <code>int</code> <p>Audio chunk length in seconds. Defaults to <code>30</code>.</p> <code>0</code> <code>output_offsets</code> <code>bool</code> <p>Whether to output offsets. Defaults to <code>False</code>.</p> <code>False</code> <code>offset_key</code> <code>str</code> <p>Offset dictionary key. Defaults to <code>\"text\"</code>.</p> <code>'text'</code> <code>return_timestamps</code> <code>Union[str, bool]</code> <p><code>return_timestamps</code> argument in <code>AutomaticSpeechRecognitionPipeline</code>'s <code>__call__</code> method. Use <code>\"char\"</code> for CTC-based models and <code>True</code> for Whisper-based models. Defaults to <code>True</code>.</p> <code>True</code> <code>keep_whitespace</code> <code>bool</code> <p>Whether to presere whitespace predictions. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[List[Dict[str, Union[str, float]]]], List[str]]</code> <p>List of predictions.</p> Source code in <code>speechline/modules/audio_transcriber.py</code> <pre><code>def inference(\n    self,\n    dataset: Dataset,\n    chunk_length_s: int = 0,\n    output_offsets: bool = False,\n    offset_key: str = \"text\",\n    return_timestamps: Union[str, bool] = True,\n    keep_whitespace: bool = False,\n    **kwargs,\n) -&gt; Union[List[List[Dict[str, Union[str, float]]]], List[str]]:\n    \"\"\"\n    Inference/prediction function to be mapped to a dataset.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n        chunk_length_s (int, optional):\n            Audio chunk length in seconds. Defaults to `30`.\n        output_offsets (bool, optional):\n            Whether to output offsets. Defaults to `False`.\n        offset_key (str, optional):\n            Offset dictionary key. Defaults to `\"text\"`.\n        return_timestamps (Union[str, bool], optional):\n            `return_timestamps` argument in `AutomaticSpeechRecognitionPipeline`'s\n            `__call__` method. Use `\"char\"` for CTC-based models and\n            `True` for Whisper-based models.\n            Defaults to `True`.\n        keep_whitespace (bool, optional):\n            Whether to presere whitespace predictions. Defaults to `False`.\n\n    Returns:\n        Union[List[List[Dict[str, Union[str, float]]]], List[str]]:\n            List of predictions.\n    \"\"\"\n\n    def _format_timestamps_to_offsets(\n        timestamps: Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]],\n        offset_key: str = \"text\",\n        keep_whitespace: bool = False,\n    ) -&gt; List[Dict[str, Union[str, float]]]:\n        \"\"\"\n        Formats `AutomaticSpeechRecognitionPipeline`'s timestamp outputs to\n        a list of offsets with the following format:\n\n        ```json\n        [\n            {\n                \"{offset_key}\": {text},\n                \"start_time\": {start_time},\n                \"end_time\": {end_time}\n            },\n            {\n                \"{offset_key}\": {text},\n                \"start_time\": {start_time},\n                \"end_time\": {end_time}\n            },\n            ...\n        ]\n        ```\n\n        Args:\n            timestamps (Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]]):  # noqa: E501\n                Output timestamps from `AutomaticSpeechRecognitionPipeline`.\n            offset_key (str, optional):\n                Transcript dictionary key in offset. Defaults to `\"text\"`.\n            keep_whitespace (bool, optional):\n                Whether to presere whitespace predictions. Defaults to `False`.\n\n        Returns:\n            List[Dict[str, Union[str, float]]]:\n                List of offsets.\n        \"\"\"\n        return [\n            {\n                offset_key: o[\"text\"] if keep_whitespace else o[\"text\"].strip(),\n                \"start_time\": round(o[\"timestamp\"][0], 3),\n                \"end_time\": round(o[\"timestamp\"][1], 3),\n            }\n            for o in timestamps[\"chunks\"]\n            if o[\"text\"] != \" \" or keep_whitespace\n        ]\n\n    def _format_timestamps_to_transcript(\n        timestamps: Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]],\n    ) -&gt; str:\n        \"\"\"\n        Formats `AutomaticSpeechRecognitionPipeline`'s timestamp outputs\n        to a transcript string.\n\n        Args:\n            timestamps (Dict[str, Union[str, List[Dict[str, Union[str, Tuple[float, float]]]]]]):  # noqa: E501\n                Output timestamps from `AutomaticSpeechRecognitionPipeline`.\n\n        Returns:\n            str:\n                Transcript string.\n        \"\"\"\n        return \" \".join([o[\"text\"].strip() for o in timestamps[\"chunks\"] if o[\"text\"] != \" \"])\n\n    def _get_audio_array(\n        dataset: Dataset,\n    ) -&gt; Generator[Dict[str, Union[np.ndarray, int, str]], None, None]:\n        for item in dataset:\n            yield {**item[\"audio\"]}\n\n    results = []\n\n    for out in tqdm(\n        self.pipeline(\n            _get_audio_array(dataset),\n            chunk_length_s=chunk_length_s,\n            return_timestamps=return_timestamps,\n            **kwargs,\n        ),\n        total=len(dataset),\n        desc=\"Transcribing Audios\",\n    ):\n        prediction = (\n            _format_timestamps_to_offsets(\n                out,\n                offset_key=offset_key,\n                keep_whitespace=keep_whitespace,\n            )\n            if output_offsets\n            else _format_timestamps_to_transcript(out)\n        )\n        results.append(prediction)\n\n    return results\n</code></pre>"},{"location":"reference/pipelines/audio_classification_with_padding/","title":"Audio Classification with Padding","text":""},{"location":"reference/pipelines/audio_classification_with_padding/#speechline.pipelines.audio_classification.AudioClassificationWithPaddingPipeline","title":"<code> speechline.pipelines.audio_classification.AudioClassificationWithPaddingPipeline            (AudioClassificationPipeline)         </code>","text":"<p>Subclass of <code>AudioClassificationPipeline</code>. Pads/truncates audio array to maximum length before performing audio classification.</p> Source code in <code>speechline/pipelines/audio_classification.py</code> <pre><code>class AudioClassificationWithPaddingPipeline(AudioClassificationPipeline):\n    \"\"\"\n    Subclass of `AudioClassificationPipeline`.\n    Pads/truncates audio array to maximum length before performing audio classification.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.max_duration_s = kwargs.get(\"max_duration_s\")\n        super().__init__(*args, **kwargs)\n\n    def preprocess(self, inputs: np.ndarray) -&gt; torch.Tensor:\n        \"\"\"\n        Pre-process `inputs` to a maximum length used during model's training.\n        Let `max_length = int(sampling_rate * max_duration_s)`.\n        Audio arrays shorter than `max_length` will be padded to `max_length`,\n        while arrays longer than `max_length` will be truncated to `max_length`.\n\n\n        Args:\n            inputs (np.ndarray):\n                Input audio array.\n\n        Returns:\n            torch.Tensor:\n                Pre-processed audio array as PyTorch tensors.\n        \"\"\"\n        processed = self.feature_extractor(\n            inputs,\n            sampling_rate=self.feature_extractor.sampling_rate,\n            return_tensors=\"pt\",\n            max_length=int(self.feature_extractor.sampling_rate * self.max_duration_s),\n            truncation=True,\n        )\n        return processed\n</code></pre>"},{"location":"reference/pipelines/audio_classification_with_padding/#speechline.pipelines.audio_classification.AudioClassificationWithPaddingPipeline.preprocess","title":"<code>preprocess(self, inputs)</code>","text":"<p>Pre-process <code>inputs</code> to a maximum length used during model's training. Let <code>max_length = int(sampling_rate * max_duration_s)</code>. Audio arrays shorter than <code>max_length</code> will be padded to <code>max_length</code>, while arrays longer than <code>max_length</code> will be truncated to <code>max_length</code>.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>np.ndarray</code> <p>Input audio array.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Pre-processed audio array as PyTorch tensors.</p> Source code in <code>speechline/pipelines/audio_classification.py</code> <pre><code>def preprocess(self, inputs: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"\n    Pre-process `inputs` to a maximum length used during model's training.\n    Let `max_length = int(sampling_rate * max_duration_s)`.\n    Audio arrays shorter than `max_length` will be padded to `max_length`,\n    while arrays longer than `max_length` will be truncated to `max_length`.\n\n\n    Args:\n        inputs (np.ndarray):\n            Input audio array.\n\n    Returns:\n        torch.Tensor:\n            Pre-processed audio array as PyTorch tensors.\n    \"\"\"\n    processed = self.feature_extractor(\n        inputs,\n        sampling_rate=self.feature_extractor.sampling_rate,\n        return_tensors=\"pt\",\n        max_length=int(self.feature_extractor.sampling_rate * self.max_duration_s),\n        truncation=True,\n    )\n    return processed\n</code></pre>"},{"location":"reference/pipelines/audio_multilabel_classification/","title":"Audio Multi Label Classification","text":""},{"location":"reference/pipelines/audio_multilabel_classification/#speechline.pipelines.audio_multilabel_classification.AudioMultiLabelClassificationPipeline","title":"<code> speechline.pipelines.audio_multilabel_classification.AudioMultiLabelClassificationPipeline            (AudioClassificationPipeline)         </code>","text":"<p>Subclass of <code>AudioClassificationPipeline</code>. Performs multi-label audio classification instead of multi-class classification. Applies Sigmoid on logits instead of Softmax.</p> Source code in <code>speechline/pipelines/audio_multilabel_classification.py</code> <pre><code>class AudioMultiLabelClassificationPipeline(AudioClassificationPipeline):\n    \"\"\"\n    Subclass of `AudioClassificationPipeline`.\n    Performs multi-label audio classification instead of multi-class classification.\n    Applies Sigmoid on logits instead of Softmax.\n    \"\"\"\n\n    def _sanitize_parameters(self, **kwargs) -&gt; Tuple[Dict, Dict, Dict]:\n        \"\"\"\n        Forces post-processor to return all probabilities.\n\n        Returns:\n            Tuple[Dict, Dict, Dict]:\n                Tuple consisting of:\n\n                    1. Preprocess parameters (empty).\n                    2. Forward parameters (empty).\n                    3. Postprocess parameters (`top_k = num_labels`).\n        \"\"\"\n        postprocess_params = {\"top_k\": self.model.config.num_labels}\n        return {}, {}, postprocess_params\n\n    def postprocess(self, model_outputs: ModelOutput, **kwargs) -&gt; np.ndarray:\n        \"\"\"\n        Applies Sigmoid on logits.\n\n        Args:\n            model_outputs (ModelOutput):\n                Generic HuggingFace model outputs.\n\n        Returns:\n            np.ndarray:\n                List of probabilities.\n        \"\"\"\n        probs = model_outputs.logits[0]\n        sigmoid = torch.nn.Sigmoid()\n        scores = sigmoid(probs).cpu().numpy()\n        return scores\n</code></pre>"},{"location":"reference/pipelines/audio_multilabel_classification/#speechline.pipelines.audio_multilabel_classification.AudioMultiLabelClassificationPipeline.postprocess","title":"<code>postprocess(self, model_outputs, **kwargs)</code>","text":"<p>Applies Sigmoid on logits.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>ModelOutput</code> <p>Generic HuggingFace model outputs.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>List of probabilities.</p> Source code in <code>speechline/pipelines/audio_multilabel_classification.py</code> <pre><code>def postprocess(self, model_outputs: ModelOutput, **kwargs) -&gt; np.ndarray:\n    \"\"\"\n    Applies Sigmoid on logits.\n\n    Args:\n        model_outputs (ModelOutput):\n            Generic HuggingFace model outputs.\n\n    Returns:\n        np.ndarray:\n            List of probabilities.\n    \"\"\"\n    probs = model_outputs.logits[0]\n    sigmoid = torch.nn.Sigmoid()\n    scores = sigmoid(probs).cpu().numpy()\n    return scores\n</code></pre>"},{"location":"reference/scripts/aac_to_wav/","title":"<code>aac</code>-to-<code>wav</code> Audio Converter","text":""},{"location":"reference/scripts/aac_to_wav/#usage","title":"Usage","text":"example_aac_to_wav.sh<pre><code>python scripts/aac_to_wav.py [-h] -i INPUT_DIR [-c CHANNEL] [-r RATE]\n</code></pre> <pre><code>Batch-convert aac audios in a folder to wav format with ffmpeg.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT_DIR, --input_dir INPUT_DIR\n                        Directory of input audios to convert.\n  -c CHANNEL, --channel CHANNEL\n                        Number of audio channels in output.\n  -r RATE, --rate RATE  Sample rate of audio output.\n</code></pre>"},{"location":"reference/scripts/aac_to_wav/#example","title":"Example","text":"<pre><code>python scripts/aac_to_wav.py --input_dir=\"dropbox/\" -c 1 -r 16000 \n</code></pre>"},{"location":"reference/scripts/aac_to_wav/#scripts.aac_to_wav","title":"<code>scripts.aac_to_wav</code>","text":""},{"location":"reference/scripts/aac_to_wav/#scripts.aac_to_wav.convert_to_wav","title":"<code>convert_to_wav(input_audio_path, num_channels=1, sampling_rate=16000)</code>","text":"<p>Convert aac audio file to wav at same directory.</p> <p>Parameters:</p> Name Type Description Default <code>input_audio_path</code> <code>str</code> <p>Path to aac file.</p> required <code>num_channels</code> <code>int</code> <p>Number of output audio channels. Defaults to <code>1</code>.</p> <code>1</code> <code>sampling_rate</code> <code>int</code> <p>Output audio sampling rate. Defaults to <code>16_000</code>.</p> <code>16000</code> <p>Returns:</p> Type Description <code>subprocess.CompletedProcess</code> <p>Finished subprocess.</p> Source code in <code>scripts/aac_to_wav.py</code> <pre><code>def convert_to_wav(\n    input_audio_path: str, num_channels: int = 1, sampling_rate: int = 16_000\n) -&gt; subprocess.CompletedProcess:\n    \"\"\"\n    Convert aac audio file to wav at same directory.\n\n    Args:\n        input_audio_path (str):\n            Path to aac file.\n        num_channels (int, optional):\n            Number of output audio channels. Defaults to `1`.\n        sampling_rate (int, optional):\n            Output audio sampling rate. Defaults to `16_000`.\n\n    Returns:\n        subprocess.CompletedProcess:\n            Finished subprocess.\n    \"\"\"\n    # replace input file's extension to wav as output file path\n    output_audio_path = Path(input_audio_path).with_suffix(\".wav\")\n\n    # equivalent to:\n    # ffmpeg -i {input_audio_path} -acodec pcm_s16le -ac {num_channels} \\\n    #       -ar {sampling_rate} {output_audio_path}\n    job = subprocess.run(\n        [\n            \"ffmpeg\",\n            \"-loglevel\",\n            \"quiet\",\n            \"-hide_banner\",\n            \"-y\",\n            \"-i\",\n            input_audio_path,\n            \"-acodec\",\n            \"pcm_s16le\",\n            \"-ac\",\n            str(num_channels),\n            \"-ar\",\n            str(sampling_rate),\n            str(output_audio_path),\n        ],\n        stderr=subprocess.DEVNULL,\n        stdout=subprocess.DEVNULL,\n        stdin=subprocess.PIPE,\n    )\n\n    return job\n</code></pre>"},{"location":"reference/scripts/aac_to_wav/#scripts.aac_to_wav.parse_args","title":"<code>parse_args(args)</code>","text":"<p>Utility argument parser function for batch audio conversion.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>scripts/aac_to_wav.py</code> <pre><code>def parse_args(args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for batch audio conversion.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python scripts/aac_to_wav.py\",\n        description=\"Batch-convert aac audios in a folder to wav format with ffmpeg.\",\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Directory of input audios to convert.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--channel\",\n        type=int,\n        default=1,\n        help=\"Number of audio channels in output.\",\n    )\n    parser.add_argument(\n        \"-r\", \"--rate\", type=int, default=16_000, help=\"Sample rate of audio output.\"\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/scripts/create_hf_dataset/","title":"Create HuggingFace Datset","text":""},{"location":"reference/scripts/create_hf_dataset/#usage","title":"Usage","text":"example_create_hf_dataset.sh<pre><code>python scripts/create_hf_dataset.py [-h] -i INPUT_DIR --dataset_name DATASET_NAME [--phonemize PHONEMIZE] [--private PRIVATE]\n</code></pre> <pre><code>Create HuggingFace dataset from SpeechLine outputs.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INPUT_DIR, --input_dir INPUT_DIR\n                        Directory of input audios.\n  --dataset_name DATASET_NAME\n                        HuggingFace dataset repository name.\n  --phonemize PHONEMIZE\n                        Phonemize text.\n  --private PRIVATE     Set HuggingFace dataset to private.\n</code></pre>"},{"location":"reference/scripts/create_hf_dataset/#example","title":"Example","text":"<pre><code>python scripts/create_hf_dataset.py \\\n    --input_dir=\"training/\" \\\n    --dataset_name=\"myname/mydataset\" \\\n    --private=\"True\" \\\n    --phonemize=\"True\"\n</code></pre>"},{"location":"reference/scripts/create_hf_dataset/#scripts.create_hf_dataset","title":"<code>scripts.create_hf_dataset</code>","text":""},{"location":"reference/scripts/create_hf_dataset/#scripts.create_hf_dataset.create_dataset","title":"<code>create_dataset(input_dir, dataset_name, private=True, phonemize=False)</code>","text":"<p>Creates HuggingFace dataset from SpeechLine outputs. Ensures unique utterance and speaker IDs in each subset.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to input audio directory.</p> required <code>dataset_name</code> <code>str</code> <p>HuggingFace dataset name.</p> required <code>private</code> <code>bool</code> <p>Set HuggingFace dataset as private. Defaults to <code>True</code>.</p> <code>True</code> <code>phonemize</code> <code>bool</code> <p>Phonemize text to phoneme strings. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>Created HuggingFace dataset.</p> Source code in <code>scripts/create_hf_dataset.py</code> <pre><code>def create_dataset(\n    input_dir: str, dataset_name: str, private: bool = True, phonemize: bool = False\n) -&gt; DatasetDict:\n    \"\"\"\n    Creates HuggingFace dataset from SpeechLine outputs.\n    Ensures unique utterance and speaker IDs in each subset.\n\n    Args:\n        input_dir (str):\n            Path to input audio directory.\n        dataset_name (str):\n            HuggingFace dataset name.\n        private (bool, optional):\n            Set HuggingFace dataset as private. Defaults to `True`.\n        phonemize (bool, optional):\n            Phonemize text to phoneme strings. Defaults to `False`.\n\n    Returns:\n        DatasetDict:\n            Created HuggingFace dataset.\n    \"\"\"\n    audios = glob(f\"{input_dir}/**/*.wav\")\n    df = pd.DataFrame({\"audio\": audios})\n    # `audio` =  `\"{dir}/{language}/{speaker}_{utt_id}.wav\"`\n    df[\"language\"] = df[\"audio\"].apply(lambda x: x.split(\"/\")[-2])\n    df[\"speaker\"] = df[\"audio\"].apply(lambda x: x.split(\"/\")[-1].split(\"_\")[0])\n    df[\"text\"] = df[\"audio\"].apply(lambda x: parse_tsv(Path(x).with_suffix(\".tsv\")))\n\n    tqdm.pandas(desc=\"Phonemization\")\n\n    if phonemize:\n        df[\"phonemes\"] = df.progress_apply(\n            lambda row: get_g2p(row[\"language\"].split(\"-\")[0])(row[\"text\"]), axis=1\n        )\n\n    speaker, counts = np.unique(df[\"speaker\"], return_counts=True)\n    speaker2count = {s: c for s, c in zip(speaker, counts)}\n\n    train_num = int(0.7 * len(df))\n    test_num = int(0.9 * len(df))\n\n    train_speakers, test_speakers, valid_speakers = [], [], []\n    total = 0\n\n    for speaker, count in sorted(\n        speaker2count.items(), key=lambda item: item[1], reverse=True\n    ):\n        if total &lt; train_num and total &lt; test_num:\n            train_speakers.append(speaker)\n        elif total &lt; test_num:\n            test_speakers.append(speaker)\n        else:\n            valid_speakers.append(speaker)\n        total += count\n\n    train_df = df[df[\"speaker\"].isin(train_speakers)].reset_index(drop=True)\n    test_df = df[df[\"speaker\"].isin(test_speakers)].reset_index(drop=True)\n    valid_df = df[df[\"speaker\"].isin(valid_speakers)].reset_index(drop=True)\n\n    train_ds = Dataset.from_pandas(train_df).cast_column(\"audio\", Audio())\n    test_ds = Dataset.from_pandas(test_df).cast_column(\"audio\", Audio())\n    valid_ds = Dataset.from_pandas(valid_df).cast_column(\"audio\", Audio())\n\n    dataset = DatasetDict({\"train\": train_ds, \"test\": test_ds, \"validation\": valid_ds})\n    dataset.push_to_hub(dataset_name, private=private)\n    return dataset\n</code></pre>"},{"location":"reference/scripts/create_hf_dataset/#scripts.create_hf_dataset.parse_args","title":"<code>parse_args(args)</code>","text":"<p>Utility argument parser function for dataset creation.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>scripts/create_hf_dataset.py</code> <pre><code>def parse_args(args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for dataset creation.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python scripts/create_hf_datasets.py\",\n        description=\"Create HuggingFace dataset from SpeechLine outputs.\",\n    )\n\n    parser.add_argument(\n        \"-i\",\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Directory of input audios.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        required=True,\n        help=\"HuggingFace dataset repository name.\",\n    )\n    parser.add_argument(\"--phonemize\", type=bool, default=False, help=\"Phonemize text.\")\n    parser.add_argument(\n        \"--private\", type=bool, default=True, help=\"Set HuggingFace dataset to private.\"\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/scripts/create_hf_dataset/#scripts.create_hf_dataset.parse_tsv","title":"<code>parse_tsv(path)</code>","text":"<p>Join text transcripts of TSV annotation.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to TSV file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Joined text transcript.</p> Source code in <code>scripts/create_hf_dataset.py</code> <pre><code>def parse_tsv(path: str) -&gt; str:\n    \"\"\"\n    Join text transcripts of TSV annotation.\n\n    Args:\n        path (str):\n            Path to TSV file.\n\n    Returns:\n        str:\n            Joined text transcript.\n    \"\"\"\n    with open(path) as fd:\n        rows = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n        return \" \".join(row[2] for row in rows)\n</code></pre>"},{"location":"reference/scripts/data_logger/","title":"Audio Data Logger","text":""},{"location":"reference/scripts/data_logger/#usage","title":"Usage","text":"example_data_logger.sh<pre><code>python scripts/data_logger.py [-h] -u URL -i INPUT_DIR -l LABEL\n</code></pre> <pre><code>Log region-grouped total audio duration to AirTable.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -u URL, --url URL     AirTable URL.\n  -i INPUT_DIR, --input_dir INPUT_DIR\n                        Directory of input audios to log.\n  -l LABEL, --label LABEL\n                        Log record label. E.g. training/archive.\n</code></pre>"},{"location":"reference/scripts/data_logger/#example","title":"Example","text":"example_data_logger.sh<pre><code>export AIRTABLE_API_KEY=\"AIRTABLE_API_KEY\"\nexport AIRTABLE_URL=\"AIRTABLE_TABLE_URL\"\npython scripts/data_logger.py --url $AIRTABLE_URL --input_dir dropbox/ --label archive\npython scripts/data_logger.py --url $AIRTABLE_URL --input_dir training/ --label training\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger","title":"<code> scripts.data_logger.DataLogger        </code>","text":"Source code in <code>scripts/data_logger.py</code> <pre><code>class DataLogger:\n    def parse_args(self, args: List[str]) -&gt; argparse.Namespace:\n        \"\"\"\n        Utility argument parser function for data logging to AirTable.\n\n        Args:\n            args (List[str]):\n                List of arguments.\n\n        Returns:\n            argparse.Namespace:\n                Objects with arguments values as attributes.\n        \"\"\"\n        parser = argparse.ArgumentParser(\n            prog=\"python scripts/data_logger.py\",\n            description=\"Log region-grouped total audio duration to AirTable.\",\n        )\n\n        parser.add_argument(\n            \"-u\", \"--url\", type=str, required=True, help=\"AirTable URL.\"\n        )\n        parser.add_argument(\n            \"-i\",\n            \"--input_dir\",\n            type=str,\n            required=True,\n            help=\"Directory of input audios to log.\",\n        )\n        parser.add_argument(\n            \"-l\",\n            \"--label\",\n            type=str,\n            required=True,\n            help=\"Log record label. E.g. training/archive.\",\n        )\n        return parser.parse_args(args)\n\n    def get_audio_duration(self, audio_path: str) -&gt; float:\n        \"\"\"\n        Calculate audio duration via ffprobe.\n        Equivalent to:\n        ```sh title=\"example_get_audio_duration.sh\"\n        ffprobe -v quiet -of csv=p=0 -show_entries format=duration {audio_path}\n        ```\n\n        Args:\n            audio_path (str):\n                Path to audio file.\n\n        Returns:\n            float:\n                Duration in seconds.\n        \"\"\"\n        job = subprocess.run(\n            [\n                \"ffprobe\",\n                \"-v\",\n                \"quiet\",\n                \"-of\",\n                \"csv=p=0\",\n                \"-show_entries\",\n                \"format=duration\",\n                audio_path,\n            ],\n            stderr=subprocess.DEVNULL,\n            stdout=subprocess.PIPE,\n            stdin=subprocess.PIPE,\n        )\n        duration = float(job.stdout.decode())\n        return duration\n\n    def get_language_total_audio_duration(self, input_dir: str) -&gt; Dict[str, float]:\n        \"\"\"\n        Map language folders in `input_dir` to their respective total audio duration.\n        Assumes `input_dir` as `{input_dir}/{lang}/{audio}.wav`.\n\n        ### Example\n        ```pycon title=\"example_get_language_total_audio_duration.py\"\n        &gt;&gt;&gt; logger = DataLogger()\n        &gt;&gt;&gt; logger.get_language_total_audio_duration(\"dropbox/\")\n        {'en-au': 3.936, 'id-id': 3.797}\n        ```\n\n        Args:\n            input_dir (str):\n                Path to input directory.\n\n        Returns:\n            Dict[str, float]:\n                Dictionary of language to total audio duration.\n        \"\"\"\n        languages = [f.name for f in os.scandir(input_dir) if f.is_dir()]\n        language2duration = {}\n        for language in languages:\n            audios = glob(f\"{input_dir}/{language}/*.wav\")\n            duration = round(sum(p_umap(self.get_audio_duration, audios)), 3)\n            language2duration[language] = duration\n        return language2duration\n\n    def build_payload(\n        self, date: str, label: str, language: str, duration: float\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Builds payload for AirTable record.\n        AirTable record has the following structure:\n        ```pycon\n        &gt;&gt;&gt; {\n        ...     \"date\": {YYYY-MM-DD},\n        ...     \"label\": {label},\n        ...     \"language\": {lang},\n        ...     \"language-code\": {lang-country},\n        ...     \"duration\": {duration},\n        ... }\n        ```\n\n        Args:\n            date (str):\n                Logging date.\n            label (str):\n                Audio folder label.\n            language (str):\n                Language code (lang-country). E.g. `en-us`.\n            duration (float):\n                Duration in seconds.\n\n        Returns:\n            Dict[str, Dict[str, Any]]:\n                AirTable record payload.\n        \"\"\"\n        return {\n            \"fields\": {\n                \"date\": date,\n                \"label\": label,\n                \"language\": language.split(\"-\")[0],\n                \"language-code\": language,\n                \"duration\": duration,\n            }\n        }\n\n    def log(self, url: str, input_dir: str, label: str) -&gt; bool:\n        \"\"\"\n        Logs region-grouped total audio duration in `input_dir` to AirTable at `url`.\n\n        Args:\n            url (str):\n                AirTable URL.\n            input_dir (str):\n                Input directory to log.\n            label (str):\n                Log record label.\n\n        Returns:\n            bool:\n                Whether upload was a success.\n        \"\"\"\n        airtable = AirTable(url)\n        language2duration = self.get_language_total_audio_duration(input_dir)\n        records = [\n            self.build_payload(\n                str(date.today()),\n                label,\n                language,\n                duration,\n            )\n            for language, duration in language2duration.items()\n        ]\n        return airtable.batch_add_records(records)\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.build_payload","title":"<code>build_payload(self, date, label, language, duration)</code>","text":"<p>Builds payload for AirTable record. AirTable record has the following structure: <pre><code>&gt;&gt;&gt; {\n...     \"date\": {YYYY-MM-DD},\n...     \"label\": {label},\n...     \"language\": {lang},\n...     \"language-code\": {lang-country},\n...     \"duration\": {duration},\n... }\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>Logging date.</p> required <code>label</code> <code>str</code> <p>Audio folder label.</p> required <code>language</code> <code>str</code> <p>Language code (lang-country). E.g. <code>en-us</code>.</p> required <code>duration</code> <code>float</code> <p>Duration in seconds.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>AirTable record payload.</p> Source code in <code>scripts/data_logger.py</code> <pre><code>def build_payload(\n    self, date: str, label: str, language: str, duration: float\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Builds payload for AirTable record.\n    AirTable record has the following structure:\n    ```pycon\n    &gt;&gt;&gt; {\n    ...     \"date\": {YYYY-MM-DD},\n    ...     \"label\": {label},\n    ...     \"language\": {lang},\n    ...     \"language-code\": {lang-country},\n    ...     \"duration\": {duration},\n    ... }\n    ```\n\n    Args:\n        date (str):\n            Logging date.\n        label (str):\n            Audio folder label.\n        language (str):\n            Language code (lang-country). E.g. `en-us`.\n        duration (float):\n            Duration in seconds.\n\n    Returns:\n        Dict[str, Dict[str, Any]]:\n            AirTable record payload.\n    \"\"\"\n    return {\n        \"fields\": {\n            \"date\": date,\n            \"label\": label,\n            \"language\": language.split(\"-\")[0],\n            \"language-code\": language,\n            \"duration\": duration,\n        }\n    }\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.get_audio_duration","title":"<code>get_audio_duration(self, audio_path)</code>","text":"<p>Calculate audio duration via ffprobe. Equivalent to: example_get_audio_duration.sh<pre><code>ffprobe -v quiet -of csv=p=0 -show_entries format=duration {audio_path}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to audio file.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Duration in seconds.</p> Source code in <code>scripts/data_logger.py</code> <pre><code>def get_audio_duration(self, audio_path: str) -&gt; float:\n    \"\"\"\n    Calculate audio duration via ffprobe.\n    Equivalent to:\n    ```sh title=\"example_get_audio_duration.sh\"\n    ffprobe -v quiet -of csv=p=0 -show_entries format=duration {audio_path}\n    ```\n\n    Args:\n        audio_path (str):\n            Path to audio file.\n\n    Returns:\n        float:\n            Duration in seconds.\n    \"\"\"\n    job = subprocess.run(\n        [\n            \"ffprobe\",\n            \"-v\",\n            \"quiet\",\n            \"-of\",\n            \"csv=p=0\",\n            \"-show_entries\",\n            \"format=duration\",\n            audio_path,\n        ],\n        stderr=subprocess.DEVNULL,\n        stdout=subprocess.PIPE,\n        stdin=subprocess.PIPE,\n    )\n    duration = float(job.stdout.decode())\n    return duration\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.get_language_total_audio_duration","title":"<code>get_language_total_audio_duration(self, input_dir)</code>","text":"<p>Map language folders in <code>input_dir</code> to their respective total audio duration. Assumes <code>input_dir</code> as <code>{input_dir}/{lang}/{audio}.wav</code>.</p>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.get_language_total_audio_duration--example","title":"Example","text":"example_get_language_total_audio_duration.py<pre><code>&gt;&gt;&gt; logger = DataLogger()\n&gt;&gt;&gt; logger.get_language_total_audio_duration(\"dropbox/\")\n{'en-au': 3.936, 'id-id': 3.797}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to input directory.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of language to total audio duration.</p> Source code in <code>scripts/data_logger.py</code> <pre><code>def get_language_total_audio_duration(self, input_dir: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Map language folders in `input_dir` to their respective total audio duration.\n    Assumes `input_dir` as `{input_dir}/{lang}/{audio}.wav`.\n\n    ### Example\n    ```pycon title=\"example_get_language_total_audio_duration.py\"\n    &gt;&gt;&gt; logger = DataLogger()\n    &gt;&gt;&gt; logger.get_language_total_audio_duration(\"dropbox/\")\n    {'en-au': 3.936, 'id-id': 3.797}\n    ```\n\n    Args:\n        input_dir (str):\n            Path to input directory.\n\n    Returns:\n        Dict[str, float]:\n            Dictionary of language to total audio duration.\n    \"\"\"\n    languages = [f.name for f in os.scandir(input_dir) if f.is_dir()]\n    language2duration = {}\n    for language in languages:\n        audios = glob(f\"{input_dir}/{language}/*.wav\")\n        duration = round(sum(p_umap(self.get_audio_duration, audios)), 3)\n        language2duration[language] = duration\n    return language2duration\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.log","title":"<code>log(self, url, input_dir, label)</code>","text":"<p>Logs region-grouped total audio duration in <code>input_dir</code> to AirTable at <code>url</code>.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>AirTable URL.</p> required <code>input_dir</code> <code>str</code> <p>Input directory to log.</p> required <code>label</code> <code>str</code> <p>Log record label.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether upload was a success.</p> Source code in <code>scripts/data_logger.py</code> <pre><code>def log(self, url: str, input_dir: str, label: str) -&gt; bool:\n    \"\"\"\n    Logs region-grouped total audio duration in `input_dir` to AirTable at `url`.\n\n    Args:\n        url (str):\n            AirTable URL.\n        input_dir (str):\n            Input directory to log.\n        label (str):\n            Log record label.\n\n    Returns:\n        bool:\n            Whether upload was a success.\n    \"\"\"\n    airtable = AirTable(url)\n    language2duration = self.get_language_total_audio_duration(input_dir)\n    records = [\n        self.build_payload(\n            str(date.today()),\n            label,\n            language,\n            duration,\n        )\n        for language, duration in language2duration.items()\n    ]\n    return airtable.batch_add_records(records)\n</code></pre>"},{"location":"reference/scripts/data_logger/#scripts.data_logger.DataLogger.parse_args","title":"<code>parse_args(self, args)</code>","text":"<p>Utility argument parser function for data logging to AirTable.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>scripts/data_logger.py</code> <pre><code>def parse_args(self, args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for data logging to AirTable.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python scripts/data_logger.py\",\n        description=\"Log region-grouped total audio duration to AirTable.\",\n    )\n\n    parser.add_argument(\n        \"-u\", \"--url\", type=str, required=True, help=\"AirTable URL.\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Directory of input audios to log.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--label\",\n        type=str,\n        required=True,\n        help=\"Log record label. E.g. training/archive.\",\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/scripts/download_s3_bucket/","title":"S3 Bucket Downloader","text":""},{"location":"reference/scripts/download_s3_bucket/#usage","title":"Usage","text":"example_download_s3_bucket.sh<pre><code>python scripts/download_s3_bucket.py [-h] -b BUCKET -p PREFIX -o OUTPUT_DIR [-r REGION]\n</code></pre> <pre><code>Download an S3 bucket with folder prefix.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -b BUCKET, --bucket BUCKET\n                        S3 bucket name.\n  -p PREFIX, --prefix PREFIX\n                        S3 folder prefix.\n  -o OUTPUT_DIR, --output_dir OUTPUT_DIR\n                        Path to local output directory.\n  -r REGION, --region REGION\n                        AWS region name.\n</code></pre>"},{"location":"reference/scripts/download_s3_bucket/#example","title":"Example","text":"example_download_s3_bucket.sh<pre><code>python scripts/download_s3_bucket.py --bucket=\"my_bucket\" --prefix=\"recordings/\" --output_dir=\"downloads/\"\n</code></pre>"},{"location":"reference/scripts/download_s3_bucket/#scripts.download_s3_bucket","title":"<code>scripts.download_s3_bucket</code>","text":""},{"location":"reference/scripts/download_s3_bucket/#scripts.download_s3_bucket.parse_args","title":"<code>parse_args(args)</code>","text":"<p>Utility argument parser function for S3 bucket downloader.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>scripts/download_s3_bucket.py</code> <pre><code>def parse_args(args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for S3 bucket downloader.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python scripts/download_s3_bucket.py\",\n        description=\"Download an S3 bucket with folder prefix.\",\n    )\n\n    parser.add_argument(\n        \"-b\", \"--bucket\", type=str, required=True, help=\"S3 bucket name.\"\n    )\n    parser.add_argument(\n        \"-p\", \"--prefix\", type=str, required=True, help=\"S3 folder prefix.\"\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        type=str,\n        required=True,\n        help=\"Path to local output directory.\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--region\",\n        type=str,\n        default=\"ap-southeast-1\",\n        help=\"AWS region name.\",\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/scripts/upload_s3_bucket/","title":"S3 Bucket Uploader","text":""},{"location":"reference/scripts/upload_s3_bucket/#usage","title":"Usage","text":"example_upload_s3_bucket.sh<pre><code>python scripts/upload_s3_bucket.py [-h] -b BUCKET -p PREFIX -i INPUT_DIR [-r REGION]\n</code></pre> <pre><code>Upload a directory to S3 bucket.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -b BUCKET, --bucket BUCKET\n                        S3 bucket name.\n  -p PREFIX, --prefix PREFIX\n                        S3 folder prefix.\n  -i INPUT_DIR, --input_dir INPUT_DIR\n                        Path to local directory to upload.\n  -r REGION, --region REGION\n                        AWS region name.\n</code></pre>"},{"location":"reference/scripts/upload_s3_bucket/#example","title":"Example","text":"example_upload_s3_bucket.sh<pre><code>python scripts/upload_s3_bucket.py --bucket=\"my_bucket\" --prefix=\"recordings/\" --input_dir=\"uploads/\"\n</code></pre>"},{"location":"reference/scripts/upload_s3_bucket/#scripts.upload_s3_bucket","title":"<code>scripts.upload_s3_bucket</code>","text":""},{"location":"reference/scripts/upload_s3_bucket/#scripts.upload_s3_bucket.parse_args","title":"<code>parse_args(args)</code>","text":"<p>Utility argument parser function for S3 bucket uploader.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>List[str]</code> <p>List of arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Objects with arguments values as attributes.</p> Source code in <code>scripts/upload_s3_bucket.py</code> <pre><code>def parse_args(args: List[str]) -&gt; argparse.Namespace:\n    \"\"\"\n    Utility argument parser function for S3 bucket uploader.\n\n    Args:\n        args (List[str]):\n            List of arguments.\n\n    Returns:\n        argparse.Namespace:\n            Objects with arguments values as attributes.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"python scripts/upload_s3_bucket.py\",\n        description=\"Upload a directory to S3 bucket.\",\n    )\n\n    parser.add_argument(\n        \"-b\", \"--bucket\", type=str, required=True, help=\"S3 bucket name.\"\n    )\n    parser.add_argument(\n        \"-p\", \"--prefix\", type=str, required=True, help=\"S3 folder prefix.\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--input_dir\",\n        type=str,\n        required=True,\n        help=\"Path to local directory to upload.\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--region\",\n        type=str,\n        default=\"ap-southeast-1\",\n        help=\"AWS region name.\",\n    )\n    return parser.parse_args(args)\n</code></pre>"},{"location":"reference/segmenters/phoneme_overlap_segmenter/","title":"Phoneme Overlap Segmenter","text":""},{"location":"reference/segmenters/phoneme_overlap_segmenter/#speechline.segmenters.phoneme_overlap_segmenter.PhonemeOverlapSegmenter","title":"<code> speechline.segmenters.phoneme_overlap_segmenter.PhonemeOverlapSegmenter            (Segmenter)         </code>","text":"Source code in <code>speechline/segmenters/phoneme_overlap_segmenter.py</code> <pre><code>class PhonemeOverlapSegmenter(Segmenter):\n    def __init__(self, lexicon: Dict[str, List[str]]):\n        \"\"\"\n        Phoneme-overlap segmenter, with phoneme variations.\n\n        Args:\n            lexicon (Dict[str, List[str]]):\n                Lexicon of words and their phoneme variations.\n        \"\"\"\n        self.lexicon = self._normalize_lexicon(lexicon)\n\n    def _normalize_text(self, text: str) -&gt; str:\n        text = text.lower().strip()\n        return text\n\n    def _normalize_phonemes(self, phonemes: str) -&gt; str:\n        \"\"\"\n        Remove diacritics from phonemes.\n        Modified from: [Michael McAuliffe](https://memcauliffe.com/speaker-dictionaries-and-multilingual-ipa.html#multilingual-ipa-mode) # noqa: E501\n\n        Args:\n            phonemes (str):\n                Phonemes to normalize.\n\n        Returns:\n            str:\n                Normalized phonemes.\n        \"\"\"\n        diacritics = [\"\u02d0\", \"\u02d1\", \"\u0306\", \"\u032f\", \"\u0361\", \"\u203f\", \"\u035c\", \"\u0329\", \"\u02c8\", \"\u02cc\"]\n        for d in diacritics:\n            phonemes = phonemes.replace(d, \"\")\n        return phonemes.strip()\n\n    def _normalize_lexicon(self, lexicon: Dict[str, List[str]]) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Normalizes phonemes in lexicon and deduplicates.\n\n        Args:\n            lexicon (Dict[str, List[str]]):\n                Lexicon to normalize.\n\n        Returns:\n            Dict[str, List[str]]:\n                Normalized lexicon.\n        \"\"\"\n        return {word: set(self._normalize_phonemes(p) for p in phonemes) for word, phonemes in lexicon.items()}\n\n    def _merge_offsets(self, offsets: List[Dict[str, Union[str, float]]]) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Merge phoneme-level offsets into word-bounded phoneme offsets.\n\n        Args:\n            offsets (List[Dict[str, Union[str, float]]]):\n                List of phoneme offsets.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                List of word-bounded phoneme offset segments.\n        \"\"\"\n        result = []\n        current_item = {\"text\": [], \"start_time\": None, \"end_time\": None}\n        for item in offsets:\n            if item[\"text\"] != \" \":\n                if current_item[\"start_time\"] is None:\n                    current_item[\"start_time\"] = item[\"start_time\"]\n                current_item[\"end_time\"] = item[\"end_time\"]\n                current_item[\"text\"].append(item[\"text\"])\n            else:\n                if current_item[\"start_time\"] is not None:\n                    result.append(current_item)\n                    current_item = {\"text\": [], \"start_time\": None, \"end_time\": None}\n\n        if current_item[\"start_time\"] is not None:\n            result.append(current_item)\n\n        for r in result:\n            r[\"text\"] = \" \".join(r[\"text\"])\n\n        return result\n\n    def _generate_combinations(self, ground_truth: List[str]) -&gt; List[List[str]]:\n        \"\"\"\n        Generate all possible phoneme combinations for a given word.\n\n        Args:\n            ground_truth (List[str]):\n                List of words.\n\n        Returns:\n            List[List[str]]:\n                List of phoneme combinations.\n        \"\"\"\n\n        def g2p(text: str) -&gt; List[str]:\n            phonemes = []\n            for words in sentences(text):\n                for word in words:\n                    if word.is_major_break or word.is_minor_break:\n                        phonemes.append(word.text)\n                    else:\n                        phonemes.append(\" \".join(word.phonemes))\n            return phonemes\n\n        combinations = []\n        for word in ground_truth:\n            normalized_word = self._normalize_text(word)\n            if normalized_word in self.lexicon:\n                phonemes = self.lexicon[normalized_word]\n            else:\n                phonemes = g2p(normalized_word)\n            combinations.append(phonemes)\n        return combinations\n\n    def chunk_offsets(\n        self,\n        offsets: List[Dict[str, Union[str, float]]],\n        ground_truth: List[str],\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Chunk phoneme-level offsets into word-bounded phoneme offsets.\n\n        ### Example\n        ```pycon title=\"example_phoneme_overlap_segmenter.py\"\n        &gt;&gt;&gt; from speechline.segmenters import PhonemeOverlapSegmenter\n        &gt;&gt;&gt; ground_truth = [\"Her\", \"red\", \"umbrella\", \"is\", \"just\", \"the\", \"best\"]\n        &gt;&gt;&gt; lexicon = {\n        ...     \"her\": [\"h \u02c8\u025a\", \"h \u025c \u0279\", \"\u025c \u0279\", \"h \u025c\u02d0 \u0279\", \"\u0259 \u0279\"],\n        ...     \"red\": [\"\u0279 \u02c8\u025b d\", \"\u0279 \u025b d\"],\n        ...     \"umbrella\": [\"\u02c8\u028c m b \u0279 \u02cc\u025b l \u0259\", \"\u028c m b \u0279 \u025b l \u0259\"],\n        ...     \"is\": [\"\u02c8\u026a z\", \"\u026a z\"],\n        ...     \"just\": [\"d\u0361\u0292 \u02c8\u028c s t\", \"d\u0361\u0292 \u028c s t\"],\n        ...     \"the\": [\"\u00f0 \u0259\", \"\u00f0 i\", \"\u00f0 i\u02d0\", \"\u00f0 \u026a\"],\n        ...     \"best\": [\"b \u02c8\u025b s t\", \"b \u025b s t\"]\n        ... }\n        &gt;&gt;&gt; offsets = [\n        ...     {'text': 'h', 'start_time': 0.16, 'end_time': 0.18},\n        ...     {'text': '\u025d', 'start_time': 0.26, 'end_time': 0.28},\n        ...     {'text': ' ', 'start_time': 0.3, 'end_time': 0.34},\n        ...     {'text': '\u0279', 'start_time': 0.36, 'end_time': 0.38},\n        ...     {'text': '\u025b', 'start_time': 0.44, 'end_time': 0.46},\n        ...     {'text': 'd', 'start_time': 0.5, 'end_time': 0.52},\n        ...     {'text': ' ', 'start_time': 0.6, 'end_time': 0.64},\n        ...     {'text': '\u0259', 'start_time': 0.72, 'end_time': 0.74},\n        ...     {'text': 'm', 'start_time': 0.76, 'end_time': 0.78},\n        ...     {'text': 'b', 'start_time': 0.82, 'end_time': 0.84},\n        ...     {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.88},\n        ...     {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n        ...     {'text': 'l', 'start_time': 0.98, 'end_time': 1.0},\n        ...     {'text': '\u0259', 'start_time': 1.12, 'end_time': 1.14},\n        ...     {'text': ' ', 'start_time': 1.3, 'end_time': 1.34},\n        ...     {'text': '\u026a', 'start_time': 1.4, 'end_time': 1.42},\n        ...     {'text': 'z', 'start_time': 1.44, 'end_time': 1.46},\n        ...     {'text': ' ', 'start_time': 1.52, 'end_time': 1.56},\n        ...     {'text': 'd\u0292', 'start_time': 1.58, 'end_time': 1.6},\n        ...     {'text': '\u028c', 'start_time': 1.66, 'end_time': 1.68},\n        ...     {'text': 's', 'start_time': 1.7, 'end_time': 1.72},\n        ...     {'text': 't', 'start_time': 1.78, 'end_time': 1.8},\n        ...     {'text': ' ', 'start_time': 1.84, 'end_time': 1.88},\n        ...     {'text': '\u03b8', 'start_time': 1.88, 'end_time': 1.9},\n        ...     {'text': ' ', 'start_time': 1.96, 'end_time': 2.0},\n        ...     {'text': 'b', 'start_time': 2.0, 'end_time': 2.02},\n        ...     {'text': '\u025b', 'start_time': 2.12, 'end_time': 2.14},\n        ...     {'text': 's', 'start_time': 2.18, 'end_time': 2.2},\n        ...     {'text': 't', 'start_time': 2.32, 'end_time': 2.34}\n        ... ]\n        &gt;&gt;&gt; segmenter = PhonemeOverlapSegmenter(lexicon)\n        &gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n        [\n            [\n                {'text': '\u0279 \u025b d', 'start_time': 0.36, 'end_time': 0.52}\n            ],\n            [\n                {'text': '\u026a z', 'start_time': 1.4, 'end_time': 1.46},\n                {'text': 'd\u0292 \u028c s t', 'start_time': 1.58, 'end_time': 1.8}\n            ],\n            [\n                {'text': 'b \u025b s t', 'start_time': 2.0, 'end_time': 2.34}\n            ]\n        ]\n        ```\n\n        Args:\n            offsets (List[Dict[str, Union[str, float]]]):\n                List of phoneme offsets.\n            ground_truth (List[str]):\n                List of words.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                List of word-bounded phoneme offset segments.\n        \"\"\"\n\n        ground_truth = self._generate_combinations(ground_truth)\n        merged_offsets = self._merge_offsets(offsets)\n        transcripts = [self._normalize_phonemes(o[\"text\"]) for o in merged_offsets]\n\n        idxs, index = [], 0  # index in ground truth\n        for i, word in enumerate(transcripts):\n            if index &gt;= len(ground_truth):\n                break\n            for var in ground_truth[index:]:\n                # match\n                if word in var:\n                    idxs.append(i)\n                    break\n            index += 1\n\n        # if no matches\n        if not idxs:\n            return []\n\n        # collapse longest consecutive indices\n        merged_idxs = []\n        start, end = idxs[0], idxs[0] + 1\n        for i in idxs[1:]:\n            if i == end:\n                end += 1\n            else:\n                merged_idxs.append((start, end))\n                start, end = i, i + 1\n        merged_idxs.append((start, end))\n\n        # segment according to longest consecutive indices\n        segments = [merged_offsets[i:j] for (i, j) in merged_idxs]\n        return segments\n</code></pre>"},{"location":"reference/segmenters/phoneme_overlap_segmenter/#speechline.segmenters.phoneme_overlap_segmenter.PhonemeOverlapSegmenter.__init__","title":"<code>__init__(self, lexicon)</code>  <code>special</code>","text":"<p>Phoneme-overlap segmenter, with phoneme variations.</p> <p>Parameters:</p> Name Type Description Default <code>lexicon</code> <code>Dict[str, List[str]]</code> <p>Lexicon of words and their phoneme variations.</p> required Source code in <code>speechline/segmenters/phoneme_overlap_segmenter.py</code> <pre><code>def __init__(self, lexicon: Dict[str, List[str]]):\n    \"\"\"\n    Phoneme-overlap segmenter, with phoneme variations.\n\n    Args:\n        lexicon (Dict[str, List[str]]):\n            Lexicon of words and their phoneme variations.\n    \"\"\"\n    self.lexicon = self._normalize_lexicon(lexicon)\n</code></pre>"},{"location":"reference/segmenters/phoneme_overlap_segmenter/#speechline.segmenters.phoneme_overlap_segmenter.PhonemeOverlapSegmenter.chunk_offsets","title":"<code>chunk_offsets(self, offsets, ground_truth, **kwargs)</code>","text":"<p>Chunk phoneme-level offsets into word-bounded phoneme offsets.</p>"},{"location":"reference/segmenters/phoneme_overlap_segmenter/#speechline.segmenters.phoneme_overlap_segmenter.PhonemeOverlapSegmenter.chunk_offsets--example","title":"Example","text":"example_phoneme_overlap_segmenter.py<pre><code>&gt;&gt;&gt; from speechline.segmenters import PhonemeOverlapSegmenter\n&gt;&gt;&gt; ground_truth = [\"Her\", \"red\", \"umbrella\", \"is\", \"just\", \"the\", \"best\"]\n&gt;&gt;&gt; lexicon = {\n...     \"her\": [\"h \u02c8\u025a\", \"h \u025c \u0279\", \"\u025c \u0279\", \"h \u025c\u02d0 \u0279\", \"\u0259 \u0279\"],\n...     \"red\": [\"\u0279 \u02c8\u025b d\", \"\u0279 \u025b d\"],\n...     \"umbrella\": [\"\u02c8\u028c m b \u0279 \u02cc\u025b l \u0259\", \"\u028c m b \u0279 \u025b l \u0259\"],\n...     \"is\": [\"\u02c8\u026a z\", \"\u026a z\"],\n...     \"just\": [\"d\u0361\u0292 \u02c8\u028c s t\", \"d\u0361\u0292 \u028c s t\"],\n...     \"the\": [\"\u00f0 \u0259\", \"\u00f0 i\", \"\u00f0 i\u02d0\", \"\u00f0 \u026a\"],\n...     \"best\": [\"b \u02c8\u025b s t\", \"b \u025b s t\"]\n... }\n&gt;&gt;&gt; offsets = [\n...     {'text': 'h', 'start_time': 0.16, 'end_time': 0.18},\n...     {'text': '\u025d', 'start_time': 0.26, 'end_time': 0.28},\n...     {'text': ' ', 'start_time': 0.3, 'end_time': 0.34},\n...     {'text': '\u0279', 'start_time': 0.36, 'end_time': 0.38},\n...     {'text': '\u025b', 'start_time': 0.44, 'end_time': 0.46},\n...     {'text': 'd', 'start_time': 0.5, 'end_time': 0.52},\n...     {'text': ' ', 'start_time': 0.6, 'end_time': 0.64},\n...     {'text': '\u0259', 'start_time': 0.72, 'end_time': 0.74},\n...     {'text': 'm', 'start_time': 0.76, 'end_time': 0.78},\n...     {'text': 'b', 'start_time': 0.82, 'end_time': 0.84},\n...     {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.88},\n...     {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n...     {'text': 'l', 'start_time': 0.98, 'end_time': 1.0},\n...     {'text': '\u0259', 'start_time': 1.12, 'end_time': 1.14},\n...     {'text': ' ', 'start_time': 1.3, 'end_time': 1.34},\n...     {'text': '\u026a', 'start_time': 1.4, 'end_time': 1.42},\n...     {'text': 'z', 'start_time': 1.44, 'end_time': 1.46},\n...     {'text': ' ', 'start_time': 1.52, 'end_time': 1.56},\n...     {'text': 'd\u0292', 'start_time': 1.58, 'end_time': 1.6},\n...     {'text': '\u028c', 'start_time': 1.66, 'end_time': 1.68},\n...     {'text': 's', 'start_time': 1.7, 'end_time': 1.72},\n...     {'text': 't', 'start_time': 1.78, 'end_time': 1.8},\n...     {'text': ' ', 'start_time': 1.84, 'end_time': 1.88},\n...     {'text': '\u03b8', 'start_time': 1.88, 'end_time': 1.9},\n...     {'text': ' ', 'start_time': 1.96, 'end_time': 2.0},\n...     {'text': 'b', 'start_time': 2.0, 'end_time': 2.02},\n...     {'text': '\u025b', 'start_time': 2.12, 'end_time': 2.14},\n...     {'text': 's', 'start_time': 2.18, 'end_time': 2.2},\n...     {'text': 't', 'start_time': 2.32, 'end_time': 2.34}\n... ]\n&gt;&gt;&gt; segmenter = PhonemeOverlapSegmenter(lexicon)\n&gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n[\n    [\n        {'text': '\u0279 \u025b d', 'start_time': 0.36, 'end_time': 0.52}\n    ],\n    [\n        {'text': '\u026a z', 'start_time': 1.4, 'end_time': 1.46},\n        {'text': 'd\u0292 \u028c s t', 'start_time': 1.58, 'end_time': 1.8}\n    ],\n    [\n        {'text': 'b \u025b s t', 'start_time': 2.0, 'end_time': 2.34}\n    ]\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>List of phoneme offsets.</p> required <code>ground_truth</code> <code>List[str]</code> <p>List of words.</p> required <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of word-bounded phoneme offset segments.</p> Source code in <code>speechline/segmenters/phoneme_overlap_segmenter.py</code> <pre><code>def chunk_offsets(\n    self,\n    offsets: List[Dict[str, Union[str, float]]],\n    ground_truth: List[str],\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Chunk phoneme-level offsets into word-bounded phoneme offsets.\n\n    ### Example\n    ```pycon title=\"example_phoneme_overlap_segmenter.py\"\n    &gt;&gt;&gt; from speechline.segmenters import PhonemeOverlapSegmenter\n    &gt;&gt;&gt; ground_truth = [\"Her\", \"red\", \"umbrella\", \"is\", \"just\", \"the\", \"best\"]\n    &gt;&gt;&gt; lexicon = {\n    ...     \"her\": [\"h \u02c8\u025a\", \"h \u025c \u0279\", \"\u025c \u0279\", \"h \u025c\u02d0 \u0279\", \"\u0259 \u0279\"],\n    ...     \"red\": [\"\u0279 \u02c8\u025b d\", \"\u0279 \u025b d\"],\n    ...     \"umbrella\": [\"\u02c8\u028c m b \u0279 \u02cc\u025b l \u0259\", \"\u028c m b \u0279 \u025b l \u0259\"],\n    ...     \"is\": [\"\u02c8\u026a z\", \"\u026a z\"],\n    ...     \"just\": [\"d\u0361\u0292 \u02c8\u028c s t\", \"d\u0361\u0292 \u028c s t\"],\n    ...     \"the\": [\"\u00f0 \u0259\", \"\u00f0 i\", \"\u00f0 i\u02d0\", \"\u00f0 \u026a\"],\n    ...     \"best\": [\"b \u02c8\u025b s t\", \"b \u025b s t\"]\n    ... }\n    &gt;&gt;&gt; offsets = [\n    ...     {'text': 'h', 'start_time': 0.16, 'end_time': 0.18},\n    ...     {'text': '\u025d', 'start_time': 0.26, 'end_time': 0.28},\n    ...     {'text': ' ', 'start_time': 0.3, 'end_time': 0.34},\n    ...     {'text': '\u0279', 'start_time': 0.36, 'end_time': 0.38},\n    ...     {'text': '\u025b', 'start_time': 0.44, 'end_time': 0.46},\n    ...     {'text': 'd', 'start_time': 0.5, 'end_time': 0.52},\n    ...     {'text': ' ', 'start_time': 0.6, 'end_time': 0.64},\n    ...     {'text': '\u0259', 'start_time': 0.72, 'end_time': 0.74},\n    ...     {'text': 'm', 'start_time': 0.76, 'end_time': 0.78},\n    ...     {'text': 'b', 'start_time': 0.82, 'end_time': 0.84},\n    ...     {'text': '\u0279', 'start_time': 0.84, 'end_time': 0.88},\n    ...     {'text': '\u025b', 'start_time': 0.92, 'end_time': 0.94},\n    ...     {'text': 'l', 'start_time': 0.98, 'end_time': 1.0},\n    ...     {'text': '\u0259', 'start_time': 1.12, 'end_time': 1.14},\n    ...     {'text': ' ', 'start_time': 1.3, 'end_time': 1.34},\n    ...     {'text': '\u026a', 'start_time': 1.4, 'end_time': 1.42},\n    ...     {'text': 'z', 'start_time': 1.44, 'end_time': 1.46},\n    ...     {'text': ' ', 'start_time': 1.52, 'end_time': 1.56},\n    ...     {'text': 'd\u0292', 'start_time': 1.58, 'end_time': 1.6},\n    ...     {'text': '\u028c', 'start_time': 1.66, 'end_time': 1.68},\n    ...     {'text': 's', 'start_time': 1.7, 'end_time': 1.72},\n    ...     {'text': 't', 'start_time': 1.78, 'end_time': 1.8},\n    ...     {'text': ' ', 'start_time': 1.84, 'end_time': 1.88},\n    ...     {'text': '\u03b8', 'start_time': 1.88, 'end_time': 1.9},\n    ...     {'text': ' ', 'start_time': 1.96, 'end_time': 2.0},\n    ...     {'text': 'b', 'start_time': 2.0, 'end_time': 2.02},\n    ...     {'text': '\u025b', 'start_time': 2.12, 'end_time': 2.14},\n    ...     {'text': 's', 'start_time': 2.18, 'end_time': 2.2},\n    ...     {'text': 't', 'start_time': 2.32, 'end_time': 2.34}\n    ... ]\n    &gt;&gt;&gt; segmenter = PhonemeOverlapSegmenter(lexicon)\n    &gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n    [\n        [\n            {'text': '\u0279 \u025b d', 'start_time': 0.36, 'end_time': 0.52}\n        ],\n        [\n            {'text': '\u026a z', 'start_time': 1.4, 'end_time': 1.46},\n            {'text': 'd\u0292 \u028c s t', 'start_time': 1.58, 'end_time': 1.8}\n        ],\n        [\n            {'text': 'b \u025b s t', 'start_time': 2.0, 'end_time': 2.34}\n        ]\n    ]\n    ```\n\n    Args:\n        offsets (List[Dict[str, Union[str, float]]]):\n            List of phoneme offsets.\n        ground_truth (List[str]):\n            List of words.\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            List of word-bounded phoneme offset segments.\n    \"\"\"\n\n    ground_truth = self._generate_combinations(ground_truth)\n    merged_offsets = self._merge_offsets(offsets)\n    transcripts = [self._normalize_phonemes(o[\"text\"]) for o in merged_offsets]\n\n    idxs, index = [], 0  # index in ground truth\n    for i, word in enumerate(transcripts):\n        if index &gt;= len(ground_truth):\n            break\n        for var in ground_truth[index:]:\n            # match\n            if word in var:\n                idxs.append(i)\n                break\n        index += 1\n\n    # if no matches\n    if not idxs:\n        return []\n\n    # collapse longest consecutive indices\n    merged_idxs = []\n    start, end = idxs[0], idxs[0] + 1\n    for i in idxs[1:]:\n        if i == end:\n            end += 1\n        else:\n            merged_idxs.append((start, end))\n            start, end = i, i + 1\n    merged_idxs.append((start, end))\n\n    # segment according to longest consecutive indices\n    segments = [merged_offsets[i:j] for (i, j) in merged_idxs]\n    return segments\n</code></pre>"},{"location":"reference/segmenters/segmenter/","title":"Audio Segmenter","text":""},{"location":"reference/segmenters/segmenter/#speechline.segmenters.segmenter.Segmenter","title":"<code> speechline.segmenters.segmenter.Segmenter        </code>","text":"Source code in <code>speechline/segmenters/segmenter.py</code> <pre><code>class Segmenter:\n    def chunk_audio_segments(\n        self,\n        audio_path: str,\n        outdir: str,\n        offsets: List[Dict[str, Union[str, float]]],\n        do_noise_classify: bool = False,\n        minimum_chunk_duration: float = 1.0,\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Chunks an audio file based on its offsets.\n        Generates and exports WAV audio chunks and aligned TSV phoneme transcripts.\n\n        Args:\n            audio_path (str):\n                Path to audio file to chunk.\n            outdir (str):\n                Output directory to save chunked audio.\n                Per-region subfolders will be generated under this directory.\n            offsets (List[Dict[str, Union[str, float]]]):\n                List of phoneme offsets.\n            do_noise_classify (bool, optional):\n                Whether to perform noise classification on empty chunks.\n                Defaults to `False`.\n            minimum_chunk_duration (float, optional):\n                Minimum chunk duration (in seconds) to be exported.\n                Defaults to 0.3 second.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                List of offsets for every segment.\n        \"\"\"\n        segments = self.chunk_offsets(offsets, **kwargs)\n        # skip empty segments (undetected transcripts)\n        if len(segments) == 0:\n            return [[{}]]\n\n        if do_noise_classify:\n            segments = self.insert_empty_tags(segments, **kwargs)\n            segments = self.classify_noise(segments, audio_path, **kwargs)\n\n        if isinstance(audio_path, str):\n            audio = AudioSegment.from_file(audio_path)\n        elif isinstance(audio_path, dict):\n            audio = np_f32_to_pydub(audio_path)\n            audio_path = audio_path[\"path\"]\n\n        audio_segments: List[AudioSegment] = [\n            audio[s[0][\"start_time\"] * 1000 : s[-1][\"end_time\"] * 1000] for s in segments\n        ]\n\n        # shift segments based on their respective index start times\n        shifted_segments = [self._shift_offsets(segment) for segment in segments]\n\n        # create output directory folder and subfolders\n        os.makedirs(get_outdir_path(audio_path, outdir), exist_ok=True)\n\n        for idx, (segment, audio_segment) in enumerate(zip(shifted_segments, audio_segments)):\n            # skip export if audio segment does not meet minimum chunk duration\n            if len(audio_segment) &lt; minimum_chunk_duration * 1000:\n                continue\n\n            # export TSV transcripts and WAV audio segment\n            output_tsv_path = get_chunk_path(audio_path, outdir, idx, \"tsv\")\n            export_segment_transcripts_tsv(output_tsv_path, segment)\n\n            output_audio_path = get_chunk_path(audio_path, outdir, idx, \"wav\")\n            export_segment_audio_wav(output_audio_path, audio_segment)\n\n        return shifted_segments\n\n    def classify_noise(\n        self,\n        segments: List[List[Dict[str, Union[str, float]]]],\n        audio_path: str,\n        noise_classifier: AudioModule,\n        noise_classifier_threshold: float,\n        empty_tag: str = \"&lt;EMPTY&gt;\",\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Classify empty tags as noise.\n\n        Args:\n            segments (List[List[Dict[str, Union[str, float]]]]):\n                List of chunked segments with empty tag.\n            audio_path (str):\n                Path to audio file to chunk.\n            noise_classifier (AudioModule):\n                Audio Module to perform noise classification.\n            noise_classifier_threshold (float):\n                Minimum probability threshold for multi label classification.\n            empty_tag (str, optional):\n                Special empty tag.\n                Defaults to `\"&lt;EMPTY&gt;\"`.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                Chunk segments with classified noise tags.\n        \"\"\"\n        pos, empty_tag_pos = 0, {}\n        for i, segment in enumerate(segments):\n            for j, offset in enumerate(segment):\n                if offset[\"text\"] == empty_tag:\n                    empty_tag_pos[pos] = (i, j)\n                    pos += 1\n\n        # return original segments if no empty tags\n        if len(empty_tag_pos) == 0:\n            return segments\n\n        audio = AudioSegment.from_file(audio_path)\n        audio_arrays = [\n            {\n                \"path\": None,\n                \"array\": pydub_to_np(audio[offset[\"start_time\"] * 1000 : offset[\"end_time\"] * 1000]),\n                \"sampling_rate\": audio.frame_rate,\n            }\n            for segment in segments\n            for offset in segment\n            if offset[\"text\"] == empty_tag\n        ]\n\n        dataset = Dataset.from_dict({\"audio\": audio_arrays})\n        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=noise_classifier.sampling_rate))\n\n        outputs = noise_classifier.predict(dataset, threshold=noise_classifier_threshold)\n\n        for idx, predictions in enumerate(outputs):\n            if len(predictions) &gt; 0:\n                i, j = empty_tag_pos[idx]\n                offset = segments[i][j]\n                label = max(predictions, key=lambda item: item[\"score\"])[\"label\"]\n                offset[\"text\"] = f\"&lt;{label}&gt;\"\n\n        return segments\n\n    def insert_empty_tags(\n        self,\n        segments: List[List[Dict[str, Union[str, float]]]],\n        minimum_empty_duration: float,\n        empty_tag: str = \"&lt;EMPTY&gt;\",\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Inserts special `&lt;EMPTY&gt;` tag to mark for noise classification.\n        Inserts tags at indices in segments where empty duration\n        is at least `minimum_empty_duration`.\n\n        Args:\n            segments (List[List[Dict[str, Union[str, float]]]]):\n                List of chunked segments to insert into.\n            minimum_empty_duration (float):\n                Minimum silence duration in seconds.\n            empty_tag (str, optional):\n                Special empty tag.\n                Defaults to \"&lt;EMPTY&gt;\".\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                Updated segments where empty tags have been inserted.\n        \"\"\"\n        for segment in segments:\n            gaps = [round(next[\"start_time\"] - curr[\"end_time\"], 3) for curr, next in zip(segment, segment[1:])]\n\n            for idx, gap in reversed(list(enumerate(gaps))):\n                if gap &gt;= minimum_empty_duration:\n                    start_time = segment[idx][\"end_time\"]\n                    end_time = segment[idx + 1][\"start_time\"]\n                    empty_offset = {\n                        \"text\": empty_tag,\n                        \"start_time\": start_time,\n                        \"end_time\": end_time,\n                    }\n                    segment.insert(idx + 1, empty_offset)\n        return segments\n\n    def _shift_offsets(self, offset: List[Dict[str, Union[str, float]]]) -&gt; List[Dict[str, Union[str, float]]]:\n        \"\"\"\n        Shift start and end time of offsets by index start time.\n        Subtracts all start and end times by index start time.\n\n        Args:\n            offset (List[Dict[str, Union[str, float]]]):\n                Offsets to shift.\n\n        Returns:\n            List[Dict[str, Union[str, float]]]:\n                Shifted offsets.\n        \"\"\"\n        index_start = offset[0][\"start_time\"]\n        shifted_offset = [\n            {\n                \"text\": o[\"text\"],\n                \"start_time\": round(o[\"start_time\"] - index_start, 3),\n                \"end_time\": round(o[\"end_time\"] - index_start, 3),\n            }\n            for o in offset\n        ]\n        return shifted_offset\n</code></pre>"},{"location":"reference/segmenters/segmenter/#speechline.segmenters.segmenter.Segmenter.chunk_audio_segments","title":"<code>chunk_audio_segments(self, audio_path, outdir, offsets, do_noise_classify=False, minimum_chunk_duration=1.0, **kwargs)</code>","text":"<p>Chunks an audio file based on its offsets. Generates and exports WAV audio chunks and aligned TSV phoneme transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>audio_path</code> <code>str</code> <p>Path to audio file to chunk.</p> required <code>outdir</code> <code>str</code> <p>Output directory to save chunked audio. Per-region subfolders will be generated under this directory.</p> required <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>List of phoneme offsets.</p> required <code>do_noise_classify</code> <code>bool</code> <p>Whether to perform noise classification on empty chunks. Defaults to <code>False</code>.</p> <code>False</code> <code>minimum_chunk_duration</code> <code>float</code> <p>Minimum chunk duration (in seconds) to be exported. Defaults to 0.3 second.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of offsets for every segment.</p> Source code in <code>speechline/segmenters/segmenter.py</code> <pre><code>def chunk_audio_segments(\n    self,\n    audio_path: str,\n    outdir: str,\n    offsets: List[Dict[str, Union[str, float]]],\n    do_noise_classify: bool = False,\n    minimum_chunk_duration: float = 1.0,\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Chunks an audio file based on its offsets.\n    Generates and exports WAV audio chunks and aligned TSV phoneme transcripts.\n\n    Args:\n        audio_path (str):\n            Path to audio file to chunk.\n        outdir (str):\n            Output directory to save chunked audio.\n            Per-region subfolders will be generated under this directory.\n        offsets (List[Dict[str, Union[str, float]]]):\n            List of phoneme offsets.\n        do_noise_classify (bool, optional):\n            Whether to perform noise classification on empty chunks.\n            Defaults to `False`.\n        minimum_chunk_duration (float, optional):\n            Minimum chunk duration (in seconds) to be exported.\n            Defaults to 0.3 second.\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            List of offsets for every segment.\n    \"\"\"\n    segments = self.chunk_offsets(offsets, **kwargs)\n    # skip empty segments (undetected transcripts)\n    if len(segments) == 0:\n        return [[{}]]\n\n    if do_noise_classify:\n        segments = self.insert_empty_tags(segments, **kwargs)\n        segments = self.classify_noise(segments, audio_path, **kwargs)\n\n    if isinstance(audio_path, str):\n        audio = AudioSegment.from_file(audio_path)\n    elif isinstance(audio_path, dict):\n        audio = np_f32_to_pydub(audio_path)\n        audio_path = audio_path[\"path\"]\n\n    audio_segments: List[AudioSegment] = [\n        audio[s[0][\"start_time\"] * 1000 : s[-1][\"end_time\"] * 1000] for s in segments\n    ]\n\n    # shift segments based on their respective index start times\n    shifted_segments = [self._shift_offsets(segment) for segment in segments]\n\n    # create output directory folder and subfolders\n    os.makedirs(get_outdir_path(audio_path, outdir), exist_ok=True)\n\n    for idx, (segment, audio_segment) in enumerate(zip(shifted_segments, audio_segments)):\n        # skip export if audio segment does not meet minimum chunk duration\n        if len(audio_segment) &lt; minimum_chunk_duration * 1000:\n            continue\n\n        # export TSV transcripts and WAV audio segment\n        output_tsv_path = get_chunk_path(audio_path, outdir, idx, \"tsv\")\n        export_segment_transcripts_tsv(output_tsv_path, segment)\n\n        output_audio_path = get_chunk_path(audio_path, outdir, idx, \"wav\")\n        export_segment_audio_wav(output_audio_path, audio_segment)\n\n    return shifted_segments\n</code></pre>"},{"location":"reference/segmenters/segmenter/#speechline.segmenters.segmenter.Segmenter.classify_noise","title":"<code>classify_noise(self, segments, audio_path, noise_classifier, noise_classifier_threshold, empty_tag='&lt;EMPTY&gt;', **kwargs)</code>","text":"<p>Classify empty tags as noise.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of chunked segments with empty tag.</p> required <code>audio_path</code> <code>str</code> <p>Path to audio file to chunk.</p> required <code>noise_classifier</code> <code>AudioModule</code> <p>Audio Module to perform noise classification.</p> required <code>noise_classifier_threshold</code> <code>float</code> <p>Minimum probability threshold for multi label classification.</p> required <code>empty_tag</code> <code>str</code> <p>Special empty tag. Defaults to <code>\"&lt;EMPTY&gt;\"</code>.</p> <code>'&lt;EMPTY&gt;'</code> <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>Chunk segments with classified noise tags.</p> Source code in <code>speechline/segmenters/segmenter.py</code> <pre><code>def classify_noise(\n    self,\n    segments: List[List[Dict[str, Union[str, float]]]],\n    audio_path: str,\n    noise_classifier: AudioModule,\n    noise_classifier_threshold: float,\n    empty_tag: str = \"&lt;EMPTY&gt;\",\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Classify empty tags as noise.\n\n    Args:\n        segments (List[List[Dict[str, Union[str, float]]]]):\n            List of chunked segments with empty tag.\n        audio_path (str):\n            Path to audio file to chunk.\n        noise_classifier (AudioModule):\n            Audio Module to perform noise classification.\n        noise_classifier_threshold (float):\n            Minimum probability threshold for multi label classification.\n        empty_tag (str, optional):\n            Special empty tag.\n            Defaults to `\"&lt;EMPTY&gt;\"`.\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            Chunk segments with classified noise tags.\n    \"\"\"\n    pos, empty_tag_pos = 0, {}\n    for i, segment in enumerate(segments):\n        for j, offset in enumerate(segment):\n            if offset[\"text\"] == empty_tag:\n                empty_tag_pos[pos] = (i, j)\n                pos += 1\n\n    # return original segments if no empty tags\n    if len(empty_tag_pos) == 0:\n        return segments\n\n    audio = AudioSegment.from_file(audio_path)\n    audio_arrays = [\n        {\n            \"path\": None,\n            \"array\": pydub_to_np(audio[offset[\"start_time\"] * 1000 : offset[\"end_time\"] * 1000]),\n            \"sampling_rate\": audio.frame_rate,\n        }\n        for segment in segments\n        for offset in segment\n        if offset[\"text\"] == empty_tag\n    ]\n\n    dataset = Dataset.from_dict({\"audio\": audio_arrays})\n    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=noise_classifier.sampling_rate))\n\n    outputs = noise_classifier.predict(dataset, threshold=noise_classifier_threshold)\n\n    for idx, predictions in enumerate(outputs):\n        if len(predictions) &gt; 0:\n            i, j = empty_tag_pos[idx]\n            offset = segments[i][j]\n            label = max(predictions, key=lambda item: item[\"score\"])[\"label\"]\n            offset[\"text\"] = f\"&lt;{label}&gt;\"\n\n    return segments\n</code></pre>"},{"location":"reference/segmenters/segmenter/#speechline.segmenters.segmenter.Segmenter.insert_empty_tags","title":"<code>insert_empty_tags(self, segments, minimum_empty_duration, empty_tag='&lt;EMPTY&gt;', **kwargs)</code>","text":"<p>Inserts special <code>&lt;EMPTY&gt;</code> tag to mark for noise classification. Inserts tags at indices in segments where empty duration is at least <code>minimum_empty_duration</code>.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of chunked segments to insert into.</p> required <code>minimum_empty_duration</code> <code>float</code> <p>Minimum silence duration in seconds.</p> required <code>empty_tag</code> <code>str</code> <p>Special empty tag. Defaults to \"\". <code>'&lt;EMPTY&gt;'</code> <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>Updated segments where empty tags have been inserted.</p> Source code in <code>speechline/segmenters/segmenter.py</code> <pre><code>def insert_empty_tags(\n    self,\n    segments: List[List[Dict[str, Union[str, float]]]],\n    minimum_empty_duration: float,\n    empty_tag: str = \"&lt;EMPTY&gt;\",\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Inserts special `&lt;EMPTY&gt;` tag to mark for noise classification.\n    Inserts tags at indices in segments where empty duration\n    is at least `minimum_empty_duration`.\n\n    Args:\n        segments (List[List[Dict[str, Union[str, float]]]]):\n            List of chunked segments to insert into.\n        minimum_empty_duration (float):\n            Minimum silence duration in seconds.\n        empty_tag (str, optional):\n            Special empty tag.\n            Defaults to \"&lt;EMPTY&gt;\".\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            Updated segments where empty tags have been inserted.\n    \"\"\"\n    for segment in segments:\n        gaps = [round(next[\"start_time\"] - curr[\"end_time\"], 3) for curr, next in zip(segment, segment[1:])]\n\n        for idx, gap in reversed(list(enumerate(gaps))):\n            if gap &gt;= minimum_empty_duration:\n                start_time = segment[idx][\"end_time\"]\n                end_time = segment[idx + 1][\"start_time\"]\n                empty_offset = {\n                    \"text\": empty_tag,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                }\n                segment.insert(idx + 1, empty_offset)\n    return segments\n</code></pre>"},{"location":"reference/segmenters/silence_segmenter/","title":"Silence Segmenter","text":""},{"location":"reference/segmenters/silence_segmenter/#speechline.segmenters.silence_segmenter.SilenceSegmenter","title":"<code> speechline.segmenters.silence_segmenter.SilenceSegmenter            (Segmenter)         </code>","text":"Source code in <code>speechline/segmenters/silence_segmenter.py</code> <pre><code>class SilenceSegmenter(Segmenter):\n    def chunk_offsets(\n        self,\n        offsets: List[Dict[str, Union[str, float]]],\n        silence_duration: float,\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Chunk transcript offsets based on in-between silence duration.\n\n        ### Example\n        ```pycon title=\"example_silence_segmenter.py\"\n        &gt;&gt;&gt; from speechline.segmenters import SilenceSegmenter\n        &gt;&gt;&gt; segmenter = SilenceSegmenter()\n        &gt;&gt;&gt; offsets = [\n        ...     {\"start_time\": 0.0, \"end_time\": 0.4},\n        ...     {\"start_time\": 0.4, \"end_time\": 0.5},\n        ...     {\"start_time\": 0.6, \"end_time\": 0.8},\n        ...     {\"start_time\": 1.1, \"end_time\": 1.3},\n        ...     {\"start_time\": 1.5, \"end_time\": 1.7},\n        ...     {\"start_time\": 1.7, \"end_time\": 2.0},\n        ... ]\n        &gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.2)\n        [\n            [\n                {\"start_time\": 0.0, \"end_time\": 0.4},\n                {\"start_time\": 0.4, \"end_time\": 0.5},\n                {\"start_time\": 0.6, \"end_time\": 0.8},\n            ],\n            [\n                {\"start_time\": 1.1, \"end_time\": 1.3}\n            ],\n            [\n                {\"start_time\": 1.5, \"end_time\": 1.7},\n                {\"start_time\": 1.7, \"end_time\": 2.0}\n            ],\n        ]\n        &gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.1)\n        [\n            [\n                {\"start_time\": 0.0, \"end_time\": 0.4},\n                {\"start_time\": 0.4, \"end_time\": 0.5}\n            ],\n            [\n                {\"start_time\": 0.6, \"end_time\": 0.8}\n            ],\n            [\n                {\"start_time\": 1.1, \"end_time\": 1.3}\n            ],\n            [\n                {\"start_time\": 1.5, \"end_time\": 1.7},\n                {\"start_time\": 1.7, \"end_time\": 2.0}\n            ],\n        ]\n        ```\n\n        Args:\n            offsets (List[Dict[str, Union[str, float]]]):\n                Offsets to chunk.\n            silence_duration (float):\n                Minimum in-between silence duration (in seconds) to consider as gaps.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                List of chunked/segmented offsets.\n        \"\"\"\n        # calculate gaps in between offsets\n        gaps = [\n            round(next[\"start_time\"] - curr[\"end_time\"], 3)\n            for curr, next in zip(offsets, offsets[1:])\n        ]\n        # generate segment slices (start and end indices) based on silence\n        slices = (\n            [0]\n            + [idx + 1 for idx, gap in enumerate(gaps) if gap &gt;= silence_duration]\n            + [len(offsets)]\n        )\n        # group consecutive offsets (segments) based on slices\n        segments = [offsets[i:j] for i, j in zip(slices, slices[1:])]\n        return segments\n</code></pre>"},{"location":"reference/segmenters/silence_segmenter/#speechline.segmenters.silence_segmenter.SilenceSegmenter.chunk_offsets","title":"<code>chunk_offsets(self, offsets, silence_duration, **kwargs)</code>","text":"<p>Chunk transcript offsets based on in-between silence duration.</p>"},{"location":"reference/segmenters/silence_segmenter/#speechline.segmenters.silence_segmenter.SilenceSegmenter.chunk_offsets--example","title":"Example","text":"example_silence_segmenter.py<pre><code>&gt;&gt;&gt; from speechline.segmenters import SilenceSegmenter\n&gt;&gt;&gt; segmenter = SilenceSegmenter()\n&gt;&gt;&gt; offsets = [\n...     {\"start_time\": 0.0, \"end_time\": 0.4},\n...     {\"start_time\": 0.4, \"end_time\": 0.5},\n...     {\"start_time\": 0.6, \"end_time\": 0.8},\n...     {\"start_time\": 1.1, \"end_time\": 1.3},\n...     {\"start_time\": 1.5, \"end_time\": 1.7},\n...     {\"start_time\": 1.7, \"end_time\": 2.0},\n... ]\n&gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.2)\n[\n    [\n        {\"start_time\": 0.0, \"end_time\": 0.4},\n        {\"start_time\": 0.4, \"end_time\": 0.5},\n        {\"start_time\": 0.6, \"end_time\": 0.8},\n    ],\n    [\n        {\"start_time\": 1.1, \"end_time\": 1.3}\n    ],\n    [\n        {\"start_time\": 1.5, \"end_time\": 1.7},\n        {\"start_time\": 1.7, \"end_time\": 2.0}\n    ],\n]\n&gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.1)\n[\n    [\n        {\"start_time\": 0.0, \"end_time\": 0.4},\n        {\"start_time\": 0.4, \"end_time\": 0.5}\n    ],\n    [\n        {\"start_time\": 0.6, \"end_time\": 0.8}\n    ],\n    [\n        {\"start_time\": 1.1, \"end_time\": 1.3}\n    ],\n    [\n        {\"start_time\": 1.5, \"end_time\": 1.7},\n        {\"start_time\": 1.7, \"end_time\": 2.0}\n    ],\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>Offsets to chunk.</p> required <code>silence_duration</code> <code>float</code> <p>Minimum in-between silence duration (in seconds) to consider as gaps.</p> required <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of chunked/segmented offsets.</p> Source code in <code>speechline/segmenters/silence_segmenter.py</code> <pre><code>def chunk_offsets(\n    self,\n    offsets: List[Dict[str, Union[str, float]]],\n    silence_duration: float,\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Chunk transcript offsets based on in-between silence duration.\n\n    ### Example\n    ```pycon title=\"example_silence_segmenter.py\"\n    &gt;&gt;&gt; from speechline.segmenters import SilenceSegmenter\n    &gt;&gt;&gt; segmenter = SilenceSegmenter()\n    &gt;&gt;&gt; offsets = [\n    ...     {\"start_time\": 0.0, \"end_time\": 0.4},\n    ...     {\"start_time\": 0.4, \"end_time\": 0.5},\n    ...     {\"start_time\": 0.6, \"end_time\": 0.8},\n    ...     {\"start_time\": 1.1, \"end_time\": 1.3},\n    ...     {\"start_time\": 1.5, \"end_time\": 1.7},\n    ...     {\"start_time\": 1.7, \"end_time\": 2.0},\n    ... ]\n    &gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.2)\n    [\n        [\n            {\"start_time\": 0.0, \"end_time\": 0.4},\n            {\"start_time\": 0.4, \"end_time\": 0.5},\n            {\"start_time\": 0.6, \"end_time\": 0.8},\n        ],\n        [\n            {\"start_time\": 1.1, \"end_time\": 1.3}\n        ],\n        [\n            {\"start_time\": 1.5, \"end_time\": 1.7},\n            {\"start_time\": 1.7, \"end_time\": 2.0}\n        ],\n    ]\n    &gt;&gt;&gt; segmenter.chunk_offsets(offsets, silence_duration=0.1)\n    [\n        [\n            {\"start_time\": 0.0, \"end_time\": 0.4},\n            {\"start_time\": 0.4, \"end_time\": 0.5}\n        ],\n        [\n            {\"start_time\": 0.6, \"end_time\": 0.8}\n        ],\n        [\n            {\"start_time\": 1.1, \"end_time\": 1.3}\n        ],\n        [\n            {\"start_time\": 1.5, \"end_time\": 1.7},\n            {\"start_time\": 1.7, \"end_time\": 2.0}\n        ],\n    ]\n    ```\n\n    Args:\n        offsets (List[Dict[str, Union[str, float]]]):\n            Offsets to chunk.\n        silence_duration (float):\n            Minimum in-between silence duration (in seconds) to consider as gaps.\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            List of chunked/segmented offsets.\n    \"\"\"\n    # calculate gaps in between offsets\n    gaps = [\n        round(next[\"start_time\"] - curr[\"end_time\"], 3)\n        for curr, next in zip(offsets, offsets[1:])\n    ]\n    # generate segment slices (start and end indices) based on silence\n    slices = (\n        [0]\n        + [idx + 1 for idx, gap in enumerate(gaps) if gap &gt;= silence_duration]\n        + [len(offsets)]\n    )\n    # group consecutive offsets (segments) based on slices\n    segments = [offsets[i:j] for i, j in zip(slices, slices[1:])]\n    return segments\n</code></pre>"},{"location":"reference/segmenters/word_overlap_segmenter/","title":"Word Overlap Segmenter","text":""},{"location":"reference/segmenters/word_overlap_segmenter/#speechline.segmenters.word_overlap_segmenter.WordOverlapSegmenter","title":"<code> speechline.segmenters.word_overlap_segmenter.WordOverlapSegmenter            (Segmenter)         </code>","text":"Source code in <code>speechline/segmenters/word_overlap_segmenter.py</code> <pre><code>class WordOverlapSegmenter(Segmenter):\n    def normalize(self, text: str) -&gt; str:\n        text = text.lower().strip()\n        return text\n\n    def chunk_offsets(\n        self,\n        offsets: List[Dict[str, Union[str, float]]],\n        ground_truth: List[str],\n        **kwargs,\n    ) -&gt; List[List[Dict[str, Union[str, float]]]]:\n        \"\"\"\n        Chunk transcript offsets based on overlaps with ground truth.\n\n        ### Example\n        ```pycon title=\"example_word_overlap_segmenter.py\"\n        &gt;&gt;&gt; from speechline.segmenters import WordOverlapSegmenter\n        &gt;&gt;&gt; segmenter = WordOverlapSegmenter()\n        &gt;&gt;&gt; offsets = [\n        ...     {'end_time': 0.28, 'start_time': 0.18, 'text': 'HER'},\n        ...     {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n        ...     {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'},\n        ...     {'end_time': 1.46, 'start_time': 1.4, 'text': 'IS'},\n        ...     {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n        ...     {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n        ...     {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n        ... ]\n        &gt;&gt;&gt; ground_truth = [\"red\", \"umbrella\", \"just\", \"the\", \"best\"]\n        &gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n        [\n            [\n                {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n                {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'}\n            ],\n            [\n                {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n                {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n                {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n            ]\n        ]\n        ```\n\n        Args:\n            offsets (List[Dict[str, Union[str, float]]]):\n                Offsets to chunk.\n            ground_truth (List[str]):\n                List of ground truth words to compare with offsets.\n\n        Returns:\n            List[List[Dict[str, Union[str, float]]]]:\n                List of chunked/segmented offsets.\n        \"\"\"\n        ground_truth = [self.normalize(g) for g in ground_truth]\n        transcripts = [self.normalize(o[\"text\"]) for o in offsets]\n\n        matcher = SequenceMatcher(None, transcripts, ground_truth)\n        idxs = [(i1, i2) for tag, i1, i2, *_ in matcher.get_opcodes() if tag == \"equal\"]\n        segments = [offsets[i:j] for (i, j) in idxs]\n        return segments\n</code></pre>"},{"location":"reference/segmenters/word_overlap_segmenter/#speechline.segmenters.word_overlap_segmenter.WordOverlapSegmenter.chunk_offsets","title":"<code>chunk_offsets(self, offsets, ground_truth, **kwargs)</code>","text":"<p>Chunk transcript offsets based on overlaps with ground truth.</p>"},{"location":"reference/segmenters/word_overlap_segmenter/#speechline.segmenters.word_overlap_segmenter.WordOverlapSegmenter.chunk_offsets--example","title":"Example","text":"example_word_overlap_segmenter.py<pre><code>&gt;&gt;&gt; from speechline.segmenters import WordOverlapSegmenter\n&gt;&gt;&gt; segmenter = WordOverlapSegmenter()\n&gt;&gt;&gt; offsets = [\n...     {'end_time': 0.28, 'start_time': 0.18, 'text': 'HER'},\n...     {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n...     {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'},\n...     {'end_time': 1.46, 'start_time': 1.4, 'text': 'IS'},\n...     {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n...     {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n...     {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n... ]\n&gt;&gt;&gt; ground_truth = [\"red\", \"umbrella\", \"just\", \"the\", \"best\"]\n&gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n[\n    [\n        {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n        {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'}\n    ],\n    [\n        {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n        {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n        {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n    ]\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>Offsets to chunk.</p> required <code>ground_truth</code> <code>List[str]</code> <p>List of ground truth words to compare with offsets.</p> required <p>Returns:</p> Type Description <code>List[List[Dict[str, Union[str, float]]]]</code> <p>List of chunked/segmented offsets.</p> Source code in <code>speechline/segmenters/word_overlap_segmenter.py</code> <pre><code>def chunk_offsets(\n    self,\n    offsets: List[Dict[str, Union[str, float]]],\n    ground_truth: List[str],\n    **kwargs,\n) -&gt; List[List[Dict[str, Union[str, float]]]]:\n    \"\"\"\n    Chunk transcript offsets based on overlaps with ground truth.\n\n    ### Example\n    ```pycon title=\"example_word_overlap_segmenter.py\"\n    &gt;&gt;&gt; from speechline.segmenters import WordOverlapSegmenter\n    &gt;&gt;&gt; segmenter = WordOverlapSegmenter()\n    &gt;&gt;&gt; offsets = [\n    ...     {'end_time': 0.28, 'start_time': 0.18, 'text': 'HER'},\n    ...     {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n    ...     {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'},\n    ...     {'end_time': 1.46, 'start_time': 1.4, 'text': 'IS'},\n    ...     {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n    ...     {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n    ...     {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n    ... ]\n    &gt;&gt;&gt; ground_truth = [\"red\", \"umbrella\", \"just\", \"the\", \"best\"]\n    &gt;&gt;&gt; segmenter.chunk_offsets(offsets, ground_truth)\n    [\n        [\n            {'end_time': 0.52, 'start_time': 0.34, 'text': 'RED'},\n            {'end_time': 1.12, 'start_time': 0.68, 'text': 'UMBRELLA'}\n        ],\n        [\n            {'end_time': 1.78, 'start_time': 1.56, 'text': 'JUST'},\n            {'end_time': 1.94, 'start_time': 1.86, 'text': 'THE'},\n            {'end_time': 2.3, 'start_time': 1.98, 'text': 'BEST'}\n        ]\n    ]\n    ```\n\n    Args:\n        offsets (List[Dict[str, Union[str, float]]]):\n            Offsets to chunk.\n        ground_truth (List[str]):\n            List of ground truth words to compare with offsets.\n\n    Returns:\n        List[List[Dict[str, Union[str, float]]]]:\n            List of chunked/segmented offsets.\n    \"\"\"\n    ground_truth = [self.normalize(g) for g in ground_truth]\n    transcripts = [self.normalize(o[\"text\"]) for o in offsets]\n\n    matcher = SequenceMatcher(None, transcripts, ground_truth)\n    idxs = [(i1, i2) for tag, i1, i2, *_ in matcher.get_opcodes() if tag == \"equal\"]\n    segments = [offsets[i:j] for (i, j) in idxs]\n    return segments\n</code></pre>"},{"location":"reference/transcribers/wav2vec2/","title":"Wav2Vec2 Transcriber","text":""},{"location":"reference/transcribers/wav2vec2/#speechline.transcribers.wav2vec2.Wav2Vec2Transcriber","title":"<code> speechline.transcribers.wav2vec2.Wav2Vec2Transcriber            (AudioTranscriber)         </code>","text":"<p>Wav2Vec2-CTC model for speech recognition.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace model hub checkpoint.</p> required Source code in <code>speechline/transcribers/wav2vec2.py</code> <pre><code>class Wav2Vec2Transcriber(AudioTranscriber):\n    \"\"\"\n    Wav2Vec2-CTC model for speech recognition.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace model hub checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str) -&gt; None:\n        super().__init__(model_checkpoint)\n\n    def predict(\n        self,\n        dataset: Dataset,\n        chunk_length_s: int = 30,\n        output_offsets: bool = False,\n        return_timestamps: str = \"word\",\n        keep_whitespace: bool = False,\n    ) -&gt; Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n        \"\"\"\n        Performs inference on `dataset`.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n            chunk_length_s (int):\n                Audio chunk length during inference. Defaults to `30`.\n            output_offsets (bool, optional):\n                Whether to output timestamps. Defaults to `False`.\n            return_timestamps (str, optional):\n                Returned timestamp level. Defaults to `\"word\"`.\n            keep_whitespace (bool, optional):\n                Whether to presere whitespace predictions. Defaults to `False`.\n\n        Returns:\n            Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n                Defaults to list of transcriptions.\n                If `output_offsets` is `True`, return list of offsets.\n\n        ### Example\n        ```pycon title=\"example_transcriber_predict.py\"\n        &gt;&gt;&gt; from speechline.transcribers import Wav2Vec2Transcriber\n        &gt;&gt;&gt; from datasets import Dataset, Audio\n        &gt;&gt;&gt; transcriber = Wav2Vec2Transcriber(\"bookbot/wav2vec2-ljspeech-gruut\")\n        &gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n        ...     \"audio\", Audio(sampling_rate=transcriber.sr)\n        ... )\n        &gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n        &gt;&gt;&gt; transcripts\n        [\"\u026a t \u026a z n o\u028a t \u028c p\"]\n        &gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n        &gt;&gt;&gt; offsets\n        [\n            [\n                {\"text\": \"\u026a\", \"start_time\": 0.0, \"end_time\": 0.02},\n                {\"text\": \"t\", \"start_time\": 0.26, \"end_time\": 0.3},\n                {\"text\": \"\u026a\", \"start_time\": 0.34, \"end_time\": 0.36},\n                {\"text\": \"z\", \"start_time\": 0.42, \"end_time\": 0.44},\n                {\"text\": \"n\", \"start_time\": 0.5, \"end_time\": 0.54},\n                {\"text\": \"o\u028a\", \"start_time\": 0.54, \"end_time\": 0.58},\n                {\"text\": \"t\", \"start_time\": 0.58, \"end_time\": 0.62},\n                {\"text\": \"\u028c\", \"start_time\": 0.76, \"end_time\": 0.78},\n                {\"text\": \"p\", \"start_time\": 0.92, \"end_time\": 0.94},\n            ]\n        ]\n        ```\n        \"\"\"\n        return self.inference(\n            dataset,\n            chunk_length_s=chunk_length_s,\n            output_offsets=output_offsets,\n            offset_key=\"text\",\n            return_timestamps=return_timestamps,\n            keep_whitespace=keep_whitespace,\n        )\n</code></pre>"},{"location":"reference/transcribers/wav2vec2/#speechline.transcribers.wav2vec2.Wav2Vec2Transcriber.predict","title":"<code>predict(self, dataset, chunk_length_s=30, output_offsets=False, return_timestamps='word', keep_whitespace=False)</code>","text":"<p>Performs inference on <code>dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <code>chunk_length_s</code> <code>int</code> <p>Audio chunk length during inference. Defaults to <code>30</code>.</p> <code>30</code> <code>output_offsets</code> <code>bool</code> <p>Whether to output timestamps. Defaults to <code>False</code>.</p> <code>False</code> <code>return_timestamps</code> <code>str</code> <p>Returned timestamp level. Defaults to <code>\"word\"</code>.</p> <code>'word'</code> <code>keep_whitespace</code> <code>bool</code> <p>Whether to presere whitespace predictions. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[Dict[str, Union[str, float]]]]]</code> <p>Defaults to list of transcriptions.     If <code>output_offsets</code> is <code>True</code>, return list of offsets.</p>"},{"location":"reference/transcribers/wav2vec2/#speechline.transcribers.wav2vec2.Wav2Vec2Transcriber.predict--example","title":"Example","text":"example_transcriber_predict.py<pre><code>&gt;&gt;&gt; from speechline.transcribers import Wav2Vec2Transcriber\n&gt;&gt;&gt; from datasets import Dataset, Audio\n&gt;&gt;&gt; transcriber = Wav2Vec2Transcriber(\"bookbot/wav2vec2-ljspeech-gruut\")\n&gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n...     \"audio\", Audio(sampling_rate=transcriber.sr)\n... )\n&gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n&gt;&gt;&gt; transcripts\n[\"\u026a t \u026a z n o\u028a t \u028c p\"]\n&gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n&gt;&gt;&gt; offsets\n[\n    [\n        {\"text\": \"\u026a\", \"start_time\": 0.0, \"end_time\": 0.02},\n        {\"text\": \"t\", \"start_time\": 0.26, \"end_time\": 0.3},\n        {\"text\": \"\u026a\", \"start_time\": 0.34, \"end_time\": 0.36},\n        {\"text\": \"z\", \"start_time\": 0.42, \"end_time\": 0.44},\n        {\"text\": \"n\", \"start_time\": 0.5, \"end_time\": 0.54},\n        {\"text\": \"o\u028a\", \"start_time\": 0.54, \"end_time\": 0.58},\n        {\"text\": \"t\", \"start_time\": 0.58, \"end_time\": 0.62},\n        {\"text\": \"\u028c\", \"start_time\": 0.76, \"end_time\": 0.78},\n        {\"text\": \"p\", \"start_time\": 0.92, \"end_time\": 0.94},\n    ]\n]\n</code></pre> Source code in <code>speechline/transcribers/wav2vec2.py</code> <pre><code>def predict(\n    self,\n    dataset: Dataset,\n    chunk_length_s: int = 30,\n    output_offsets: bool = False,\n    return_timestamps: str = \"word\",\n    keep_whitespace: bool = False,\n) -&gt; Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n    \"\"\"\n    Performs inference on `dataset`.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n        chunk_length_s (int):\n            Audio chunk length during inference. Defaults to `30`.\n        output_offsets (bool, optional):\n            Whether to output timestamps. Defaults to `False`.\n        return_timestamps (str, optional):\n            Returned timestamp level. Defaults to `\"word\"`.\n        keep_whitespace (bool, optional):\n            Whether to presere whitespace predictions. Defaults to `False`.\n\n    Returns:\n        Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n            Defaults to list of transcriptions.\n            If `output_offsets` is `True`, return list of offsets.\n\n    ### Example\n    ```pycon title=\"example_transcriber_predict.py\"\n    &gt;&gt;&gt; from speechline.transcribers import Wav2Vec2Transcriber\n    &gt;&gt;&gt; from datasets import Dataset, Audio\n    &gt;&gt;&gt; transcriber = Wav2Vec2Transcriber(\"bookbot/wav2vec2-ljspeech-gruut\")\n    &gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n    ...     \"audio\", Audio(sampling_rate=transcriber.sr)\n    ... )\n    &gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n    &gt;&gt;&gt; transcripts\n    [\"\u026a t \u026a z n o\u028a t \u028c p\"]\n    &gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n    &gt;&gt;&gt; offsets\n    [\n        [\n            {\"text\": \"\u026a\", \"start_time\": 0.0, \"end_time\": 0.02},\n            {\"text\": \"t\", \"start_time\": 0.26, \"end_time\": 0.3},\n            {\"text\": \"\u026a\", \"start_time\": 0.34, \"end_time\": 0.36},\n            {\"text\": \"z\", \"start_time\": 0.42, \"end_time\": 0.44},\n            {\"text\": \"n\", \"start_time\": 0.5, \"end_time\": 0.54},\n            {\"text\": \"o\u028a\", \"start_time\": 0.54, \"end_time\": 0.58},\n            {\"text\": \"t\", \"start_time\": 0.58, \"end_time\": 0.62},\n            {\"text\": \"\u028c\", \"start_time\": 0.76, \"end_time\": 0.78},\n            {\"text\": \"p\", \"start_time\": 0.92, \"end_time\": 0.94},\n        ]\n    ]\n    ```\n    \"\"\"\n    return self.inference(\n        dataset,\n        chunk_length_s=chunk_length_s,\n        output_offsets=output_offsets,\n        offset_key=\"text\",\n        return_timestamps=return_timestamps,\n        keep_whitespace=keep_whitespace,\n    )\n</code></pre>"},{"location":"reference/transcribers/whisper/","title":"Whisper Transcriber","text":""},{"location":"reference/transcribers/whisper/#speechline.transcribers.whisper.WhisperTranscriber","title":"<code> speechline.transcribers.whisper.WhisperTranscriber            (AudioTranscriber)         </code>","text":"<p>Whisper model for seq2seq speech recognition with its processor.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>str</code> <p>HuggingFace model hub checkpoint.</p> required Source code in <code>speechline/transcribers/whisper.py</code> <pre><code>class WhisperTranscriber(AudioTranscriber):\n    \"\"\"\n    Whisper model for seq2seq speech recognition with its processor.\n\n    Args:\n        model_checkpoint (str):\n            HuggingFace model hub checkpoint.\n    \"\"\"\n\n    def __init__(self, model_checkpoint: str) -&gt; None:\n        super().__init__(model_checkpoint)\n\n    def predict(\n        self,\n        dataset: Dataset,\n        chunk_length_s: int = 0,\n        output_offsets: bool = False,\n        return_timestamps: bool = True,\n        keep_whitespace: bool = False,\n    ) -&gt; Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n        \"\"\"\n        Performs inference on `dataset`.\n\n        Args:\n            dataset (Dataset):\n                Dataset to be inferred.\n            chunk_length_s (int):\n                Audio chunk length during inference. Defaults to `0`.\n            output_offsets (bool, optional):\n                Whether to output timestamps. Defaults to `False`.\n            return_timestamps (bool, optional):\n                Returned timestamp level. Defaults to `True`.\n            keep_whitespace (bool, optional):\n                Whether to presere whitespace predictions. Defaults to `False`.\n\n        Returns:\n            Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n                Defaults to list of transcriptions.\n                If `output_offsets` is `True`, return list of text offsets.\n\n        ### Example\n        ```pycon title=\"example_transcriber_predict.py\"\n        &gt;&gt;&gt; from speechline.transcribers import WhisperTranscriber\n        &gt;&gt;&gt; from datasets import Dataset, Audio\n        &gt;&gt;&gt; transcriber = WhisperTranscriber(\"openai/whisper-tiny\")\n        &gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n        ...     \"audio\", Audio(sampling_rate=transcriber.sr)\n        ... )\n        &gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n        &gt;&gt;&gt; transcripts\n        [\"Her red umbrella is just the best.\"]\n        &gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n        &gt;&gt;&gt; offsets\n        [\n            [\n                {\n                    \"text\": \"Her red umbrella is just the best.\",\n                    \"start_time\": 0.0,\n                    \"end_time\": 3.0,\n                }\n            ]\n        ]\n        ```\n        \"\"\"\n        return self.inference(\n            dataset,\n            chunk_length_s=chunk_length_s,\n            output_offsets=output_offsets,\n            offset_key=\"text\",\n            return_timestamps=return_timestamps,\n            keep_whitespace=keep_whitespace,\n            generate_kwargs={\"max_new_tokens\": 448},\n        )\n</code></pre>"},{"location":"reference/transcribers/whisper/#speechline.transcribers.whisper.WhisperTranscriber.predict","title":"<code>predict(self, dataset, chunk_length_s=0, output_offsets=False, return_timestamps=True, keep_whitespace=False)</code>","text":"<p>Performs inference on <code>dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Dataset to be inferred.</p> required <code>chunk_length_s</code> <code>int</code> <p>Audio chunk length during inference. Defaults to <code>0</code>.</p> <code>0</code> <code>output_offsets</code> <code>bool</code> <p>Whether to output timestamps. Defaults to <code>False</code>.</p> <code>False</code> <code>return_timestamps</code> <code>bool</code> <p>Returned timestamp level. Defaults to <code>True</code>.</p> <code>True</code> <code>keep_whitespace</code> <code>bool</code> <p>Whether to presere whitespace predictions. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[Dict[str, Union[str, float]]]]]</code> <p>Defaults to list of transcriptions.     If <code>output_offsets</code> is <code>True</code>, return list of text offsets.</p>"},{"location":"reference/transcribers/whisper/#speechline.transcribers.whisper.WhisperTranscriber.predict--example","title":"Example","text":"example_transcriber_predict.py<pre><code>&gt;&gt;&gt; from speechline.transcribers import WhisperTranscriber\n&gt;&gt;&gt; from datasets import Dataset, Audio\n&gt;&gt;&gt; transcriber = WhisperTranscriber(\"openai/whisper-tiny\")\n&gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n...     \"audio\", Audio(sampling_rate=transcriber.sr)\n... )\n&gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n&gt;&gt;&gt; transcripts\n[\"Her red umbrella is just the best.\"]\n&gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n&gt;&gt;&gt; offsets\n[\n    [\n        {\n            \"text\": \"Her red umbrella is just the best.\",\n            \"start_time\": 0.0,\n            \"end_time\": 3.0,\n        }\n    ]\n]\n</code></pre> Source code in <code>speechline/transcribers/whisper.py</code> <pre><code>def predict(\n    self,\n    dataset: Dataset,\n    chunk_length_s: int = 0,\n    output_offsets: bool = False,\n    return_timestamps: bool = True,\n    keep_whitespace: bool = False,\n) -&gt; Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n    \"\"\"\n    Performs inference on `dataset`.\n\n    Args:\n        dataset (Dataset):\n            Dataset to be inferred.\n        chunk_length_s (int):\n            Audio chunk length during inference. Defaults to `0`.\n        output_offsets (bool, optional):\n            Whether to output timestamps. Defaults to `False`.\n        return_timestamps (bool, optional):\n            Returned timestamp level. Defaults to `True`.\n        keep_whitespace (bool, optional):\n            Whether to presere whitespace predictions. Defaults to `False`.\n\n    Returns:\n        Union[List[str], List[List[Dict[str, Union[str, float]]]]]:\n            Defaults to list of transcriptions.\n            If `output_offsets` is `True`, return list of text offsets.\n\n    ### Example\n    ```pycon title=\"example_transcriber_predict.py\"\n    &gt;&gt;&gt; from speechline.transcribers import WhisperTranscriber\n    &gt;&gt;&gt; from datasets import Dataset, Audio\n    &gt;&gt;&gt; transcriber = WhisperTranscriber(\"openai/whisper-tiny\")\n    &gt;&gt;&gt; dataset = Dataset.from_dict({\"audio\": [\"sample.wav\"]}).cast_column(\n    ...     \"audio\", Audio(sampling_rate=transcriber.sr)\n    ... )\n    &gt;&gt;&gt; transcripts = transcriber.predict(dataset)\n    &gt;&gt;&gt; transcripts\n    [\"Her red umbrella is just the best.\"]\n    &gt;&gt;&gt; offsets = transcriber.predict(dataset, output_offsets=True)\n    &gt;&gt;&gt; offsets\n    [\n        [\n            {\n                \"text\": \"Her red umbrella is just the best.\",\n                \"start_time\": 0.0,\n                \"end_time\": 3.0,\n            }\n        ]\n    ]\n    ```\n    \"\"\"\n    return self.inference(\n        dataset,\n        chunk_length_s=chunk_length_s,\n        output_offsets=output_offsets,\n        offset_key=\"text\",\n        return_timestamps=return_timestamps,\n        keep_whitespace=keep_whitespace,\n        generate_kwargs={\"max_new_tokens\": 448},\n    )\n</code></pre>"},{"location":"reference/utils/airtable/","title":"AirTable Interface","text":""},{"location":"reference/utils/airtable/#speechline.utils.airtable.AirTable","title":"<code> speechline.utils.airtable.AirTable        </code>","text":"<p>AirTable table interface.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of AirTable table.</p> required <p>Exceptions:</p> Type Description <code>OSError</code> <p><code>AIRTABLE_API_KEY</code> environment is not set.</p> Source code in <code>speechline/utils/airtable.py</code> <pre><code>class AirTable:\n    \"\"\"\n    AirTable table interface.\n\n    Args:\n        url (str):\n            URL of AirTable table.\n\n    Raises:\n        OSError: `AIRTABLE_API_KEY` environment is not set.\n    \"\"\"\n\n    def __init__(self, url: str) -&gt; None:\n        airtable_api_key = os.getenv(\"AIRTABLE_API_KEY\")\n        if airtable_api_key is None:\n            raise OSError(\"AIRTABLE_API_KEY environment is not set.\")\n\n        self.url = url\n        self.headers = {\n            \"Authorization\": f\"Bearer {airtable_api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def add_records(self, records: List[Dict[str, Any]]) -&gt; bool:\n        \"\"\"\n        Add records to AirTable table.\n\n        Args:\n            records (List[Dict[str, Any]]):\n                List of records in AirTable format.\n\n        Returns:\n            bool:\n                Whether upload was a success.\n        \"\"\"\n        try:\n            response = requests.post(\n                self.url, headers=self.headers, data=json.dumps({\"records\": records})\n            )\n        except Exception:\n            return False\n\n        return True if response.ok else False\n\n    def batch_add_records(self, records: List[Dict[str, Any]]) -&gt; bool:\n        \"\"\"\n        Allow batching of record addition due to 10-element limit, then push.\n\n        Args:\n            records (List[Dict[str, Any]]):\n                List of records in AirTable format.\n\n        Returns:\n            bool:\n                Whether upload was a success.\n        \"\"\"\n        BATCH_SIZE = 10\n        for idx in range(0, len(records), BATCH_SIZE):\n            batch = records[idx : idx + BATCH_SIZE]\n            success = self.add_records(batch)\n            if not success:\n                return success\n        return True\n</code></pre>"},{"location":"reference/utils/airtable/#speechline.utils.airtable.AirTable.add_records","title":"<code>add_records(self, records)</code>","text":"<p>Add records to AirTable table.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[Dict[str, Any]]</code> <p>List of records in AirTable format.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether upload was a success.</p> Source code in <code>speechline/utils/airtable.py</code> <pre><code>def add_records(self, records: List[Dict[str, Any]]) -&gt; bool:\n    \"\"\"\n    Add records to AirTable table.\n\n    Args:\n        records (List[Dict[str, Any]]):\n            List of records in AirTable format.\n\n    Returns:\n        bool:\n            Whether upload was a success.\n    \"\"\"\n    try:\n        response = requests.post(\n            self.url, headers=self.headers, data=json.dumps({\"records\": records})\n        )\n    except Exception:\n        return False\n\n    return True if response.ok else False\n</code></pre>"},{"location":"reference/utils/airtable/#speechline.utils.airtable.AirTable.batch_add_records","title":"<code>batch_add_records(self, records)</code>","text":"<p>Allow batching of record addition due to 10-element limit, then push.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>List[Dict[str, Any]]</code> <p>List of records in AirTable format.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether upload was a success.</p> Source code in <code>speechline/utils/airtable.py</code> <pre><code>def batch_add_records(self, records: List[Dict[str, Any]]) -&gt; bool:\n    \"\"\"\n    Allow batching of record addition due to 10-element limit, then push.\n\n    Args:\n        records (List[Dict[str, Any]]):\n            List of records in AirTable format.\n\n    Returns:\n        bool:\n            Whether upload was a success.\n    \"\"\"\n    BATCH_SIZE = 10\n    for idx in range(0, len(records), BATCH_SIZE):\n        batch = records[idx : idx + BATCH_SIZE]\n        success = self.add_records(batch)\n        if not success:\n            return success\n    return True\n</code></pre>"},{"location":"reference/utils/dataset/","title":"Audio Dataset","text":""},{"location":"reference/utils/dataset/#speechline.utils.dataset","title":"<code>speechline.utils.dataset</code>","text":""},{"location":"reference/utils/dataset/#speechline.utils.dataset.format_audio_dataset","title":"<code>format_audio_dataset(df, sampling_rate=16000)</code>","text":"<p>Formats Pandas <code>DataFrame</code> as a datasets <code>Dataset</code>. Converts <code>audio</code> path column to audio arrays and resamples accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Pandas DataFrame to convert to <code>Dataset</code>.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p><code>datasets</code>' <code>Dataset</code> object usable for batch inference.</p> Source code in <code>speechline/utils/dataset.py</code> <pre><code>def format_audio_dataset(df: pd.DataFrame, sampling_rate: int = 16000) -&gt; Dataset:\n    \"\"\"\n    Formats Pandas `DataFrame` as a datasets `Dataset`.\n    Converts `audio` path column to audio arrays and resamples accordingly.\n\n    Args:\n        df (pd.DataFrame):\n            Pandas DataFrame to convert to `Dataset`.\n\n    Returns:\n        Dataset:\n            `datasets`' `Dataset` object usable for batch inference.\n    \"\"\"\n    dataset = Dataset.from_pandas(df)\n    dataset.save_to_disk(str(config.HF_DATASETS_CACHE))\n    saved_dataset = load_from_disk(str(config.HF_DATASETS_CACHE))\n    saved_dataset = saved_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n    return saved_dataset\n</code></pre>"},{"location":"reference/utils/dataset/#speechline.utils.dataset.prepare_dataframe","title":"<code>prepare_dataframe(path_to_files, audio_extension='wav')</code>","text":"<p>Prepares audio and ground truth files as Pandas <code>DataFrame</code>. Assumes files are of the following structure:</p> <pre><code>path_to_files\n\u251c\u2500\u2500 langX\n\u2502   \u251c\u2500\u2500 a.{audio_extension}\n\u2502   \u251c\u2500\u2500 a.txt\n\u2502   \u2514\u2500\u2500 b.{audio_extension}\n\u2502   \u2514\u2500\u2500 b.txt\n\u2514\u2500\u2500 langY\n    \u2514\u2500\u2500 c.{audio_extension}\n    \u2514\u2500\u2500 c.txt\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path_to_files</code> <code>str</code> <p>Path to files.</p> required <code>audio_extension</code> <code>str</code> <p>Audio extension of files to include. Defaults to \"wav\".</p> <code>'wav'</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>No audio files found.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>DataFrame consisting of:</p> <ul> <li><code>audio</code> (audio path)</li> <li><code>id</code></li> <li><code>language</code></li> <li><code>language_code</code></li> <li><code>ground_truth</code></li> </ul> Source code in <code>speechline/utils/dataset.py</code> <pre><code>def prepare_dataframe(path_to_files: str, audio_extension: str = \"wav\") -&gt; pd.DataFrame:\n    \"\"\"\n    Prepares audio and ground truth files as Pandas `DataFrame`.\n    Assumes files are of the following structure:\n\n    ```\n    path_to_files\n    \u251c\u2500\u2500 langX\n    \u2502   \u251c\u2500\u2500 a.{audio_extension}\n    \u2502   \u251c\u2500\u2500 a.txt\n    \u2502   \u2514\u2500\u2500 b.{audio_extension}\n    \u2502   \u2514\u2500\u2500 b.txt\n    \u2514\u2500\u2500 langY\n        \u2514\u2500\u2500 c.{audio_extension}\n        \u2514\u2500\u2500 c.txt\n    ```\n\n    Args:\n        path_to_files (str):\n            Path to files.\n        audio_extension (str, optional):\n            Audio extension of files to include. Defaults to \"wav\".\n\n    Raises:\n        ValueError: No audio files found.\n\n    Returns:\n        pd.DataFrame:\n            DataFrame consisting of:\n\n        - `audio` (audio path)\n        - `id`\n        - `language`\n        - `language_code`\n        - `ground_truth`\n    \"\"\"\n    audios = sorted(glob(f\"{path_to_files}/*/*.{audio_extension}\"))\n    audios = [a for a in audios if Path(a).stat().st_size &gt; 0]\n    if len(audios) == 0:\n        raise ValueError(\"No audio files found!\")\n\n    df = pd.DataFrame({\"audio\": audios})\n    # ID is filename stem (before extension)\n    df[\"id\"] = df[\"audio\"].apply(lambda f: Path(f).stem)\n    # language code is immediate parent directory\n    df[\"language_code\"] = df[\"audio\"].apply(lambda f: Path(f).parent.name)\n    df[\"language\"] = df[\"language_code\"].apply(lambda f: f.split(\"-\")[0])\n    # ground truth is same filename, except with .txt extension\n    df[\"ground_truth\"] = df[\"audio\"].apply(lambda p: Path(p).with_suffix(\".txt\"))\n    df[\"ground_truth\"] = df[\"ground_truth\"].apply(lambda p: open(p).read() if p.exists() else \"\")\n    return df\n</code></pre>"},{"location":"reference/utils/dataset/#speechline.utils.dataset.preprocess_audio_transcript","title":"<code>preprocess_audio_transcript(text)</code>","text":"<p>Preprocesses audio transcript. - Removes punctuation. - Converts to lowercase. - Removes special tags (e.g. GigaSpeech).</p> Source code in <code>speechline/utils/dataset.py</code> <pre><code>def preprocess_audio_transcript(text: str) -&gt; str:\n    \"\"\"\n    Preprocesses audio transcript.\n    - Removes punctuation.\n    - Converts to lowercase.\n    - Removes special tags (e.g. GigaSpeech).\n    \"\"\"\n    tags = [\n        \"&lt;COMMA&gt;\",\n        \"&lt;PERIOD&gt;\",\n        \"&lt;QUESTIONMARK&gt;\",\n        \"&lt;EXCLAMATIONPOINT&gt;\",\n        \"&lt;SIL&gt;\",\n        \"&lt;MUSIC&gt;\",\n        \"&lt;NOISE&gt;\",\n        \"&lt;OTHER&gt;\",\n    ]\n    chars_to_remove_regex = \"[\\,\\?\\.\\!\\-\\;\\:\\\"\\\u201c\\%\\\u2018\\\u201d\\\ufffd'\\\u2019]\"\n    text = re.sub(chars_to_remove_regex, \" \", text).lower().strip()\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    for tag in tags:\n        text = text.replace(tag.lower(), \"\").strip()\n    return text\n</code></pre>"},{"location":"reference/utils/g2p/","title":"Grapheme-to-Phoneme Converter","text":""},{"location":"reference/utils/g2p/#speechline.utils.g2p","title":"<code>speechline.utils.g2p</code>","text":""},{"location":"reference/utils/g2p/#speechline.utils.g2p.g2p_en","title":"<code>g2p_en(text)</code>","text":"<p>Convert English text to string of phonemes via gruut.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>English text to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Phoneme string.</p> Source code in <code>speechline/utils/g2p.py</code> <pre><code>def g2p_en(text: str) -&gt; str:\n    \"\"\"\n    Convert English text to string of phonemes via gruut.\n\n    Args:\n        text (str):\n            English text to convert.\n\n    Returns:\n        str:\n            Phoneme string.\n    \"\"\"\n    phonemes = []\n    for words in sentences(text):\n        for word in words:\n            if word.is_major_break or word.is_minor_break:\n                phonemes += word.text\n            elif word.phonemes:\n                phonemes += word.phonemes\n    return \" \".join(phonemes)\n</code></pre>"},{"location":"reference/utils/g2p/#speechline.utils.g2p.g2p_id","title":"<code>g2p_id(text)</code>","text":"<p>Convert Indonesian text to string of phonemes via g2p_id.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Indonesian text to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Phoneme string.</p> Source code in <code>speechline/utils/g2p.py</code> <pre><code>def g2p_id(text: str) -&gt; str:\n    \"\"\"\n    Convert Indonesian text to string of phonemes via g2p_id.\n\n    Args:\n        text (str):\n            Indonesian text to convert.\n\n    Returns:\n        str:\n            Phoneme string.\n    \"\"\"\n    g2p = G2p()\n    phonemes = g2p(text)\n    return \" \".join(p for phoneme in phonemes for p in phoneme)\n</code></pre>"},{"location":"reference/utils/g2p/#speechline.utils.g2p.get_g2p","title":"<code>get_g2p(language)</code>","text":"<p>Gets the corresponding g2p function given <code>language</code>.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language code. Can be in the form of <code>en-US</code> or simply <code>en</code>.</p> required <p>Exceptions:</p> Type Description <code>NotImplementedError</code> <p>Language has no g2p function implemented yet.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>G2p callable function.</p> Source code in <code>speechline/utils/g2p.py</code> <pre><code>def get_g2p(language: str) -&gt; Callable:\n    \"\"\"\n    Gets the corresponding g2p function given `language`.\n\n    Args:\n        language (str):\n            Language code. Can be in the form of `en-US` or simply `en`.\n\n    Raises:\n        NotImplementedError: Language has no g2p function implemented yet.\n\n    Returns:\n        Callable:\n            G2p callable function.\n    \"\"\"\n\n    LANG2G2P = {\n        \"en\": g2p_en,\n        \"id\": g2p_id,\n    }\n\n    if language.lower() not in LANG2G2P:\n        raise NotImplementedError(f\"{language} has no g2p function yet!\")\n    return LANG2G2P[language.lower()]\n</code></pre>"},{"location":"reference/utils/io/","title":"I/O","text":""},{"location":"reference/utils/io/#speechline.utils.io","title":"<code>speechline.utils.io</code>","text":""},{"location":"reference/utils/io/#speechline.utils.io.export_segment_audio_wav","title":"<code>export_segment_audio_wav(output_wav_path, segment)</code>","text":"<p>Export segment audio to WAV.</p> <p>Equivalent to:</p> example_export_segment_audio_wav.sh<pre><code>ffmpeg -i {segment} -acodec pcm_s16le -ac 1 -ar 16000 {output_wav_path}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_wav_path</code> <code>str</code> <p>Path to WAV file.</p> required <code>segment</code> <code>AudioSegment</code> <p>Audio segment to export.</p> required Source code in <code>speechline/utils/io.py</code> <pre><code>def export_segment_audio_wav(output_wav_path: str, segment: AudioSegment) -&gt; None:\n    \"\"\"\n    Export segment audio to WAV.\n\n    Equivalent to:\n\n    ```sh title=\"example_export_segment_audio_wav.sh\"\n    ffmpeg -i {segment} -acodec pcm_s16le -ac 1 -ar 16000 {output_wav_path}\n    ```\n\n    Args:\n        output_wav_path (str):\n            Path to WAV file.\n        segment (AudioSegment):\n            Audio segment to export.\n    \"\"\"\n    parameters = [\"-acodec\", \"pcm_s16le\", \"-ac\", \"1\", \"-ar\", \"16000\"]\n    segment.export(output_wav_path, format=\"wav\", parameters=parameters)\n</code></pre>"},{"location":"reference/utils/io/#speechline.utils.io.export_segment_transcripts_tsv","title":"<code>export_segment_transcripts_tsv(output_tsv_path, segment)</code>","text":"<p>Export segment transcripts to TSV of structure:</p> example_output_segment_transcripts.tsv<pre><code>start_time_in_secs  end_time_in_secs        label\nstart_time_in_secs  end_time_in_secs        label\n...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_tsv_path</code> <code>str</code> <p>Path to TSV file.</p> required <code>segment</code> <code>List[Dict[str, Union[str, float]]]</code> <p>List of offsets in segment.</p> required Source code in <code>speechline/utils/io.py</code> <pre><code>def export_segment_transcripts_tsv(output_tsv_path: str, segment: List[Dict[str, Union[str, float]]]) -&gt; None:\n    \"\"\"\n    Export segment transcripts to TSV of structure:\n\n    ```tsv title=\"example_output_segment_transcripts.tsv\"\n    start_time_in_secs\\tend_time_in_secs\\tlabel\n    start_time_in_secs\\tend_time_in_secs\\tlabel\n    ...\n    ```\n\n    Args:\n        output_tsv_path (str):\n            Path to TSV file.\n        segment (List[Dict[str, Union[str, float]]]):\n            List of offsets in segment.\n    \"\"\"\n    with open(output_tsv_path, \"w\") as f:\n        for s in segment:\n            f.write(f'{s[\"start_time\"]}\\t{s[\"end_time\"]}\\t{s[\"text\"]}\\n')\n</code></pre>"},{"location":"reference/utils/io/#speechline.utils.io.export_transcripts_json","title":"<code>export_transcripts_json(output_json_path, offsets)</code>","text":"<p>Exports transcript with offsets as JSON.</p> example_output_transcripts.json<pre><code>[\n  {\n    \"text\": {text},\n    \"start_time\": {start_time},\n    \"end_time\": {end_time}\n  },\n  {\n    \"text\": {text},\n    \"start_time\": {start_time},\n    \"end_time\": {end_time}\n  },\n  ...\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_json_path</code> <code>str</code> <p>Path to output JSON file.</p> required <code>offsets</code> <code>List[Dict[str, Union[str, float]]]</code> <p>List of offsets.</p> required Source code in <code>speechline/utils/io.py</code> <pre><code>def export_transcripts_json(\n    output_json_path: str,\n    offsets: List[Dict[str, Union[str, float]]],\n) -&gt; None:\n    \"\"\"\n    Exports transcript with offsets as JSON.\n\n    ```json title=\"example_output_transcripts.json\"\n    [\n      {\n        \"text\": {text},\n        \"start_time\": {start_time},\n        \"end_time\": {end_time}\n      },\n      {\n        \"text\": {text},\n        \"start_time\": {start_time},\n        \"end_time\": {end_time}\n      },\n      ...\n    ]\n    ```\n\n    Args:\n        output_json_path (str):\n            Path to output JSON file.\n        offsets (List[Dict[str, Union[str, float]]]):\n            List of offsets.\n    \"\"\"\n    _ = Path(output_json_path).parent.mkdir(parents=True, exist_ok=True)\n    with open(output_json_path, \"w\") as f:\n        json.dump(offsets, f, indent=2)\n</code></pre>"},{"location":"reference/utils/io/#speechline.utils.io.get_chunk_path","title":"<code>get_chunk_path(path, outdir, idx, extension)</code>","text":"<p>Generate path to chunk at output directory.</p> <p>Assumes <code>path</code> as <code>{inputdir}/{lang}/{utt_id}.{old_extension}</code>, and will return <code>{outdir}/{lang}/{utt_id}-{idx}.{extension}</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to file.</p> required <code>outdir</code> <code>str</code> <p>Output directory where file will be saved.</p> required <code>idx</code> <code>int</code> <p>Index of chunk.</p> required <code>extension</code> <code>str</code> <p>New file extension.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path to chunk at output directory.</p> Source code in <code>speechline/utils/io.py</code> <pre><code>def get_chunk_path(path: str, outdir: str, idx: int, extension: str) -&gt; str:\n    \"\"\"\n    Generate path to chunk at output directory.\n\n    Assumes `path` as `{inputdir}/{lang}/{utt_id}.{old_extension}`,\n    and will return `{outdir}/{lang}/{utt_id}-{idx}.{extension}`\n\n    Args:\n        path (str):\n            Path to file.\n        outdir (str):\n            Output directory where file will be saved.\n        idx (int):\n            Index of chunk.\n        extension (str):\n            New file extension.\n\n    Returns:\n        str:\n            Path to chunk at output directory.\n    \"\"\"\n    outdir_path = get_outdir_path(path, outdir)\n    filename = os.path.splitext(os.path.basename(path))[0]\n    output_path = f\"{os.path.join(outdir_path, filename)}-{str(idx)}.{extension}\"\n    return output_path\n</code></pre>"},{"location":"reference/utils/io/#speechline.utils.io.get_outdir_path","title":"<code>get_outdir_path(path, outdir)</code>","text":"<p>Generate path at output directory.</p> <p>Assumes <code>path</code> as <code>{inputdir}/{lang}/*.wav</code>, and will return <code>{outdir}/{lang}/*.wav</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to file.</p> required <code>outdir</code> <code>str</code> <p>Output directory where file will be saved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Path to output directory.</p> Source code in <code>speechline/utils/io.py</code> <pre><code>def get_outdir_path(path: str, outdir: str) -&gt; str:\n    \"\"\"\n    Generate path at output directory.\n\n    Assumes `path` as `{inputdir}/{lang}/*.wav`,\n    and will return `{outdir}/{lang}/*.wav`\n\n    Args:\n        path (str):\n            Path to file.\n        outdir (str):\n            Output directory where file will be saved.\n\n    Returns:\n        str:\n            Path to output directory.\n    \"\"\"\n    pathname, _ = os.path.splitext(path)  # remove extension\n    components = os.path.normpath(pathname).split(os.sep)  # split into components\n    # keep last 2 components: {outdir}/{lang}/\n    output_path = f\"{os.path.join(outdir, *components[-2:-1])}\"\n    return output_path\n</code></pre>"},{"location":"reference/utils/io/#speechline.utils.io.pydub_to_np","title":"<code>pydub_to_np(audio)</code>","text":"<p>Converts pydub AudioSegment into <code>np.float32</code> of shape <code>[duration_in_seconds * sample_rate, channels]</code>, where each value is in range <code>[-1.0, 1.0]</code>. Source: StackOverflow. # noqa: E501</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>AudioSegment</code> <p>AudioSegment to convert.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Resultant NumPy array of AudioSegment.</p> Source code in <code>speechline/utils/io.py</code> <pre><code>def pydub_to_np(audio: AudioSegment) -&gt; np.ndarray:\n    \"\"\"\n    Converts pydub AudioSegment into `np.float32` of shape\n    `[duration_in_seconds * sample_rate, channels]`,\n    where each value is in range `[-1.0, 1.0]`.\n    Source: [StackOverflow](https://stackoverflow.com/questions/38015319/how-to-create-a-numpy-array-from-a-pydub-audiosegment/66922265#66922265). # noqa: E501\n\n    Args:\n        audio (AudioSegment):\n            AudioSegment to convert.\n\n    Returns:\n        np.ndarray:\n            Resultant NumPy array of AudioSegment.\n    \"\"\"\n    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((-1, audio.channels)) / (\n        1 &lt;&lt; (8 * audio.sample_width - 1)\n    )\n</code></pre>"},{"location":"reference/utils/s3/","title":"S3","text":""},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client","title":"<code> speechline.utils.s3.S3Client        </code>","text":"<p>AWS S3 Client Interface.</p> <p>Parameters:</p> Name Type Description Default <code>region_name</code> <code>str</code> <p>AWS region name. Defaults to <code>\"us-east-1\"</code>.</p> <code>'us-east-1'</code> Source code in <code>speechline/utils/s3.py</code> <pre><code>class S3Client:\n    \"\"\"\n    AWS S3 Client Interface.\n\n    Args:\n        region_name (str, optional):\n            AWS region name. Defaults to `\"us-east-1\"`.\n    \"\"\"\n\n    def __init__(self, region_name: str = \"us-east-1\") -&gt; None:\n        self.client = boto3.client(\"s3\", region_name=region_name)\n        self.resource = boto3.resource(\"s3\", region_name=region_name)\n\n    def download_s3_folder(\n        self, bucket_name: str, s3_folder: str, local_dir: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Download the contents of a folder directory in an S3 bucket.\n        Source: [StackOverflow](https://stackoverflow.com/a/62945526).\n\n        Args:\n            bucket_name (str):\n                Name of the s3 bucket\n            s3_folder (str):\n                Folder path in the s3 bucket\n            local_dir (Optional[str], optional):\n                Relative or absolute directory path in the local file system.\n                Defaults to `None`.\n        \"\"\"\n        bucket = self.resource.Bucket(bucket_name)\n        for obj in bucket.objects.filter(Prefix=s3_folder):\n            # use key as save path if local_dir not specified, otherwise use local_dir\n            target = (\n                obj.key\n                if local_dir is None\n                else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n            )\n            # create dir if target does't exist\n            if not os.path.exists(os.path.dirname(target)):\n                os.makedirs(os.path.dirname(target))\n            # skip subfolders\n            if obj.key[-1] == \"/\":\n                continue\n            bucket.download_file(obj.key, target)\n\n    def upload_folder(self, bucket_name: str, prefix: str, local_dir: str) -&gt; None:\n        \"\"\"\n        Uploads all files under `local_dir` to S3 bucket with `prefix`.\n        Utilizes parallelism to speed up upload process.\n\n        ### Example\n        ```title=\"Sample Directory\"\n        tmp/\n        \u2514\u2500\u2500 en-us\n            \u251c\u2500\u2500 utt_0.tsv\n            \u251c\u2500\u2500 utt_0.wav\n        \u2514\u2500\u2500 id-id\n            \u251c\u2500\u2500 utt_1.tsv\n            \u2514\u2500\u2500 utt_1.wav\n        ```\n        ```pycon title=\"example_upload_folder.py\"\n        &gt;&gt;&gt; bucket_name, prefix, local_dir = \"my-bucket\", \"train/\", \"tmp/\"\n        &gt;&gt;&gt; my_client = S3Client()\n        &gt;&gt;&gt; my_client.upload_folder(bucket_name, prefix, local_dir)\n        ```\n        ```title=\"Result\"\n        Uploaded tmp/en-us/utt_0.tsv to s3://my-bucket/train/en-us/utt_0.tsv\n        Uploaded tmp/en-us/utt_0.wav to s3://my-bucket/train/en-us/utt_0.wav\n        Uploaded tmp/id-id/utt_1.tsv to s3://my-bucket/train/id-id/utt_1.tsv\n        Uploaded tmp/id-id/utt_1.wav to s3://my-bucket/train/id-id/utt_1.wav\n        ```\n\n        Args:\n            bucket_name (str):\n                S3 bucket name.\n            prefix (str):\n                Object key's prefix.\n            local_dir (str):\n                Path to local directory.\n        \"\"\"\n        paths, keys = [], []\n        # recursively walk through local directory\n        # setup file paths and object keys\n        for root, dirs, files in os.walk(local_dir):\n            # skip hidden folders\n            dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n            for file in files:\n                # skip hidden files\n                if not file.startswith(\".\"):\n                    # path to file in local dir\n                    path = os.path.join(root, file)\n                    # relative path from local dir to file\n                    relpath = os.path.relpath(path, local_dir)\n                    # object key from prefix to relative path\n                    key = os.path.join(prefix, relpath)\n                    paths.append(path)\n                    keys.append(key)\n\n        fn = partial(self.upload_file, bucket_name=bucket_name)\n        with ThreadPoolExecutor() as executor:\n            _ = list(tqdm(executor.map(fn, keys, paths), total=len(keys)))\n\n    def put_object(self, bucket_name: str, key: str, value: str) -&gt; None:\n        \"\"\"\n        Puts `value` (in str) to S3 bucket.\n\n        Args:\n            bucket_name (str):\n                S3 bucket name.\n            key (str):\n                Key to file in bucket.\n            value (str):\n                String representation of object to put in S3.\n        \"\"\"\n        self.client.put_object(Bucket=bucket_name, Key=key, Body=value)\n\n    def upload_file(self, key: str, path: str, bucket_name: str) -&gt; None:\n        \"\"\"\n        Uploads file at `path` to S3 bucket with `key` as object key.\n\n        Args:\n            key (str):\n                Key to file in bucket.\n            path (str):\n                Path to local file to upload.\n            bucket_name (str):\n                S3 bucket name.\n        \"\"\"\n        self.client.upload_file(Bucket=bucket_name, Key=key, Filename=path)\n</code></pre>"},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client.download_s3_folder","title":"<code>download_s3_folder(self, bucket_name, s3_folder, local_dir=None)</code>","text":"<p>Download the contents of a folder directory in an S3 bucket. Source: StackOverflow.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the s3 bucket</p> required <code>s3_folder</code> <code>str</code> <p>Folder path in the s3 bucket</p> required <code>local_dir</code> <code>Optional[str]</code> <p>Relative or absolute directory path in the local file system. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>speechline/utils/s3.py</code> <pre><code>def download_s3_folder(\n    self, bucket_name: str, s3_folder: str, local_dir: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Download the contents of a folder directory in an S3 bucket.\n    Source: [StackOverflow](https://stackoverflow.com/a/62945526).\n\n    Args:\n        bucket_name (str):\n            Name of the s3 bucket\n        s3_folder (str):\n            Folder path in the s3 bucket\n        local_dir (Optional[str], optional):\n            Relative or absolute directory path in the local file system.\n            Defaults to `None`.\n    \"\"\"\n    bucket = self.resource.Bucket(bucket_name)\n    for obj in bucket.objects.filter(Prefix=s3_folder):\n        # use key as save path if local_dir not specified, otherwise use local_dir\n        target = (\n            obj.key\n            if local_dir is None\n            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n        )\n        # create dir if target does't exist\n        if not os.path.exists(os.path.dirname(target)):\n            os.makedirs(os.path.dirname(target))\n        # skip subfolders\n        if obj.key[-1] == \"/\":\n            continue\n        bucket.download_file(obj.key, target)\n</code></pre>"},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client.put_object","title":"<code>put_object(self, bucket_name, key, value)</code>","text":"<p>Puts <code>value</code> (in str) to S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>S3 bucket name.</p> required <code>key</code> <code>str</code> <p>Key to file in bucket.</p> required <code>value</code> <code>str</code> <p>String representation of object to put in S3.</p> required Source code in <code>speechline/utils/s3.py</code> <pre><code>def put_object(self, bucket_name: str, key: str, value: str) -&gt; None:\n    \"\"\"\n    Puts `value` (in str) to S3 bucket.\n\n    Args:\n        bucket_name (str):\n            S3 bucket name.\n        key (str):\n            Key to file in bucket.\n        value (str):\n            String representation of object to put in S3.\n    \"\"\"\n    self.client.put_object(Bucket=bucket_name, Key=key, Body=value)\n</code></pre>"},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client.upload_file","title":"<code>upload_file(self, key, path, bucket_name)</code>","text":"<p>Uploads file at <code>path</code> to S3 bucket with <code>key</code> as object key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to file in bucket.</p> required <code>path</code> <code>str</code> <p>Path to local file to upload.</p> required <code>bucket_name</code> <code>str</code> <p>S3 bucket name.</p> required Source code in <code>speechline/utils/s3.py</code> <pre><code>def upload_file(self, key: str, path: str, bucket_name: str) -&gt; None:\n    \"\"\"\n    Uploads file at `path` to S3 bucket with `key` as object key.\n\n    Args:\n        key (str):\n            Key to file in bucket.\n        path (str):\n            Path to local file to upload.\n        bucket_name (str):\n            S3 bucket name.\n    \"\"\"\n    self.client.upload_file(Bucket=bucket_name, Key=key, Filename=path)\n</code></pre>"},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client.upload_folder","title":"<code>upload_folder(self, bucket_name, prefix, local_dir)</code>","text":"<p>Uploads all files under <code>local_dir</code> to S3 bucket with <code>prefix</code>. Utilizes parallelism to speed up upload process.</p>"},{"location":"reference/utils/s3/#speechline.utils.s3.S3Client.upload_folder--example","title":"Example","text":"<p>Sample Directory<pre><code>tmp/\n\u2514\u2500\u2500 en-us\n    \u251c\u2500\u2500 utt_0.tsv\n    \u251c\u2500\u2500 utt_0.wav\n\u2514\u2500\u2500 id-id\n    \u251c\u2500\u2500 utt_1.tsv\n    \u2514\u2500\u2500 utt_1.wav\n</code></pre> example_upload_folder.py<pre><code>&gt;&gt;&gt; bucket_name, prefix, local_dir = \"my-bucket\", \"train/\", \"tmp/\"\n&gt;&gt;&gt; my_client = S3Client()\n&gt;&gt;&gt; my_client.upload_folder(bucket_name, prefix, local_dir)\n</code></pre> Result<pre><code>Uploaded tmp/en-us/utt_0.tsv to s3://my-bucket/train/en-us/utt_0.tsv\nUploaded tmp/en-us/utt_0.wav to s3://my-bucket/train/en-us/utt_0.wav\nUploaded tmp/id-id/utt_1.tsv to s3://my-bucket/train/id-id/utt_1.tsv\nUploaded tmp/id-id/utt_1.wav to s3://my-bucket/train/id-id/utt_1.wav\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>Object key's prefix.</p> required <code>local_dir</code> <code>str</code> <p>Path to local directory.</p> required Source code in <code>speechline/utils/s3.py</code> <pre><code>def upload_folder(self, bucket_name: str, prefix: str, local_dir: str) -&gt; None:\n    \"\"\"\n    Uploads all files under `local_dir` to S3 bucket with `prefix`.\n    Utilizes parallelism to speed up upload process.\n\n    ### Example\n    ```title=\"Sample Directory\"\n    tmp/\n    \u2514\u2500\u2500 en-us\n        \u251c\u2500\u2500 utt_0.tsv\n        \u251c\u2500\u2500 utt_0.wav\n    \u2514\u2500\u2500 id-id\n        \u251c\u2500\u2500 utt_1.tsv\n        \u2514\u2500\u2500 utt_1.wav\n    ```\n    ```pycon title=\"example_upload_folder.py\"\n    &gt;&gt;&gt; bucket_name, prefix, local_dir = \"my-bucket\", \"train/\", \"tmp/\"\n    &gt;&gt;&gt; my_client = S3Client()\n    &gt;&gt;&gt; my_client.upload_folder(bucket_name, prefix, local_dir)\n    ```\n    ```title=\"Result\"\n    Uploaded tmp/en-us/utt_0.tsv to s3://my-bucket/train/en-us/utt_0.tsv\n    Uploaded tmp/en-us/utt_0.wav to s3://my-bucket/train/en-us/utt_0.wav\n    Uploaded tmp/id-id/utt_1.tsv to s3://my-bucket/train/id-id/utt_1.tsv\n    Uploaded tmp/id-id/utt_1.wav to s3://my-bucket/train/id-id/utt_1.wav\n    ```\n\n    Args:\n        bucket_name (str):\n            S3 bucket name.\n        prefix (str):\n            Object key's prefix.\n        local_dir (str):\n            Path to local directory.\n    \"\"\"\n    paths, keys = [], []\n    # recursively walk through local directory\n    # setup file paths and object keys\n    for root, dirs, files in os.walk(local_dir):\n        # skip hidden folders\n        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n        for file in files:\n            # skip hidden files\n            if not file.startswith(\".\"):\n                # path to file in local dir\n                path = os.path.join(root, file)\n                # relative path from local dir to file\n                relpath = os.path.relpath(path, local_dir)\n                # object key from prefix to relative path\n                key = os.path.join(prefix, relpath)\n                paths.append(path)\n                keys.append(key)\n\n    fn = partial(self.upload_file, bucket_name=bucket_name)\n    with ThreadPoolExecutor() as executor:\n        _ = list(tqdm(executor.map(fn, keys, paths), total=len(keys)))\n</code></pre>"},{"location":"reference/utils/tokenizer/","title":"Word Tokenizer","text":""},{"location":"reference/utils/tokenizer/#speechline.utils.tokenizer.WordTokenizer","title":"<code> speechline.utils.tokenizer.WordTokenizer        </code>  <code>dataclass</code>","text":"<p>Basic word-based splitting.</p> Source code in <code>speechline/utils/tokenizer.py</code> <pre><code>class WordTokenizer:\n    \"\"\"\n    Basic word-based splitting.\n    \"\"\"\n\n    tokenizer = TweetTokenizer(preserve_case=False)\n\n    def __call__(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Splits text into words, ignoring punctuations and case.\n\n        Args:\n            text (str):\n                Text to tokenize.\n\n        Returns:\n            List[str]:\n                List of tokens.\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        tokens = [token for token in tokens if token not in punctuation]\n        return tokens\n</code></pre>"},{"location":"reference/utils/tokenizer/#speechline.utils.tokenizer.WordTokenizer.__call__","title":"<code>__call__(self, text)</code>  <code>special</code>","text":"<p>Splits text into words, ignoring punctuations and case.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to tokenize.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of tokens.</p> Source code in <code>speechline/utils/tokenizer.py</code> <pre><code>def __call__(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Splits text into words, ignoring punctuations and case.\n\n    Args:\n        text (str):\n            Text to tokenize.\n\n    Returns:\n        List[str]:\n            List of tokens.\n    \"\"\"\n    tokens = self.tokenizer.tokenize(text)\n    tokens = [token for token in tokens if token not in punctuation]\n    return tokens\n</code></pre>"}]}